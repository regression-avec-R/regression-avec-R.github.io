---
title: "5 Inférence dans le modèle gaussien"
toc: true
---

```{r,child='../_preamble.qmd'}
```

::: {#exr-5-1 name="Questions de cours"}
A, C, A, B, B.
:::

::: {#exr-5-2 name="Théorème 5.1"}
L'IC (i) découle de la propriété (i) de la proposition 5.3.
La propriété (ii) donnant un IC pour $\sigma^2$ découle de la loi de 
$\hat \sigma^2$.
Enfin, la propriété (iii) est une conséquence de la loi obtenue propriété 
(ii) de la proposition 5.3.
:::

::: {#exr-5-3 name="Test et $R^2$"}
En utilisant l'orthogonalité des sous-espaces (figure 5.3 page 99) et le théorème de Pythagore, nous 
avons 
\begin{eqnarray*}
\|\hat Y_0-\hat Y\|^2 &=& \|\hat \varepsilon_0\|^2- \| \hat \varepsilon\|^2.
\end{eqnarray*}
Nous pouvons le démontrer de la manière suivante :
\begin{eqnarray*}
\|\hat Y_0-\hat Y\|^2 &=& \|\hat Y_0-Y+Y-\hat Y\|^2\\
&=& \|\hat \varepsilon_0\|^2+ \| \hat \varepsilon\|^2
+2\langle \hat Y_0-Y,Y-\hat Y\rangle\\
&=& \|\hat \varepsilon_0\|^2+ \| \hat \varepsilon\|^2
-2\langle Y-\hat Y_0,Y-\hat Y\rangle\\
&=& \|\hat \varepsilon_0\|^2+ \| \hat \varepsilon\|^2
-2\langle P_{X_0^\perp}Y,P_{X^\perp}Y\rangle\\
&=& \|\hat \varepsilon_0\|^2+ \| \hat \varepsilon\|^2
-2\langle (P_{X^\perp}+P_{X})P_{X_0^\perp}Y,P_{X^\perp}Y\rangle.
\end{eqnarray*}
Or $\Im(X_0) \subset \Im(X)$, nous avons 
donc $P_{X^\perp}P_{X_0^\perp}=P_{X^\perp}$. 
De plus,
$\hat \varepsilon=P_{X^\perp}Y$, cela donne
\begin{eqnarray*}
\langle (P_{X^\perp}+P_{X})P_{X_0^\perp}Y,P_{X^\perp}Y\rangle &=&
\langle P_{X^\perp}Y,P_{X^\perp}Y\rangle 
+\langle P_{X}P_{X_0^\perp}Y,P_{X^\perp}Y\rangle \\
&=& \|\hat \varepsilon\|^2 + 0.
\end{eqnarray*}
Le résultat est démontré, revenons à la statistique de test.
Introduisons les différentes écritures du $\leR$
\begin{eqnarray*}
\leR2 = \frac{\|\hat Y - \bar Y\|^2}{\|Y - \bar Y\|^2}=1 -
\frac{\|\hat \varepsilon\|^2}{\|Y - \bar Y\|^2}.
\end{eqnarray*}
La statistique de test vaut
\begin{eqnarray*}
F&=&\frac{\|\hat \varepsilon_0\|^2- \| \hat \varepsilon\|^2}
{\| Y-\hat Y\|^2}\frac{n-p}{p-p_0}\\
&=&\frac{\|\hat \varepsilon_0\|^2/\|Y-\bar Y\|^2- 
\| \hat \varepsilon\|^2/\|Y-\bar Y\|^2}
{\| Y-\hat Y\|^2/\|Y-\bar Y\|^2}\frac{n-p}{p-p_0},
\end{eqnarray*}
nous obtenons
\begin{eqnarray*}
F&=&\frac{\leR-\ro}{1-\leR}\frac{n-p}{p-p_0},
\end{eqnarray*}
soit le résultat annoncé. Cette dernière quantité est toujours 
positive car $\ro \leq \leR$ et nous avons là un moyen de
tester des modèles emboîtés *via* le coefficient de détermination.
:::

::: {#exr-5-4 name="Test et $R^2$ et constante dans le modèle"}
à corriger ;).
:::

::: {#exr-5-5 name="Ozone"}
1.  Les résultats sont dans l'ordre
    \begin{eqnarray*}
    6.2, 0.8, 6.66, -1.5, -1, 50, 5, 124.
    \end{eqnarray*}

2.  La statistique de test de nullité du paramètre se trouve dans la troisième colonne, nous conservons $\Hz$ pour les
paramètres associés à Ne9 et Ne12, et la rejetons pour les autres.

3.  La statistique de test de nullité simultanée des paramètres autres que la constante vaut 50. Nous rejetons $\Hz$.
    
4.  Nous connaissons
    \begin{align*}
    \hat y^{p}_{n+1} &= x'_{n+1}\hat \beta,\\
    x'_{n+1}&=(1, 10, 20, 0, 0, 1)\\
    \hat \beta&=(62, -4, 5, -1.5, -0.5, 0.8)'
    \end{align*}
    et donc la prévision est $\hat y^{p}_{n+1} =122.8$. Pour l'intervalle de confiance il nous faut $\hat\sigma=16$ mais aussi la matrice $X'X$ (donc toutes les données) ce que nous n'avons pas ici. On ne peut donc faire d'intervalle de confiance.

5.  Nous sommes en présence de modèles emboîtés, nous pouvons appliquer la formule adaptée (voir l'exercice précédent) :
    \begin{eqnarray*}
    F&=& \frac{\leR2-\ro2}{1-\leR2}\frac{n-p}{p-p_0}\\
    &=& \frac{0.66-0.5}{1-0.66}\frac{124}{2}= 29.
    \end{eqnarray*}
    Nous conservons $\Hz$, c'est-à-dire le modèle le plus simple.
:::

::: {#exr-5-6 name="Équivalence du test T et du test F"}
Récrivons la statistique de test $F$, en se rappelant que $X_0$ est
la matrice $X$ privée de sa $j^e$ colonne, celle correspondant au
coefficient que l'on teste :
 \begin{eqnarray*}
 F&=&\frac{\|X\hat \beta-P_{X_0}X\hat \beta\|^2}{\hat \sigma^2}
  =\frac{\|X_j\hat \beta_j-\hat \beta_jP_{X_0}X_j\|^2}{\hat \sigma^2}
=\frac{\hat\beta_j^2}{\hat \sigma^2}X_j'(I-P_{X_0})X_j.
 \end{eqnarray*}
Récrivons maintenant le carré de la statistique $T$ en explicitant $\hat
 \sigma^2_{\hat \beta_j}$ :
\begin{eqnarray*}
T^2&=&\frac{\hat \beta_j^2}{\hat \sigma^2 [(X'X)^{-1}]_{jj}},
\end{eqnarray*}
où $[(X'X)^{-1}]_{jj}$ est le $j^e$ élément diagonal de la 
matrice $(X'X)^{-1}$. Afin de calculer ce terme, nous utilisons 
la formule permettant d'obtenir l'inverse d'une matrice bloc, formule 
donnée en annexe A.2 page 416.
Pour appliquer facilement cette formule, en changeant
l'ordre des variables, la matrice $X$ devient $(X_0|X_j)$ et
$X'X$ s'écrit alors
\begin{eqnarray*}
X'X&=&\left(
\begin{array}{c|c}
X'_0X_0&X'_0X_j\\\hline
X'_jX_0&X'_jX_j
\end{array}\right).
\end{eqnarray*}
Son inverse, en utilisant la formule d'inverse de matrice bloc, est 
\begin{eqnarray*}
[(X'X)^{-1}]_{jj}&=&\left(X'_jX_j-X'_jX_0(X'_0X_0)^{-1}X'_0X_j\right)^{-1}
=\left(X_j'(I-P_{X_0})X_j\right)^{-1}.
\end{eqnarray*}
Nous avons donc $T^2=F$. 
Au niveau des lois, l'égalité est aussi
valable et nous avons que le carré d'un Student à
$(n-p)$ ddl est une loi de Fisher à $(1,n-p)$ ddl. Bien entendu, le
quantile $(1-\alpha)$ d'une loi de Fisher correspond au quantile
$1-\alpha/2$ d'une loi de Student. La loi $\mathcal{T}$ est symétrique
autour de 0 et donc, lorsqu'elle est élevée au carré, les valeurs plus
faibles que $t_{n-p}(\alpha/2)$, qui ont une probabilité sous $\Hz$ de
$\alpha/2$ d'apparaître, et celles plus fortes que
$t_{n-p}(1-\alpha/2)$, qui ont une probabilité sous $\Hz$ de
$\alpha/2$ d'apparaître, deviennent toutes plus grandes que
$t^2_{n-p}(1-\alpha/2)$. La probabilité que ces valeurs dépassent ce
seuil sous $\Hz$ est de $\alpha$ et correspond donc bien par définition à
$f_{1,n-p}(1-\alpha)$.
:::

::: {#exr-5-7 name="Équivalence du test F et du test de VM"}
Nous avons noté la vraisemblance en début du chapitre par 
\begin{eqnarray*}
\mathcal{L}(Y,\beta,\sigma^2) &=& \prod_{i=1}^n f_{Y}(y_i) 
= \left(\frac{1}{2\pi\sigma^2}\right)^{n/2}\exp{\left[-\frac{1}{2 \sigma^2}
\sum_{i=1}^n \left(y_i- \sum_{j=1}^p\beta_jx_{ij}\right)^2\right]}\\
&=& \left(\frac{1}{2\pi\sigma^2}\right)^{n/2}\exp{\left[-\frac{1}{2 \sigma^2}
\|Y-X\beta\|^2\right]}.
\end{eqnarray*}
Cette vraisemblance est maximale lorsque $\hat \beta$ est l'estimateur 
des MC et que $\hat \sigma^2 = \|Y-X\hat \beta\|^2/n$. Nous avons alors
\begin{eqnarray*}
\max_{\beta,\sigma^2} \mathcal{L}(Y,\beta,\sigma^2)&=&
\left(\frac{n}{2\pi\|Y-X\hat \beta\|^2 }\right)^{n/2}\exp{\left(-\frac{n}{2}\right)}\\
&=&\left(\frac{n}{2\pi \SCR}\right)^{n/2}\exp{\left(-\frac{n}{2}\right)}
=\mathcal{L}(Y,\hat \beta,\hat \sigma^2),
\end{eqnarray*}
où $\SCR=\|Y-X\hat \beta\|^2$.\index{Somme des carrés!résiduelle}


Sous l'hypothèse 
$\Hz$ nous obtenons de façon évidente le résultat suivant : 
\begin{eqnarray*}
\max_{\beta,\sigma^2} \mathcal{L}_0(Y,\beta_0,\sigma^2) 
=\left(\frac{n}{2\pi \SCR_0}\right)^{n/2}\exp{\left(-\frac{n}{2}\right)}
=\mathcal{L}_0(Y,\hat \beta_0,\hat \sigma^2_0),
\end{eqnarray*}
où $\SCR_0$ correspond à la somme des carrés résiduels sous $\Hz$, c'est-à-dire 
$\SCR_0=\|Y-X_0\hat \beta_0\|^2$.
On définit le test du rapport de vraisemblance maximale (VM) par la 
région critique \citep{leh59} suivante :
\begin{eqnarray*}
\mathcal{D}_\alpha = \left\{
Y \in \R^n : \lambda=\frac{\mathcal{L}_0(Y,\hat \beta_0,\hat \sigma^2)}
{\mathcal{L}(Y,\hat \beta,\hat \sigma^2)} < \lambda_0 
\right\}.
\end{eqnarray*}
La statistique du rapport de vraisemblance maximale vaut ici
\begin{eqnarray*}
\lambda = \left(\frac{\SCR}{\SCR_0}\right)^{n/2} = 
\left(\frac{\SCR_0}{\SCR}\right)^{-n/2}.
\end{eqnarray*}
Le test du rapport de VM rejette $\Hz$ lorsque la statistique $\lambda$ 
est inférieure à une valeur $\lambda_0$ définie de façon à avoir le 
niveau du test égal à $\alpha$. Le problème qui reste à étudier est de
connaître la distribution (au moins sous $\Hz$) de $\lambda$.
Définissons, pour $\lambda$ positif, la fonction bijective $g$ suivante :
\begin{eqnarray*}
g(\lambda) = \lambda^{-2/n}-1.
\end{eqnarray*}
La fonction $g$ est décroissante (sa dérivée est toujours négative), 
donc $\lambda<\lambda_0$ si et seulement si 
$g(\lambda)>g(\lambda_0)$. Cette fonction $g$ va nous permettre de nous 
ramener à des statistiques dont la loi est connue. Nous avons alors 
\begin{eqnarray*}
g(\lambda)&>&g(\lambda_0)\\
\frac{\SCR_0-\SCR}{\SCR}&>&g(\lambda_0)\\
\frac{n-p}{p-p_0}\frac{\SCR_0-\SCR}{\SCR}&>&f_0
\end{eqnarray*}
où $f_0$ est déterminée par 
\begin{eqnarray*}
P_{\Hz}\left(\frac{n-p}{p-p_0}\frac{\SCR_0-\SCR}{\SCR}>f_0
\right)=\alpha,
\end{eqnarray*}
avec la loi de cette statistique qui est une loi $\mathcal{F}_{p-p_0,n-p}$
(cf.~section précédente).  Le test du rapport de VM est donc équivalent
au test qui rejette $\Hz$ lorsque la statistique
\begin{eqnarray*}
F=\frac{n-p}{p-p_0}\frac{\SCR_0-\SCR}{\SCR}
\end{eqnarray*}
est supérieure à $f_0$, où $f_0$ est la valeur du fractile $\alpha$ 
de la loi de Fisher à $(p-p_0,n-p)$ degrés de liberté.
:::

::: {#exr-5-8 name="Test de Fisher pour une hypothèse linéaire quelconque"}
Nous pouvons toujours traduire l'hypothèse $\Hz$ : 
$R\beta=r$ en terme de sous-espace de $\M_X$. Lorsque $r=0$, 
nous avons un sous-espace vectoriel de $\M_X$ et lorsque 
$r\neq 0$ nous avons un sous-espace affine de $\M_X$. Dans 
les deux cas, nous noterons ce sous-espace $\M_0$ et 
$\M_0 \subset \M_X$. Cependant nous ne pourrons 
plus le visualiser facilement comme nous l'avons fait précédemment 
avec $\M_{X_0}$ où nous avions enlevé des colonnes à la 
matrice $X$. Nous allons décomposer l'espace $\M_X$ en deux 
sous-espaces orthogonaux
\begin{eqnarray*}
\M_X = \M_0 \stackrel{\perp}{\oplus} ( \M_0^\perp \cap \M_X ).
\end{eqnarray*}
\noindent
Sous $\Hz$, l'estimation des moindres carrés donne $\hat Y_0$
projection orthogonale de $Y$ sur $\M_0$ et nous appliquons la même
démarche pour construire la statistique de test. La démonstration 
est donc la même que celle du théorème 5.2. C'est-à-dire 
que nous regardons si $\hat Y_0$ est proche de $\hat Y$ et nous avons donc 
\begin{eqnarray*}
F&=&\frac{\|\hat Y -\hat Y_0\|^2/\dim(\M_0^{\perp}  \cap \M_X)}{\|Y - \hat Y\|^2/
\dim(\M_{X^{\perp}})}\\ 
&=&\frac{n-p}{q} \frac{\|Y-\hat Y_0\|^2-\|Y-\hat Y\|^2}{ \|Y-\hat Y\|^2}\\
&=& \frac{n-p}{q}\frac{\SCR_0-\SCR}{\SCR}\sim \mathcal{F}_{q,n-p}.
\end{eqnarray*}
Le problème du test réside dans le calcul de $\hat Y_0$. Dans la
partie précédente, il était facile de calculer $\hat Y_0$ car
nous avions la forme explicite du projecteur sur $\M_0$. 
Une première façon de procéder revient à trouver la forme du
projecteur sur $\M_0$. Une autre façon de faire est de récrire le
problème de minimisation sous la contrainte $R\beta=r$. Ces deux
manières d'opérer sont présentées en détail dans la correction 
de l'exercice 2.13. Dans tous les cas 
l'estimateur des MC contraints 
par  $R\beta=r$ est défini par
\begin{eqnarray*}
\hat \beta_0&=&\hat \beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\hat \beta).
\end{eqnarray*}
:::

::: {#exr-5-9 name="Généralisation de la régression ridge"}
  Soit la fonction à minimiser
  \begin{align*}
    R(\beta)&=\|Y-X\beta\|^2 -\sum_{j=1}^{p}\delta_j(\beta_j^2) \\
    &= (Y-X\beta)'(Y-X\beta) - \beta' \Delta \beta
  \end{align*}
  avec $\delta_{1}, \dotsc, \delta_{p}$ des réels positifs ou nuls.

  Sachant que $\frac{\partial \beta' A\beta}{\partial \beta}=2A\beta$ (avec $A$ symétrique) et que $\frac{\partial X\beta}{\partial \beta}=X'$ nous avons la dérivée partielle suivante
  \begin{align*}
    \frac{\partial R}{\partial \beta}&=-2X'(Y-X\beta)  + 2\Delta \beta
  \end{align*}
  En annulant cette dérivée nous avons
  \begin{align*}
    -2X'(Y-X\hat\beta_{\mathrm{RG}}) + 2\Delta \hat\beta_{\mathrm{RG}}&=0\\
         (X'X + \Delta) \hat\beta_{\mathrm{RG}} &=  X'Y
  \end{align*}
  donc en prémultipliant par $(X'X-\Delta)^{-1}$ nous obtenons
  \begin{align*}
    \hat\beta_{\mathrm{RG}}=(X'X-\Delta)^{-1}X'Y.
  \end{align*}
  En régression multiple le nombre de paramètres est $p=\tr(P_{X})$
  avec $P_{X}$ la matrice de l'endomorphisme qui permet d'obtenir
  $\hat Y$ à partir de $Y$. Dans cette régression ridge, nous avons
  que
  \begin{align*}
    \hat Y_{\mathrm{RG}}&=X\hat\beta_{\mathrm{RG}}=X(X'X-\Delta)^{-1}X'Y
  \end{align*}
  donc la matrice de l'endomorphisme est ici $X(X'X-\Delta)^{-1}X'$ et
  le nombre équivalent de paramètres est $\tr(X(X'X-\Delta)^{-1}X')$.
:::

::: {#exr-5-10 name="IC pour la régression ridge"}
1.  Loi de $\hat \beta$ : $\NO(\beta, \sigma^{2}(X'X)^{-1})$ grâce au modèle et à $\HH_3$.

2.  Loi de $\hat \beta_{\mathrm{ridge}}(\tilde\kappa) $. Comme $\hat \beta_{\mathrm{ridge}}(\tilde\kappa)= (X'X-\tilde\kappa I)^{-1}X'Y$ avec $A=(X'X-\tilde\kappa I)^{-1}X'$ qui est une matrice fixe. Avec $\HH_{3}$ et le modèle de régression multiple on a que $Y\sim\NO(X\beta, \sigma^{2}I)$.

    Puisque $Y$ est un vecteur gaussien, il en est de même de $\hat \beta_{\mathrm{ridge}}(\tilde\kappa)=AY$. Calculons son espérance
    \begin{align*}
    \E(\hat \beta_{\mathrm{ridge}}(\tilde\kappa))&=\E(AY)=A\E(Y)=AX\beta\\
    &=(X'X-\tilde\kappa I)^{-1}X'X\beta
    \end{align*}
    et sa variance
    \begin{align*}
    \V(\hat \beta_{\mathrm{ridge}}(\tilde\kappa))&=\V(AY)=A\V(Y)A'=A\sigma^{2}I A' = \sigma^{2} A A'\\
    &=\sigma^{2}(X'X-\tilde\kappa I)^{-1}X'X(X'X-\tilde\kappa I)^{-1}.
    \end{align*}
    
3.  Calculons le produit scalaire de $Y-\hat Y_{\mathrm{ridge}}$ et $\hat Y_{MC}:$
    \begin{align*}
    <Y-\hat Y_{\mathrm{ridge}};\hat Y_{MC}>&=<Y-\hat Y_{MC} + \hat Y_{MC} -\hat Y_{\mathrm{ridge}};\hat Y_{MC}>  \\
    & =  <Y-\hat Y_{MC}; \hat Y_{MC}> + <   \hat Y_{MC} -\hat Y_{\mathrm{ridge}} ;  \hat Y_{MC}>\\
    &= 0 + <   \hat Y_{MC} -\hat Y_{\mathrm{ridge}} ;  \hat Y_{MC}>
    \end{align*}
    Or $\hat Y_{\mathrm{ridge}} = X\beta_{\mathrm{ridge}}(\tilde\kappa)$ donc il appartient au sous espace vectoriel $\Im(X)$, de même que $\hat Y_{MC}=P_{X}Y$. Sauf si $\tilde\kappa=0$ on a que $\hat Y_{\mathrm{ridge}}\neq \hat Y_{MC}$ donc $\hat Y_{MC} -\hat Y_{\mathrm{ridge}}$ est un vecteur non nul de $\Im(X)$ et donc son produit scalaire avec $\hat Y_{MC}\in \Im(X)$ est non nul.

4.  Il faut pouvoir démontrer l'indépendance de $\hat\sigma_{\mathrm{ridge}}$ et $\hat \beta_{\mathrm{ridge}}$. Pour le théorème 5.1, on montre l'indépendance entre$\hat \beta$ et $\hat \sigma$ en considérant les 2 vecteurs $\hat\beta$ et $\hat \varepsilon=(Y-\hat Y)$. Comme nous pouvons écrire $\hat \beta=(X'X)^{-1}X'P_XY$, $\hat \beta$ est donc une fonction fixe (dépendante uniquement des $X$) de $P_XY$. De plus, $\hat \varepsilon=P_{X^\perp}Y$ est orthogonal à $P_XY$. Ces 2 vecteurs suivent des lois normales et sont donc indépendants. Il en résulte que $\hat \beta$ et $Y-\hat Y$ sont indépendants et de même pour $\hat \beta$ et $\hat \sigma$.

    Ici, $\hat\sigma_{\mathrm{ridge}}$ est une fonction de $Y-\hat Y_{\mathrm{ridge}}$. Le vecteur $\hat\beta_{\mathrm{ridge}}=(X'X+\tilde\kappa I_p)^{-1}X'Y=(X'X+\tilde\kappa I_p)^{-1}X'P_XY$ est une fonction fixe ($\tilde \kappa$ est considéré comme fixé) de $P_XY$. Par contre, $P_XY$ n'est pas orthogonal à $(Y-\hat Y_{\mathrm{ridge}})$, comme nous l'avons montré, nous ne pouvons donc montrer l'indépendance de $\hat\beta_{\mathrm{ridge}}$ et $\hat\sigma_{\mathrm{ridge}}$.

    Une autre idée serait d'utiliser $\hat\sigma$ mais en général si l'on utilise la régression ridge c'est que l'on se doute que $\hat Y$ n'est pas un bon estimateur de $X\beta$ et donc \textit{a priori} $\hat\sigma$ qui est une fonction de $Y-\hat Y$ risque de ne pas être un bon estimateur de $\sigma$. L'estimateur $\hat\sigma$ peut même être nul, ce qui pratiquement peut arriver quand $p>n$.

5.  En général quand $X$ est fixe pour un bootstrap en régression on estime $\hat \beta$ puis on déduit les $\{\hat \epsilon_{i}\}$. De cet ensemble sont tirés de manière équiprobable avec remise $n$ résidus $\{\hat \epsilon_{i}^{*}\}$. Ces nouveaux résidus sont additionnés à $X\beta$ pour faire un nouveau vecteur $Y^{*}$ et avoir un échantillon bootstap $Y^{*}, X$.

    Ici l'estimation de $\hat \beta$ sera mauvaise (et c'est pour cela que l'on utilise la régression ridge) et plutôt que d'estimer de mauvais résidus nous allons retirer avec remise parmi les $Y_{i}, X_{i.}$ ce qui est la procédure adaptée au $X$ aléatoire mais ici nous avons peu de choix


    **Entrées** : $\tilde \kappa$ fixé, $\alpha$ fixé, $B$ choisi. <br>
    **Sorties** : IC, au niveau $\alpha$, coordonnée par coordonnée de $\beta$.
    1.  Estimer $\beta_{\mathrm{ridge}}(\tilde \kappa)$ .
    2.  En déduire $\hat \varepsilon_{\mathrm{ridge}}=Y-X\hat \beta_{\mathrm{ridge}}$.
    3.  Pour $k=1$ à $B$
        -   tirer avec remise $n$ résidus estimés parmi les $n$ coordonnées de $\hat \varepsilon_{\mathrm{ridge}}$ ;
        -   on note ces résidus (réunis dans 1 vecteur) $\hat \varepsilon_{\mathrm{ridge}}^{(k)}$ ;
        -   construire 1 échantillon $Y^{(k)}=X\beta_{\mathrm{ridge}}(\tilde \kappa)+\hat \varepsilon_{\mathrm{ridge}}^{(k)}$ ;
        -   $\tilde \kappa^{(k)} \leftarrow \tilde \kappa$ ;
        -   estimer le vecteur de paramètre $\beta_{\mathrm{ridge}}^{(k)}(\tilde \kappa^{(k)})=(X'X+\tilde\kappa^{(k)} I_p)^{-1}X'Y^{(k)}$ ;
    4.  Pour $j=1$ à $p$
        -   calculer les quantiles empiriques de niveau $\alpha/2$ et $1-\alpha/2$    pour la coordonnée $j$, sur tous les vecteurs $\{\beta_{\mathrm{ridge}}^{(k)}(\tilde \kappa)\}$ ;
        
        
6.  L'algorithme est presque le même. Cependant comme $\tilde \kappa$ n'est pas fixé, pour estimer $\beta_{\mathrm{ridge}}(\tilde \kappa)$ il faut déterminer $\tilde \kappa$ par une méthode choisie. Ensuite, à chaque estimation de $\beta_{\mathrm{ridge}}^{(k)}(\tilde \kappa^{(k)})$, il est nécessaire au préalable de déterminer $\tilde \kappa^{(k)}$ par la même méthode que celle utilisée pour déterminer $\tilde \kappa$.
:::
