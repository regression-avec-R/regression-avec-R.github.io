---
title: "17 Estimateurs à noyau et $k$ plus proches voisins"
toc: true
---

```{r,child='../_preamble.qmd'}
```

::: {#exr-17-1 name="Questions de cours"}
1.  A
2.  B
3.  A
:::

::: {#exr-17-2 name="Estimateur de Nadaraya-Watson"}
Il suffit de dériver la quantité à minimiser par rapport à $\beta_1$ et la valeur de $\beta_1$ qui annule cette dérivée :
$$-2\sum_{i=1}^n(y_i-\beta_1)p_i(x)=0\quad\Longleftrightarrow \quad\widehat\beta_1(x)=\frac{\sum_{i=1}^ny_ip_i(x)}{\sum_{i=1}^np_i(x)}.$$
:::

::: {#exr-17-3 name="Polynômes locaux"}

:::

::: {#exr-17-4 name="Estimateur à noyau uniforme dans $\mathbb R^p$"}
1.  En annulant la dérivée par rapport à $a$, on obtient
$$\widehat m(x)=\frac{\sum_{i=1}^ny_iK\left(\frac{\|x_i-x\|}{h}\right)}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}.$$

2.  On a
$$\V[\widehat m(x)]= \frac{\sigma^2}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}$$
et
$$\E[\widehat m(x)]-m(x)=\frac{\sum_{i=1}^n(m(x_i)-m(x))K\left(\frac{\|x_i-x\|}{h}\right)}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}.$$

3.  On a maintenant $|m(x_i)-m(x)|\leq L\|x_i-x\|$. Or 
$$K\left(\frac{\|x_i-x\|}{h}\right)$$
est non nul si et seulement si $\|x_i-x\|\leq h$. Donc pour tout $i=1,\dots,n$
$$L\|x_i-x\|K\left(\frac{\|x_i-x\|}{h}\right)\leq Lh K\left(\frac{\|x_i-x\|}{h}\right).$$
D'où le résultat.

4.  On a
$$\V[\widehat m(x)]= \frac{\sigma^2}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}=\frac{\sigma^2}{\sum_{i=1}^n\ind_{B_h}(x_i-x)}.$$
Or 
$$\sum_{i=1}^n\ind_{B_h}(x_i-x)\geq C_1n\textrm{Vol}(B_h)\geq C_1\gamma_dnh^d$$
où $\gamma_d=\pi^{d/2}/\Gamma(d/2+1)$. On a donc
$$\V[\widehat m(x)]\leq \frac{\sigma^2}{C_1\gamma_dnh^d}=\frac{C_2\sigma^2}{nh^d}$$
avec $C_2=1/(C_1\gamma_d)$.

5.  On déduit
$$\E[(\widehat m(x)-m(x))^2]\leq L^2h^2+\frac{C_2\sigma^2}{nh^d}.$$

6.  Soit $M(h)$ le majorant ci-dessus. On a
$$M(h)'=2hL^2-\frac{C_2\sigma^2d}{n}h^{-d-1}.$$
La dérivée s'annule pour 
$$h_{opt}=\frac{2L^2}{C_2\sigma^2d}n^{-\frac{1}{d+2}}.$$
Lorsque $h=h_{opt}$ l'erreur quadratique vérifie
$$\E[(\hat m(x)-m(x))^2]=\mathrm{O}\left(n^{-\frac{2}{d+2}}\right).$$
La vitesse diminue lorsque la dimension $d$ augmente, c'est le **fléau de la dimension**.

:::

::: {#exr-17-5 name="Vitesse de la régression univariée en design equi-espacé"}
1.  $\widehat\beta$ minimise $\sum_{i=1}^n(Y_i-\beta x_i)^2$. On a donc
$$\widehat\beta=\frac{\sum_{i=1}^nx_iY_i}{\sum_{i=1}^nx_i^2}.$$

2.  On déduit
$$\E[\widehat\beta]=\beta\quad\textrm{et}\quad\V(\widehat\beta)=\frac{\sigma^2}{\sum_{i=1}^nx_i^2}.$$

3.  Comme 
$$\sum_{i=1}^nx_i^2=\frac{(n+1)(2n+1)}{6n},$$
on obtient le résultat demandé.

:::

::: {#exr-17-6 name="Critère LOO"}
1.  On désigne par $\widehat F_h$ le vecteur $(\widehat f_h(x_i),i=1,\dots,n)$, $\widehat F_k$ le vecteur $(\widehat f_k(x_i),i=1,\dots,n)$ et $\mathbb Y=(y_1,\dots,y_n)$. On voit facilement que
$$\widehat F_h=S_h\mathbb Y\quad\text{et}\quad \widehat F_k=S_k\mathbb Y$$
où $S_h$ et $S_k$ sont des matrices $n\times n$ dont le terme général est défini par
$$S_{ij,h}=\frac{K((x_i-x_j)/h)}{\sum_l K((x_i-x_l)/h)}
\quad\text{et}\quad
S_{ij,k}=
\left\{
\begin{array}{ll}
  1/k&\text{ si $x_j$ est parmi les $k$-ppv de $x_i$} \\
  0 & \text{ sinon}.
  \end{array}
\right.$$


2.  Pour simplifier on note $K_{ij}=K((x_i-x_j)/h)$ On a
$$\widehat f_h^i(x_i)=\frac{\sum_{j\neq i}K_{ij}y_j}{\sum_{j\neq i}K_{ij}}.$$
Par conséquent
$$\widehat f_h^i(x_i)\left[\sum_{j=1}^nK_{ij}-K_{ii}\right]=\sum_{j\neq i}K_{ij}y_j.$$
On obtient le résultat demandé en divisant tout $\sum_{j=1}^nK_{ij}$. Pour l'estimateur de plus proches voisins, on remarque que, si on enlève la $i$ème observation alors l'estimateur des $k$ plus proches voisins de $x_i$ s'obtient à partir ce celui des $k+1$ plus proches voisins avec la $i$ème observation de la façon suivante :
$$\widehat f_k^i(x_i)=\frac{k+1}{k}\sum_{j\neq i}S_{ij,k+1}y_j.$$
On obtient le résultat demandé on observant que $S_{ii,k+1}=1/(k+1)$ et donc
$$\frac{1}{1-S_{ii,k+1}}=\frac{k+1}{k}.$$

3.  On obtient pour l'estimateur à noyau
\begin{align*}
LOO(\widehat f_h)= & \frac{1}{n}\sum_{i=1}^n\left(y_i-\widehat f_h^i(x_i)\right)^2 \\
= & \frac{1}{n}\sum_{i=1}^n\left(\frac{y_i-S_{ii,h}y_i-\sum_{j\neq i}S_{ij,h}y_{j}}{1-S_{ii,h}}\right)^2 \\
= & \frac{1}{n}\sum_{i=1}^n\left(\frac{y_i-\widehat f_h(x_i)}{1-S_{ii,h}}\right)^2.
\end{align*}
Le calcul est similaire pour l'estimateur des plus proches voisins.
:::

::: {#exr-17-7 name="Caret et kppv"}
On importe les données et on ne garde que les deux variables demandées :
```{r}
ozone <- read.table("../donnees/ozone.txt",header=TRUE,sep=";")
df <- ozone[,c("O3","T12")]
```

On construit la grille de plus proches voisins candidats :
```{r}
grille <- data.frame(k=1:40)
```

On indique à **caret** qu'on veut faire de la validation croisée 10 blocs :
```{r}
library(caret)
ctrl <- trainControl(method="cv")
```

On lance la validation croisée avec la fonction **train** :
```{r}
set.seed(1234)
sel.k <- train(O3~.,data=df,method="knn",trControl=ctrl,tuneGrid=grille)
sel.k
```

On sélectionnera 
```{r}
sel.k$bestTune
```
plus proches voisins.

:::

