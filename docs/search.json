[
  {
    "objectID": "correction/chap2.html",
    "href": "correction/chap2.html",
    "title": "2 La régression linéaire multiple",
    "section": "",
    "text": "Exercice 1 (Question de cours) A, A, B, B, B, C.\n\n\nExercice 2 (Covariance de \\(\\hat\\varepsilon\\) et \\(\\hat Y\\)) Nous allons montrer que, pour tout autre estimateur \\(\\tilde{\\beta}\\) de \\(\\beta\\) linéaire et sans biais, \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta})  \\geq \\mathop{\\mathrm{V}}(\\hat \\beta)\\). Décomposons la variance de \\(\\tilde{\\beta}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta})  = \\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta)+\\mathop{\\mathrm{V}}(\\hat \\beta) -\n2 \\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\\] Les variances étant définies positives, si nous montrons que \\(\\mathop{\\mathrm{Cov}}(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0\\), nous aurons fini la démonstration.\\ Puisque \\(\\tilde{\\beta}\\) est linéaire, \\(\\tilde{\\beta} = A Y\\). De plus, nous savons qu’il est sans biais, c’est-à-dire \\(\\mathbf E(\\tilde{\\beta}) = \\beta\\) pour tout \\(\\beta\\), donc \\(A X = I\\). La covariance devient : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=&\n\\mathop{\\mathrm{Cov}}(A Y,(X'X)^{-1}X'Y) - \\mathop{\\mathrm{V}}(\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\\]\n\n\nExercice 3 (Théorème de Gauss Markov) Nous devons montrer que, parmi tous les estimateurs linéaires sans biais, l’estimateur de MC est celui qui a la plus petite variance. La linéarité de \\(\\hat \\beta\\) est évidente. Calculons sa variance : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta) = \\mathop{\\mathrm{V}}((X'X)^{-1}X'Y) =\n(X'X)^{-1}X'\\mathop{\\mathrm{V}}(Y)X(X'X)^{-1}=\\sigma^2 (X'X)^{-1}.\n\\end{eqnarray*}\\] Nous allons montrer que, pour tout autre estimateur \\(\\tilde{\\beta}\\) de \\(\\beta\\) linéaire et sans biais, \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta})  \\geq \\mathop{\\mathrm{V}}(\\hat \\beta)\\). Décomposons la variance de \\(\\tilde{\\beta}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta})  = \\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta)+\\mathop{\\mathrm{V}}(\\hat \\beta) -\n2 \\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\\] Les variances étant définies positives, si nous montrons que \\(\\mathop{\\mathrm{Cov}}(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0\\), nous aurons fini la démonstration.\\ Puisque \\(\\tilde{\\beta}\\) est linéaire, \\(\\tilde{\\beta} = A Y\\). De plus, nous savons qu’il est sans biais, c’est-à-dire \\(\\mathbf E(\\tilde{\\beta}) = \\beta\\) pour tout \\(\\beta\\), donc \\(A X = I\\). La covariance devient : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=&\n\\mathop{\\mathrm{Cov}}(A Y,(X'X)^{-1}X'Y) - \\mathop{\\mathrm{V}}(\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\\]\n\n\nExercice 4 (Représentation des variables) Nous représentons les données dans \\(\\mathbb R^2\\) pour le premier jeu et dans \\(\\mathbb R^3\\) pour le second.\n\n\n\n\n\nDans le premier modèle, nous projetons \\(Y\\) sur l’espace engendré par \\(X\\), soit la droite de vecteur directeur \\(\\overrightarrow{OX}\\). Nous trouvons par le calcul \\(\\hat \\beta = 1.47\\), résultat que nous aurions pu trouver graphiquement car \\(\\overrightarrow{O \\hat Y}=\n\\hat \\beta . \\overrightarrow{OX}\\).\nConsidérons \\(\\mathbb R^3\\) muni de la base orthonormée \\((\\vec{i},\\vec{j},\\vec{k})\\). Les vecteurs \\(\\overrightarrow{OX}\\) et \\(\\overrightarrow{OZ}\\) engendrent le même plan que celui engendré par \\((\\vec{i},\\vec{j})\\). La projection de \\(Y\\) sur ce plan donne \\(\\overrightarrow{O \\hat Y}\\). Il est quasiment impossible de trouver \\(\\hat \\beta\\) et \\(\\hat \\gamma\\) graphiquement mais nous trouvons par le calcul \\(\\hat \\beta = -3.33\\) et \\(\\hat \\gamma =5\\).\n\n\nExercice 5 (Modèles emboîtés) Nous obtenons \\[\\begin{eqnarray*}\n\\hat Y_p = X \\hat \\beta \\quad  \\hbox{et} \\quad \\hat Y_q= X_q \\hat \\gamma.\n\\end{eqnarray*}\\] Par définition du \\(\\mathbb R2\\), il faut comparer la norme au carré des vecteurs \\(\\hat Y_p\\) et \\(\\hat Y_q\\). Notons les espaces engendrés par les colonnes de \\(X_q\\) et \\(X\\), \\(\\mathcal M_{X_q}\\) et \\(\\mathcal M_{X}\\), nous avons \\(\\mathcal M_{X_q} \\subset  \\mathcal M_{X}\\). Nous obtenons alors \\[\\begin{eqnarray*}\n\\hat Y_p = P_{X_p}Y\n= (P_{X_q} + P_{X^{\\perp}_q})P_{X_p}Y &=& P_{X_q}P_{X_p}Y + P_{X^{\\perp}_q}P_{X_p}Y\\\\\n&=& P_{X_q}Y + P_{X^{\\perp}_q \\cap X_p} Y\\\\\n&=& \\hat Y_q + P_{X^{\\perp}_q \\cap X_p} Y.\n\\end{eqnarray*}\\] En utilisant le théorème de Pythagore, nous avons \\[\\begin{eqnarray*}\n\\| \\hat Y_p \\|^2 &=& \\|\\hat Y_q \\|^2 + \\| P_{X^{\\perp}_q \\cap X_p} Y \\|^2\n\\geq \\|\\hat Y_q \\|^2,\n\\end{eqnarray*}\\] d’où \\[\\begin{eqnarray*}\n\\mathbb R2(p)=\\frac{\\| \\hat Y_p \\|^2}{\\| Y \\|^2} \\geq\n\\frac{\\| \\hat Y_q \\|^2}{\\| Y \\|^2} =\\mathbb R2(q).\n\\end{eqnarray*}\\]\nEn conclusion, lorsque les modèles sont emboîtés \\(\\mathcal M_{X_q} \\subset  \\mathcal M_{X}\\), le \\(\\mathbb R2\\) du modèle le plus grand (ayant le plus de variables) sera toujours plus grand que le \\(\\mathbb R2\\) du modèle le plus petit.\n\n\nExercice 6 La matrice \\(X'X\\) est symétrique, \\(n\\) vaut 30 et \\(\\bar x= \\bar z=0\\). Le coefficient de corrélation \\[\\begin{equation*}\n\\rho_{x,z} = \\frac{\\sum_{i=1}^{30} (x_i -\\bar x)(z_i - \\bar z)}\n{\\sqrt{\\sum_{i=1}^{30} (x_i -\\bar x)^2\\sum_{i=1}^{30} (z_i - \\bar z)^2}}\n=\\frac{\\sum_{i=1}^{30} x_i z_i}\n{\\sqrt{\\sum_{i=1}^{30} x_i^2 \\sum_{i=1}^{30} z_i^2}}\n=\\frac{7}{\\sqrt{150}}=0.57.\n\\end{equation*}\\] Nous avons \\[\\begin{eqnarray*}\ny_i &=& -2 +x_i+z_i+\\hat \\varepsilon_i\n\\end{eqnarray*}\\] et la moyenne vaut alors \\[\\begin{eqnarray*}\n\\bar y &=& -2 + \\bar x +\\bar z + \\frac{1}{n}\\sum_i \\hat \\varepsilon_i.\n\\end{eqnarray*}\\] La constante étant dans le modèle, la somme des résidus est nulle car le vecteur \\(\\hat \\varepsilon\\) est orthogonal au vecteur \\(\\mathbf{1}\\). Nous obtenons donc que la moyenne de \\(Y\\) vaut 2 car \\(\\bar x=0\\) et \\(\\bar z=0\\). Nous obtenons en développant \\[\\begin{eqnarray*}\n\\|\\hat Y \\|^2 &=& \\sum_{i=1}^{30}(-2+x_i+2z_i)^2\\\\\n&=& 4+10+60+14=88.\n\\end{eqnarray*}\\] Par le théorème de Pythagore, nous concluons que \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{SCT}}=\\mathop{\\mathrm{SCE}}+\\mathop{\\mathrm{SCR}}=88+12=100.\n\\end{eqnarray*}\\]\n\n\nExercice 7 (Changement d’échelle des variables explicatives) Nous avons l’estimation sur le modèle avec les variables originales qui minimise \\[\\begin{align*}\n\\mathop{\\mathrm{MCO}}(\\beta)&=\\|Y - \\sum_{j=1}^{p} X_j \\beta_j \\|^{2}\n\\end{align*}\\] Cette solution est notée \\(\\hat \\beta\\).\nNous avons l’estimation sur le modèle avec les variables prémultipliées par \\(a_{j}\\) (changement d’échelle) qui minimise\n\\[   \n\\begin{align*}\n\\tilde{\\mathop{\\mathrm{MCO}}}(\\beta)&=\\|Y - \\sum_{j=1}^{p} \\tilde X_j \\beta_j \\|^{2} = \\|Y - \\sum_{j=1}^{p} a_{j} X_j \\beta_j \\|^{2}\\\\\n&= \\|Y - \\sum_{j=1}^{p} X_j \\gamma_j \\|^{2}=\\mathop{\\mathrm{MCO}}(\\gamma),\n\\end{align*}\n\\]\nen posant en dernière ligne \\(\\gamma_{j}=a_{j} \\beta_j\\). La solution de de \\(\\mathop{\\mathrm{MCO}}(\\gamma)\\) (ou encore \\(\\mathop{\\mathrm{MCO}}(\\beta)\\)) est \\(\\hat \\beta\\). La solution de \\(\\tilde{\\mathop{\\mathrm{MCO}}}(\\beta)\\) est alors donnée par \\(\\hat \\beta_{j}=a_{j} \\tilde \\beta_j\\).\n\n\nExercice 8 (Différence entre régression multiple et régressions simples)  \n\nCalculons l’estimateur des MCO noté traditionnellement \\((X'X)^{-1}X'Y\\) avec la matrice \\(X\\) qui possède ici deux colonnes (notées ici \\(X\\) et \\(Z\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  (X'X)&=\n         \\begin{pmatrix}\n           \\|X\\|^{2} & &lt;X,Z&gt;\\\\\n           &lt;X,Z&gt; & \\|Z\\|^{2} \\\\\n         \\end{pmatrix}\n\\end{align*}\\] Son déterminant est \\(\\Delta= \\|X\\|^{2}\\|Z\\|^{2} - 2 &lt;X,Z&gt;\\) et son inverse est \\[\\begin{align*}\n  \\frac{1}{\\Delta}\n  \\begin{pmatrix}\n           \\|Z\\|^{2} & -&lt;X,Z&gt;\\\\\n           -&lt;X,Z&gt; & \\|X\\|^{2} \\\\\n  \\end{pmatrix}\n\\end{align*}\\] Ensuite \\(X'Y\\) est simplement le vecteur colonne de coordonnées \\(&lt;X,Y&gt;\\) et \\(&lt;Z,Y&gt;\\). En rassemblant le tout nous avons \\[\\begin{align*}\n  \\hat \\beta_{1}&=\\frac{1}{\\Delta}(\\|Z\\|^{2} &lt;X,Y&gt; - &lt;X,Z&gt;&lt;Z,Y&gt;),\\\\\n  \\hat \\beta_{2}&=\\frac{1}{\\Delta}(\\|X\\|^{2} &lt;Z,Y&gt; - &lt;X,Z&gt;&lt;X,Y&gt;).\n\\end{align*}\\] Si \\(&lt;X,Z&gt;=0\\) (les deux vecteurs sont orthogonaux) alors cette écriture se simplifie en \\[\n  \\hat \\beta_{1}=\\frac{&lt;X,Y&gt;}{\\|X\\|^{2}},\\quad\n  \\hat \\beta_{2}=\\frac{&lt;Z,Y&gt;}{\\|Z\\|^{2}}.\n\\]\nCalculons l’estimateur des MCO noté traditionnellement \\((X'X)^{-1}X'Y\\) avec la matrice \\(X\\) qui possède ici une colonne (notée ici \\(X\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  \\hat \\beta_{X} = \\frac{&lt;X,Y&gt;}{\\|X\\|^{2}}.\n\\end{align*}\\] Passons maintenant à la matrice qui possède ici une colonne (notée ici \\(Z\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  \\hat \\beta_{Z} = \\frac{&lt;Z,Y&gt;}{\\|Z\\|^{2}}.\n\\end{align*}\\]\nEn général les coefficients des régressions simples ne sont pas ceux obtenus par régression multiple sauf si les variables sont orthogonales.\nNous avons ici les résidus de la première régression qui sont \\[\\begin{align}\n  \\hat \\varepsilon = Y - \\hat \\beta_{X} X.\n\\end{align}\\] La deuxième régression (sur les résidus) donne le coefficient \\[\\begin{align*}\n  \\hat \\beta_{Z} = \\frac{&lt;Z,\\hat \\varepsilon&gt;}{\\|Z\\|^{2}} = \\hat \\beta_{Z} - \\hat \\beta_{X}\\frac{&lt;Z,X&gt;}{\\|Z\\|^{2}}\n\\end{align*}\\] La régression séquentielle donne des coefficients différents des régressions univariées ou bivariées sauf si les variables sont orthogonales. \\end{enumerate}\n\n\n\nExercice 9 (TP : différence entre régression multiple et régressions simples)  \n\nSimulons 2 variables explicatives avec GNU-R pour \\(n=100\\) individus selon selon deux loi uniforme \\([0,1]\\) :\n\nn &lt;- 100\nset.seed(4321) # pour fixer les simulations\nX1 &lt;- runif(n)\nX2 &lt;- runif(n)\n\nEnsuite simulons \\(Y\\) selon le modèle avec \\(\\sigma=0.5\\)\n\nsigma &lt;- 0.5\nset.seed(321) # pour fixer les simulations\nY &lt;- 2 -3*X1 + 4*X2  + rnorm(n, sd=sigma)\ndon &lt;- cbind.data.frame(Y,X1,X2)\nhead(don)\n\n         Y         X1        X2\n1 4.213477 0.33477802 0.5913398\n2 1.602748 0.90913948 0.6715464\n3 2.958646 0.41152969 0.5830570\n4 3.215113 0.04384097 0.3516151\n5 3.367004 0.76350011 0.9298713\n6 3.555247 0.75043889 0.9181180\n\n\nLe graphique est obtenu avec\n\nrgl::plot3d(X1,X2,Y)\n\nOn observe grâce au code ci-dessus que les points sont répartis autour d’un plan (d’équation \\(z=2-3x + 4y\\)).\nEffectuons la régression multiple et stockons \\(\\hat Y\\) dans Yhat :\n\n(regmult &lt;- lm(Y~1+X1+X2, data=don))\n\n\nCall:\nlm(formula = Y ~ 1 + X1 + X2, data = don)\n\nCoefficients:\n(Intercept)           X1           X2  \n      2.080       -2.972        3.827  \n\n\nNous sommes assez proches des coefficients recherchés (2, -3 et 4)\nEffectuons les régression simples\n\n(regX1 &lt;- lm(Y~1+X1, data=don))\n\n\nCall:\nlm(formula = Y ~ 1 + X1, data = don)\n\nCoefficients:\n(Intercept)           X1  \n      3.803       -2.441  \n\n\nNous voyons que le paramètre ne correspond pas à celui de \\(X_{1}\\) dans la régression multiple.\n\n(regX2 &lt;- lm(Y~1+X2, data=don))\n\n\nCall:\nlm(formula = Y ~ 1 + X2, data = don)\n\nCoefficients:\n(Intercept)           X2  \n     0.7221       3.4424  \n\n\nNous voyons que le paramètre ne correspond pas à celui de \\(X_{2}\\) dans la régression multiple.\nEnchainons après la régression simple sur \\(X_{1}\\) la régression des résidus (ce que \\(X_{1}\\) n’a pas réussi à modéliser) sur \\(X_{2}\\):\n\nresidX1 &lt;- residuals(regX1)\ndon &lt;- cbind(don, residX1)\nregX2residX1 &lt;- lm(residX1~1+X2, data=don)\nregX2residX1\n\n\nCall:\nlm(formula = residX1 ~ 1 + X2, data = don)\n\nCoefficients:\n(Intercept)           X2  \n     -1.965        3.758  \n\n\nLà encore le coefficient sur \\(X_{2}\\) ne correspond pas à celui de \\(X_{2}\\) dans la régression multiple. :La prévision est différente\n\n(predict(regX2residX1) + predict(regX1))[1:5]\n\n       1        2        3        4        5 \n3.242781 2.142450 3.024335 3.051894 3.468733 \n\n\nChangeons l’ordre\n\nresidX2 &lt;- residuals(regX2)\ndon &lt;- cbind(don, residX2)\nregX1residX2 &lt;- lm(residX2~1+X1, data=don)\nregX1residX2\n\n\nCall:\nlm(formula = residX2 ~ 1 + X1, data = don)\n\nCoefficients:\n(Intercept)           X1  \n      1.531       -2.918  \n\n\nOn a là encore des différences\n\n(predict(regX1residX2) + predict(regX2))[1:5]\n\n       1        2        3        4        5 \n3.311895 1.911850 3.059400 3.335703 3.226125 \n\n\nEnvisageons la covariance empirique\n\ncov(X1,X2)    \n\n[1] 0.01050429\n\n\net leur produit scalaire\n\nsum(X1*X2)\n\n[1] 28.47696\n\n\nLes deux variables sont loin d’être orthogonales. Centrons les\n\nX1c &lt;- X1 - mean(X1)\nX2c &lt;- X2 - mean(X2)\nsum(X1c*X2c) \n\n[1] 1.039925\n\n\nRefaisons les régressions: la multiple\n\ndonc &lt;- cbind.data.frame(Y,X1c,X2c)\n(regmultc &lt;- lm(Y~1+X1c+X2c, data=donc))\n\n\nCall:\nlm(formula = Y ~ 1 + X1c + X2c, data = donc)\n\nCoefficients:\n(Intercept)          X1c          X2c  \n      2.522       -2.972        3.827  \n\n\nNous retrouvons que seul la constante change puis l’enchainement\n\n(regX1c &lt;- lm(Y~1+X1c, data=donc))\n\n\nCall:\nlm(formula = Y ~ 1 + X1c, data = donc)\n\nCoefficients:\n(Intercept)          X1c  \n      2.522       -2.441  \n\n\nIci les variables \\(X_{1c}\\) et \\(X_{2c}\\) étant centrées elles sont orthogonales à la constante. Nous avons donc \\[\\begin{align}\n      \\Im(\\mathbf{1}, X_{1c}, X_{2c}) = \\Im(\\mathbf{1}) \\stackrel{\\perp}{\\oplus} \\Im(X_{1c}, X_{2c})\n    \\end{align}\\] Le modèle précédent nous donne grâce à l’orthogonalité ci-dessus \\(\\hat Y_{1c} = P_{\\mathbf{1}, X_{1c}}Y=P_{\\mathbf{1}}Y + P_{X_{1c}}Y\\). Le modèle complet donne de son côté \\(\\hat Y = P_{\\mathbf{1}, X_{1c},X_{2c}}Y=P_{\\mathbf{1}}Y + P_{X_{1c},X_{2c}}Y\\). Nous retrouvons donc le coefficient constant qui est la coordonnée sur \\(\\mathbf{1}\\) de \\(P_{\\mathbf{1}}Y\\) dans les deux cas.\nEnchainons sur la régression sur résidus\n\nresidX1c &lt;- residuals(regX1c)\ndonc &lt;- cbind(donc, residX1c)\n(regX2residX1c &lt;- lm(residX1c~1+X2c, data=don))\n\n\nCall:\nlm(formula = residX1c ~ 1 + X2c, data = don)\n\nCoefficients:\n(Intercept)          X2c  \n  1.669e-16    3.758e+00  \n\n\nPuisque le vecteur est \\(Y- \\hat Y_{1}=Y-P_{\\mathbf{1}}Y-P_{X_{1c}}Y\\) et que \\(X_{2c}\\perp\\mathbf{1}\\) on a la projection des résidus sur \\(\\Im(\\mathbf{1}, X_{2c})\\) qui vaut \\[\\begin{align*}\n  P_{\\mathbf{1}, X_{2c}}(Y- \\hat Y_{1c})&= P_{\\mathbf{1}}(Y- \\hat Y_{1c}) + P_{X_{2c}}(Y- \\hat Y_{1c})\\\\\n  &=P_{\\mathbf{1}}Y - P_{\\mathbf{1}}P_{\\mathbf{1}, X_{1c}}Y + P_{X_{2c}}Y -  P_{X_{2c}}P_{\\mathbf{1}, X_{1c}}Y\\\\\n  &= P_{\\mathbf{1}}Y - P_{\\mathbf{1}}Y - 0 + P_{X_{2c}}Y - 0 - P_{X_{2c}}P_{X_{1c}}Y\\\\\n  &= P_{X_{2c}} Y - P_{X_{2c}}P_{X_{1c}}Y\n\\end{align*}\\] Cette dernière somme de projection est dans \\(\\Im(X_{1c}, X_{2c})\\) qui est un sous-espace orthogonal à \\(\\Im(\\mathbf{1})\\), d’où le coefficient 0 pour la constante.\nPour les prévisions rien ne change :\n\npredict(regmultc)[1:5]\n\n       1        2        3        4        5 \n3.348336 1.948483 3.088559 3.295485 3.369866 \n\n\n\n\n\nExercice 10 (TP : régression multiple et code R)  \n\n\nozone &lt;- read.table(\"../donnees/ozone_complet.txt\",sep=\";\",header=TRUE)\nnomvar &lt;- names(ozone)\n\n\n(ch &lt;- paste(nomvar[-1],collapse = \"+\"))\n\n[1] \"T6+T9+T12+T15+T18+Ne6+Ne9+Ne12+Ne15+Ne18+Vdir6+Vvit6+Vdir9+Vvit9+Vdir12+Vvit12+Vdir15+Vvit15+Vdir18+Vvit18+Vx+maxO3v\"\n\n\n\n(ch2 &lt;- paste(\"maxO3~\",ch,sep=\"\"))\n\n[1] \"maxO3~T6+T9+T12+T15+T18+Ne6+Ne9+Ne12+Ne15+Ne18+Vdir6+Vvit6+Vdir9+Vvit9+Vdir12+Vvit12+Vdir15+Vvit15+Vdir18+Vvit18+Vx+maxO3v\"\n\n\n\nform &lt;- formula(ch2)\nlm(form,data=ozone)\n\n\nCall:\nlm(formula = form, data = ozone)\n\nCoefficients:\n(Intercept)           T6           T9          T12          T15          T18  \n  31.628418    -1.937194     0.121834     1.522143     0.609047     0.091871  \n        Ne6          Ne9         Ne12         Ne15         Ne18        Vdir6  \n   0.091969    -0.692891    -0.941925    -0.033847    -0.107068    -0.001344  \n      Vvit6        Vdir9        Vvit9       Vdir12       Vvit12       Vdir15  \n   1.586427    -0.009728    -1.163728    -0.007144     0.182232    -0.003468  \n     Vvit15       Vdir18       Vvit18           Vx       maxO3v  \n   0.246885     0.006251     0.560660     0.255474     0.465541  \n\n\n\n\n\nExercice 11 (Régression orthogonale) Les vecteurs étant orthogonaux, nous avons \\(\\mathcal M_X = \\mathcal M_U \\stackrel{\\perp}{\\oplus} \\mathcal M_V\\). Nous pouvons alors écrire \\[\\begin{eqnarray*}\n\\hat Y_X = P_X Y &=& (P_U + P_{U^{\\perp}})P_X Y \\\\\n&=& P_U P_X Y + P_{U^{\\perp}}P_X Y =  P_U Y + P_{U^{\\perp}\\cap X} Y \\\\\n&=& \\hat Y_U + \\hat Y_V.\n\\end{eqnarray*}\\] La suite de l’exercice est identique. En conclusion, effectuer une régression multiple sur des variables orthogonales revient à effectuer \\(p\\) régressions simples.\n\n\nExercice 12 (Centrage, centrage-réduction et coefficient constant)  \n\nComme la dernière colonne de \\(X\\), notée \\(X_p\\) vaut \\(\\mathbf{1}_n\\) sa moyenne empirique vaut \\(1\\) et la variable centrée issue de \\(X_p\\) est donc \\(X_p -1\\times\\mathbf{1}_n=\\boldsymbol{0}_n\\).\nNous avons le modèle sur variable centrée \\[\n\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\varepsilon\\\\\nY-\\bar Y\\mathbf{1}_n&=&\\sum_{j=1}^{p-1}{(X_j -\\bar X_j\\mathbf{1}_n)\\tilde\\beta_j}+\\varepsilon\\\\\nY&=&\\sum_{j=1}^{p-1}{\\tilde\\beta_j X_j}+ \\Bigl(\\bar Y -\\sum_{j=1}^{p-1}{\\bar X_j\\tilde\\beta_j}\\Bigr)\\mathbf{1}_n+\\varepsilon.\n\\end{eqnarray*}\n\\] En identifiant cela donne \\[\n\\begin{eqnarray}\n\\beta_j&=&\\tilde\\beta_j,\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\nonumber\\\\\n\\beta_p&=&\\bar Y\\mathbf{1}_n-\\sum_{j=1}^{p-1}{\\bar X_j\\tilde\\beta_j}.\n\\end{eqnarray}\n\\tag{1}\\] Si l’on utilise des variables centrées dans le modèle de régression, on ne met pas de colonne \\(\\mathbf{1}\\) (pas de coefficient constant - intercept). Les coefficients du modèle sur les variables originales sont égaux à ceux sur les variables centrées et le coefficient constant est donné par la formule (équation 1).\nMaintenant les variables explicatives sont centrées et réduites : \\[\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\varepsilon\\\\\nY-\\bar Y\\mathbf{1}_n&=&\\sum_{j=1}^{p-1}{\\frac{(X_j -\\bar X_j\\mathbf{1}_n)}{\\hat\\sigma_{X_j}}\\tilde\\beta_j}+\\varepsilon\\\\\nY&=&\\sum_{j=1}^{p-1}{\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}X_j}+ \\Bigl(\\bar Y-\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}\\Bigr)\\mathbf{1}_n+\\varepsilon.\n\\end{eqnarray*}\\] En identifiant cela donne \\[\\begin{eqnarray*}\n\\beta_j&=&\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}},\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\\\\n\\beta_p&=&\\bar Y\\mathbf{1}_n-\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}.\n\\end{eqnarray*}\\] Nous obtenons ici que les coefficients du modèle sur les variables originales sont égaux à ceux sur les variables centrées-réduites divisés par l’écart-type empirique des variables explicatives. Plus la variable explicative \\(X_j\\) est dispersée, plus son coefficient \\(\\beta_j\\) sera réduit par rapport à \\(\\tilde\\beta_j\\). Le coefficient constant est donné par la formule ci-dessus.\nLa variable à expliquer \\(Y\\) est elle aussi centrée-réduite~: \\[\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\tilde\\varepsilon\\\\\n\\frac{Y-\\bar Y\\mathbf{1}_n}{\\hat\\sigma_Y}&=&\\sum_{j=1}^{p-1}{\\frac{(X_j -\\bar X_j\\mathbf{1}_n)}{\\hat\\sigma_{X_j}}\\tilde\\beta_j}+\\tilde\\varepsilon\\\\\nY&=&\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}X_j}+ \\Bigl(\\bar Y-\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}\\Bigr)\\mathbf{1}_n+\\hat\\sigma_Y\\tilde\\varepsilon.\n\\end{eqnarray*}\\] En identifiant cela donne \\[\\begin{eqnarray*}\n\\beta_j&=&\\hat\\sigma_Y\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}},\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\\\\n\\beta_p&=&\\bar Y\\mathbf{1}_n-\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}},\\\\\n\\varepsilon&=&\\hat\\sigma_Y\\tilde\\varepsilon.\n\\end{eqnarray*}\\] L’écart-type empirique de \\(Y\\) entre en jeu et nous constatons que les résidus du modèle “centré-réduit” sont égaux à ceux initiaux divisés par l’écart-type empirique de \\(Y\\).\nSimulons 3 variables explicatives avec GNU-R pour \\(n=100\\) individus selon une loi uniforme \\([0,1]\\) et équirépartie sur \\([1;10]\\):\n\nn &lt;- 100\nset.seed(1234) # pour fixer les simulations\nX1 &lt;- runif(n)\nX2 &lt;- seq(1, 10, length=100)\nX3 &lt;- rep(1,n)\nX &lt;- cbind(X1, X2, X3)\nhead(X)\n\n            X1       X2 X3\n[1,] 0.1137034 1.000000  1\n[2,] 0.6222994 1.090909  1\n[3,] 0.6092747 1.181818  1\n[4,] 0.6233794 1.272727  1\n[5,] 0.8609154 1.363636  1\n[6,] 0.6403106 1.454545  1\n\n\nSimulons maintenant \\(Y\\) selon le modèle en fixant par exemple \\(\\beta=(1, 2, 0.1, -4)'\\) et \\(\\sigma=0.1\\)\n\nbeta &lt;- c(1, 0.1, -4)\nsigma &lt;- 0.1\nset.seed(12345) # pour fixer les simulations\nY &lt;- X%*%beta + rnorm(n, sd=sigma)\nY[1:5]\n\n[1] -3.727744 -3.197663 -3.283474 -3.294698 -2.942132\n\n\nCentrons les variables avec \n\nYtilde &lt;- scale(Y, scale = FALSE)\nYtilde[1:5]\n\n[1] -0.73976069 -0.20968007 -0.29549076 -0.30671453  0.04585078\n\nXtilde &lt;- scale(X, scale = FALSE)\nhead(Xtilde)\n\n             X1        X2 X3\n[1,] -0.3237939 -4.500000  0\n[2,]  0.1848021 -4.409091  0\n[3,]  0.1717775 -4.318182  0\n[4,]  0.1858822 -4.227273  0\n[5,]  0.4234181 -4.136364  0\n[6,]  0.2028133 -4.045455  0\n\n\nEt éliminons la dernière colonne de \\(\\tilde X\\)\n\nXtilde &lt;- Xtilde[, -3]\n\nEffectuons les deux régressions en constituant deux data-frames. Nous n’ajoutons pas la constante dans les modèles (-1) car elle est déjà dans la dernière colonne de \\(X\\) pour le premier modèle et on ne la souhaite pas pour le second.\n\ndon &lt;- cbind.data.frame(Y,X)\ndontilde &lt;- cbind.data.frame(Ytilde,Xtilde)\nnames(dontilde) &lt;- c(\"Ytilde\", \"X1tilde\", \"X2tilde\")\nmod &lt;- lm(Y~-1+X1+X2+X3, data=don)\nmodtilde &lt;- lm(Ytilde~-1+X1tilde+X2tilde, data=dontilde)\n\nOn retrouve bien que les coefficients des variables centrées ou non sont identiques\n\ncoef(mod)\n\n        X1         X2         X3 \n 1.0225207  0.1030809 -4.0022782 \n\n\net que l’intercept est simplement est bien celui donné dans la formule (équation 1)\n\nmean(don$Y)- sum( coef(modtilde)*colMeans(X[,1:2]))\n\n[1] -4.002278\n\n\nCentrons et réduisons les variables explicatives (les 2 premières colonnes de \\(X\\))\n\nXtilde &lt;- scale(X[, 1:2], scale = TRUE)\nhead(Xtilde)\n\n             X1        X2\n[1,] -1.1617874 -1.706220\n[2,]  0.6630787 -1.671751\n[3,]  0.6163456 -1.637282\n[4,]  0.6669539 -1.602813\n[5,]  1.5192439 -1.568344\n[6,]  0.7277037 -1.533875\n\n\nReconstuisons le data-frame qui a changé et le modèle correspondant\n\ndontilde &lt;- cbind.data.frame(Ytilde,Xtilde)\nnames(dontilde) &lt;- c(\"Ytilde\", \"X1tilde\", \"X2tilde\")\nmodtilde &lt;- lm(Ytilde~-1+X1tilde+X2tilde, data=dontilde)\n\nRetrouvons les coefficients de départ pour les variables explicatives et pour la constante\n\nmean(don$Y)- sum( coef(modtilde)/apply(X[,-3], 2, sd) *\n                  colMeans(X[,1:2]) )\n\n[1] -4.002278\n\n\nCentrons et réduisons les variables explicatives et la variable à expliquer\n\nYtilde &lt;- scale(Y, scale = TRUE)\nYtilde[1:5]\n\n[1] -1.8943177 -0.5369313 -0.7566682 -0.7854091  0.1174109\n\n\nReconstuisons le data frame qui a changé et le modèle correspondant\n\ndontilde &lt;- cbind.data.frame(Ytilde, Xtilde)\nnames(dontilde) &lt;- c(\"Ytilde\", \"X1tilde\", \"X2tilde\")\nmodtilde &lt;- lm(Ytilde~-1+X1tilde+X2tilde, data=dontilde)\n\nRetrouvons les coefficients de départ pour les variables explicatives\n\ncoef(modtilde)/apply(X[,-3],2,sd)*sd(Y)\n\n  X1tilde   X2tilde \n1.0225207 0.1030809 \n\n\net pour la constante\n\nmean(don$Y)- sum( coef(modtilde)/apply(X[,-3], 2, sd) *\n                  sd(Y)*colMeans(X[, 1:2]) )\n\n[1] -4.002278\n\n\n\n\n\nExercice 13 (Moindres carrés contraints)  \n\nL’estimateur des MC vaut \\[\\begin{eqnarray*}\n\\hat \\beta = (X'X)^{-1}X'Y,\n\\end{eqnarray*}\\]\nCalculons maintenant l’estimateur contraint. Nous pouvons procéder de deux manières différentes.\nLa première consiste à écrire le lagrangien \\[\\begin{eqnarray*}\n\\mathcal{L} = S(\\beta) - \\lambda'(R\\beta-r).\n\\end{eqnarray*}\\] Les conditions de Lagrange permettent d’obtenir un minimum \\[\\begin{eqnarray*}\n\\left\\{\n\\begin{array}{l}\n\\displaystyle\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = -2X'Y+2X'X\\hat{\\beta}_c-\nR'\\hat{\\lambda}=0,\\\\\n\\displaystyle\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = R\\hat{\\beta}_c-r=0,\n\\end{array}\n\\right.\n\\end{eqnarray*}\\] Multiplions à gauche la première égalité par \\(R(X'X)^{-1}\\), nous obtenons \\[\\begin{eqnarray*}\n-2 R(X'X)^{-1}X'Y+2R(X'X)^{-1}X'X \\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2R\\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2r-R(X'X)^{-1}R'\\hat{\\lambda}&=&0.\n\\end{eqnarray*}\\] Nous obtenons alors pour \\(\\hat \\lambda\\) \\[\\begin{eqnarray*}\n\\hat \\lambda = 2 \\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right].\n\\end{eqnarray*}\\] Remplaçons ensuite \\(\\hat \\lambda\\) \\[\\begin{eqnarray*}\n-2X'Y+2X'X\\hat{\\beta}_c-R'\\hat{\\lambda}&=&0\\\\\n-2X'Y+2X'X\\hat{\\beta}_c-2R'\\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right]&=& 0,\n\\end{eqnarray*}\\] d’où nous calculons \\(\\hat \\beta_c\\) \\[\\begin{eqnarray*}\n\\hat \\beta_c &=& (X'X)^{-1}X'Y+(X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}\n(r-R\\hat \\beta)\\\\\n&=& \\hat \\beta + (X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\\] La fonction \\(S(\\beta)\\) à minimiser est une fonction convexe sur un ensemble convexe (contraintes linéaires), le minimum est donc unique.\nUne autre façon de procéder consiste à utiliser les projecteurs. Supposons pour commencer que \\(r=0\\), la contrainte vaut donc \\(R\\beta=0\\). Calculons analytiquement le projecteur orthogonal sur \\(\\mathcal M_0\\). Rappelons que \\(\\dim(\\mathcal M_0)=p-q\\), nous avons de plus \\[\\begin{eqnarray*}\nR \\beta &=& 0 \\quad \\quad\n\\Leftrightarrow \\quad \\beta \\in Ker(R)\\\\\nR (X'X)^{-1}X'X \\beta &=& 0\\\\\nU' X \\beta &=& 0\\quad \\quad \\hbox{où} \\quad \\quad U = X (X'X)^{-1}R'.\n\\end{eqnarray*}\\] Nous avons donc que \\(\\forall \\beta \\in \\ker(R)\\), \\(U' X \\beta = 0\\), c’est-à-dire que \\(\\mathcal M_U\\), l’espace engendré par les colonnes de \\(U\\), est orthogonal à l’espace engendré par \\(X\\beta\\), \\(\\forall \\beta \\in \\ker(R)\\). Nous avons donc que \\(\\mathcal M_U \\perp \\mathcal M_0\\). Comme \\(U=X[(X'X)^{-1}R']\\), \\(\\mathcal M_U \\subset \\mathcal M_X\\). En résumé, nous avons \\[\\begin{eqnarray*}\n\\mathcal M_U \\subset \\mathcal M_X \\quad  \\hbox{et}\n\\quad  \\mathcal M_U \\perp \\mathcal M_0 \\quad \\hbox{donc}\n\\quad \\mathcal M_U \\subset  (\\mathcal M_X \\cap \\mathcal M_0^{\\perp}).\n\\end{eqnarray*}\\] Afin de montrer que les colonnes de \\(U\\) engendrent \\(\\mathcal M_X \\cap \\mathcal M_0^{\\perp}\\), il faut démontrer que la dimension des deux sous-espaces est égale. Or le rang de \\(U\\) vaut \\(q\\) (\\(R'\\) est de rang \\(q\\), \\((X'X)^{-1}\\) est de rang \\(p\\) et \\(X\\) est de rang \\(p\\)) donc la dimension de \\(\\mathcal M_U\\) vaut \\(q\\). De plus, nous avons vu que \\[\\begin{eqnarray*}\n\\mathcal M_X = \\mathcal M_0 \\stackrel{\\perp}{\\oplus}\\left(\\mathcal M_0^{\\perp} \\cap \\mathcal M_X \\right)\n\\end{eqnarray*}\\] et donc, en passant aux dimensions des sous-espaces, nous en déduisons que \\(\\dim(\\mathcal M_0^{\\perp} \\cap \\mathcal M_X )=q\\). Nous venons de démontrer que \\[\\begin{eqnarray*}\n\\mathcal M_U = \\mathcal M_X \\cap \\mathcal M_0^{\\perp}.\n\\end{eqnarray*}\\] Le projecteur orthogonal sur \\(\\mathcal M_U=\\mathcal M_X \\cap \\mathcal M_0^{\\perp}\\) s’écrit \\[\\begin{eqnarray*}\nP_{U} = U (U'U)^{-1} U'= X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'.\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\hat Y - \\hat Y_0 &=& P_U Y\\\\\nX \\hat \\beta - X \\hat \\beta_0 &=& X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'Y\\\\\n&=& X (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\\] Cela donne \\[\\begin{eqnarray*}\n\\hat \\beta_0 = \\hat \\beta - (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\\] Si maintenant \\(r\\neq 0\\), nous avons alors un sous-espace affine défini par \\(\\{\\beta\\in \\mathbb R^p : R\\beta=r\\}\\) dans lequel nous cherchons une solution qui minimise les moindres carrés. Un sous-espace affine peut être défini de manière équivalente par un point particulier \\(\\beta_p \\in \\mathbb R^p\\) tel que \\(R\\beta_p=r\\) et le sous-espace vectoriel associé \\(\\mathcal M_0^v=\\{\\beta\\in \\mathbb R^p : R\\beta=0\\}\\). Les points du sous-espace affine sont alors \\(\\{\\beta_0 \\in \\mathbb R^p : \\beta_0=\\beta_p+\\beta_0^v, \\beta_0^v \\in \\mathcal M_0^v\n\\quad et \\quad \\beta_p : R\\beta_p=r\\}\\). La solution qui minimise les moindres carrés, notée \\(\\hat \\beta_0\\), est élément de ce sous-espace affine et est définie par \\(\\hat \\beta_0=\\beta_p+\\hat \\beta_0^v\\) où \\[\\begin{eqnarray*}\n\\hat \\beta_0^v = \\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta.\n\\end{eqnarray*}\\] Nous savons que \\(R\\beta_p=r\\) donc \\[\\begin{eqnarray*}\nR\\beta_p = [R(X'X)^{-1}R'][R(X'X)^{-1}R']^{-1}r\n\\end{eqnarray*}\\] donc une solution particulière est \\(\\beta_p = (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r\\). La solution \\(\\hat \\beta_0\\) qui minimise les moindres carrés sous la contrainte \\(R\\beta=r\\) est alors \\[\n\\begin{align}\n\\hat \\beta_0 &= \\beta_p+\\hat \\beta_0^v\\\\\\nonumber\n&=(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r +\n\\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta\\\\\\nonumber\n&=\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{align}\n\\tag{2}\\]\nCalculons l’EQM de \\(\\hat \\beta\\) qui vaut selon la formule classique: \\[\\begin{align*}\n      \\mathop{\\mathrm{EQM}}&= (\\mathbf E(\\hat \\beta) - \\beta) (\\mathbf E(\\hat \\beta) - \\beta)' + \\mathop{\\mathrm{V}}(\\hat\\beta)\n    \\end{align*}\\] avec \\(\\mathbf E(\\hat \\beta)=\\beta\\) (sous l’hypothèse que \\(Y\\) est généré par le modèle de régression) et\\(\\mathop{\\mathrm{V}}(\\hat\\beta) =\\sigma^{2} (X'X)^{-1}\\).\nPour l’EQM de \\(\\hat \\beta_0\\) qui vaut selon la formule classique: \\[\\begin{align*}\n      \\mathop{\\mathrm{EQM}}&= (\\mathbf E(\\hat \\beta_0) - \\beta_0) (\\mathbf E(\\hat \\beta_0) - \\beta_0)' + \\mathop{\\mathrm{V}}(\\hat\\beta_0)\n    \\end{align*}\\] calculons d’abord \\(\\mathbf E(\\hat \\beta_0)\\) en utilisant l’équation (équation 2): \\[\\begin{align*}\n      \\mathbf E(\\hat \\beta_0)&=\\mathbf E(\\hat \\beta) + \\mathbf E[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n      &=\\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\beta)\n    \\end{align*}\\] puisque \\(\\hat \\beta\\) est sans biais. Si nous supposons que le modèle satisfait la contrainte (\\(R\\beta=r\\)) alors là encore le biais est nul.\nCalculons maintemant la variance: \\[\\begin{align*}\n      \\mathop{\\mathrm{V}}(\\hat\\beta_0) &=  \\mathop{\\mathrm{V}}[\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n                      &=\\mathop{\\mathrm{V}}(\\hat\\beta) + \\mathop{\\mathrm{V}}[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)] +\\\\\n                      &\\quad \\quad 2\\mathop{\\mathrm{Cov}}[\\hat\\beta ; (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n      &=   \\sigma^{2}V_{X}^{-1} + (\\mathrm{II}) +  (\\mathrm{III})\n    \\end{align*}\\] Intéressons nous à la seconde partie \\((\\mathrm{II})\\). Comme \\(X\\) est déterministe ainsi que \\(R\\) et \\(r\\) on a en posant pour alléger \\(V_{X}=(X'X)\\) (matrice symétrique) \\[\\begin{align*}\n      (\\mathrm{II}) &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\mathop{\\mathrm{V}}(r-R\\hat \\beta)\n                        [RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                    &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\mathop{\\mathrm{V}}(R\\hat \\beta)[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                    &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R\\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n       &= \\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\n    \\end{align*}\\] La troisième partie est \\[\\begin{align*}\n      (\\mathrm{III}) &= 2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}\\mathop{\\mathrm{Cov}}[\\hat\\beta ; (r-R\\hat \\beta)]\\\\\n                     &=-2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\mathop{\\mathrm{Cov}}[\\hat\\beta ;\\hat\\beta]\\\\\n      &= -2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\sigma^{2}V_{X}^{-1}\n    \\end{align*}\\] Ce qui donne au final \\[\\begin{align*}\n      \\mathop{\\mathrm{V}}(\\hat\\beta_0) &=\\sigma^{2}(X'X)^{-1} - \\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}\n    \\end{align*}\\] L’écart entre les 2 variances est donc de \\(\\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}\\) qui est une matrice de la forme \\(A'A\\) donc semi-définie positive.",
    "crumbs": [
      "Correction des exercices",
      "I Introduction au modèle linéaire",
      "2 La régression linéaire multiple"
    ]
  },
  {
    "objectID": "correction/chap11.html",
    "href": "correction/chap11.html",
    "title": "11 Régression logistique",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nA\nA\nB\nA\nA\nA\nB\nA\n\n\n\nExercice 2 (Interprétation des coefficients)  \n\nOn génère l’échantillon.\n\nn &lt;- 100\nset.seed(48967365)\nX &lt;- sample(c(\"A\",\"B\",\"C\"),100,replace=TRUE)\nY &lt;- rep(0,n)\nset.seed(487365)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),size=1,prob=0.95)\nset.seed(4878365)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),size=1,prob=0.95)\nset.seed(4653965)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),size=1,prob=0.05)\nY &lt;- factor(Y)\ndonnees&lt;-data.frame(Y,X)\n\nOn ajuste le modèle avec les contraintes par défaut.\n\nmodel1 &lt;- glm(Y~X,data=donnees,family=binomial)\nsummary(model1)\n\n\nCall:\nglm(formula = Y ~ X, family = binomial, data = donnees)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.2336     0.6075   3.677 0.000236 ***\nXB            0.6568     0.9470   0.694 0.487977    \nXC           -5.6348     1.1842  -4.758 1.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.489  on 99  degrees of freedom\nResidual deviance:  44.218  on 97  degrees of freedom\nAIC: 50.218\n\nNumber of Fisher Scoring iterations: 6\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_2\\) et \\(\\beta_3\\).\nOn change la modalité de référence.\n\nmodel2 &lt;- glm(Y~C(X,base=3),data=donnees,family=binomial)\nsummary(model2)\n\n\nCall:\nglm(formula = Y ~ C(X, base = 3), family = binomial, data = donnees)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -3.401      1.017  -3.346  0.00082 ***\nC(X, base = 3)A    5.635      1.184   4.758 1.95e-06 ***\nC(X, base = 3)B    6.292      1.249   5.035 4.77e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.489  on 99  degrees of freedom\nResidual deviance:  44.218  on 97  degrees of freedom\nAIC: 50.218\n\nNumber of Fisher Scoring iterations: 6\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_1\\) et \\(\\beta_2\\).\nOn remarque que dans model1 on accepte la nullité de \\(\\beta_2\\) alors qu’on la rejette dans model2. Ceci est logique dans la mesure où ces tests dépendent de la contrainte identifiante choisie. Dans model1 le test de nullité de \\(\\beta_2\\) permet de vérifier si \\(B\\) à un effet similaire à \\(A\\) sur \\(Y\\). Dans model2, on compare l’effet de \\(B\\) à celui de \\(C\\). On peut donc conclure \\(A\\) et \\(B\\) ont des effets proches sur \\(Y\\) alors que \\(B\\) et \\(C\\) ont un impact différent. Ceci est logique vu la façon dont les données ont été générées.\nTester l’effet global de \\(X\\) sur \\(Y\\) revient à tester si les coefficients \\(\\beta_1,\\beta_2\\) et \\(\\beta_3\\) sont égaux, ce qui, compte tenu des contraintes revient à considérer les hypothèses nulles :\n\n\\(\\beta_2=\\beta_3=0\\) dans model1 ;\n\\(\\beta_1=\\beta_2=0\\) dans model2.\n\nOn peut effectuer les tests de Wald ou du rapport de vraisemblance. On obtient les résultats du rapport de vraisemblance avec :\n\nlibrary(car)\nAnova(model1,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n  LR Chisq Df Pr(&gt;Chisq)    \nX   85.271  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(model2,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n               LR Chisq Df Pr(&gt;Chisq)    \nC(X, base = 3)   85.271  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn remarque ici que ces deux tests sont identiques : ils ne dépendent pas de la contrainte identifiante choisie.\n\n\n\nExercice 3 (Séparabilité)  \n\nOn génère l’échantillon demandé.\n\nset.seed(1234)\nX &lt;- c(runif(50,-1,0),runif(50,0,1))\nset.seed(5678)\nY &lt;- c(rep(0,50),rep(1,50))\ndf &lt;- data.frame(X,Y)\n\nLe graphe s’obtient avec :\n\nbeta &lt;- seq(0,100,by=0.01)\nlog_vrais &lt;- function(X,Y,beta){\n  LV &lt;- rep(0,length(beta))\n  for (i in 1:length(beta)){\n    Pbeta &lt;- exp(beta[i]*X)/(1+exp(beta[i]*X))\n    LV[i] &lt;- sum(Y*X*beta[i]-log(1+exp(X*beta[i])))\n#    gradln[i] &lt;- t(Xb)%*%(Yb-Pbeta)\n  }\n  return(LV)\n}\nLL &lt;- log_vrais(X,Y,beta)\nplot(beta,LL,type=\"l\")\n\n\n\n\n\n\n\n\nOn obtient un avertissement qui nous dit que l’algorithme d’optimisation n’a pas convergé.\n\nmodel &lt;- glm(Y~X-1,data=df,family=\"binomial\")\nmodel$coef\n\n       X \n1999.371 \n\n\nLe changement proposé supprime la séparabilité des données. On obtient bien un maximum fini pour cette nouvelle vraisemblance.\n\nY1 &lt;- Y;Y1[1] &lt;- 1\nLL1 &lt;- log_vrais(X,Y1,beta)\nplot(beta,LL1,type=\"l\")\n\n\n\n\n\n\n\nmodel1 &lt;- glm(Y1~X-1,family=\"binomial\")\nmodel1$coef\n\n       X \n10.17868 \n\n\n\n\n\nExercice 4 (Matrice hessienne) Le gradient de la log-vraisemblance en \\(\\beta\\) est donné par \\(\\nabla \\mathcal L(Y,\\beta)=X'(Y-P_\\beta)\\). Sa \\(j\\)ème composante vaut \\[\\frac{\\partial\\mathcal L}{\\partial\\beta_j}(\\beta)=\\sum_{i=1}^nx_{ij}(y_i-p_\\beta(x_i)).\\]\nOn peut donc calculer la drivée par rapport à \\(\\beta_\\ell\\) : \\[\\begin{align*}\n\\frac{\\partial\\mathcal L}{\\partial\\beta_j\\partial\\beta_\\ell}(\\beta)= & \\frac{\\partial}{\\partial\\beta_\\ell}\\left[\n\\sum_{i=1}^nx_{ij}\\left(y_i-\\frac{\\exp(x_i'\\beta)}{1+\\exp(x_i'\\beta)}\\right)\\right] \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}\\frac{\\exp(x_i'\\beta)}{[1+\\exp(x_i'\\beta)]^2} \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}p_\\beta(x_i)(1-p_\\beta(x_i)).\n\\end{align*}\\] Matriciellement on déduit donc que la hessienne vaut \\[\\nabla^2\\mathcal L(Y,\\beta)=-X'W_\\beta X,\\] où \\(W_\\beta\\) est la matrice \\(n\\times n\\) diagonale dont le \\(i\\)ème terme de la diagonale vaut \\(p_\\beta(x_i)(1-p_\\beta(x_i))\\). Par ailleurs, comme pour tout \\(i=1,\\dots,n\\), on a \\(p_\\beta(x_i)(1-p_\\beta(x_i))&gt;0\\) et que \\(X\\) est de plein rang, on déduit que \\(X'W_\\beta X\\) est définie positive et par conséquent que la hessienne est définie négative.\n\n\nExercice 5 (Modèles avec R) On importe les données\n\npanne &lt;- read.table(\"../donnees/panne.txt\",header=T)\nhead(panne)\n\n  etat age marque\n1    0   4      A\n2    0   2      C\n3    0   3      C\n4    0   9      B\n5    0   7      B\n6    0   6      A\n\n\n\nLa commande\n\nmodel &lt;- glm(etat~.,data=panne,family=binomial)\nmodel\n\n\nCall:  glm(formula = etat ~ ., family = binomial, data = panne)\n\nCoefficients:\n(Intercept)          age      marqueB      marqueC  \n    0.47808      0.01388     -0.41941     -1.45608  \n\nDegrees of Freedom: 32 Total (i.e. Null);  29 Residual\nNull Deviance:      45.72 \nResidual Deviance: 43.5     AIC: 51.5\n\n\najuste le modèle \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x_1+\\beta_2\\mathsf{1}_{x_2=B}+\\beta_3\\mathsf{1}_{x_2=C}\\] où \\(x_1\\) et \\(x_2\\) désigne respectivement les variables age et marque. On obtient les estimateurs avec\n\ncoef(model)\n\n(Intercept)         age     marqueB     marqueC \n 0.47808311  0.01388395 -0.41941071 -1.45608147 \n\n\nIl s’agit des tests de Wald pour tester l’effet des variables dans le modèle. Pour l’effet de marque, on va par exemple tester \\[H_0:\\beta_2=\\beta_3=0\\quad\\text{contre}\\quad H_1:\\beta_2\\neq 0\\text{ ou }\\beta_3\\neq 0.\\] Sous \\(H_0\\) la statistique de Wald suit une loi du \\(\\chi^2\\) à 4-2=2 degrés de liberté. Pour le test de la variable age le nombre de degrés de liberté manquant est 1. On retrouve cela dans la sortie\n\nlibrary(car)\nAnova(model,type=3,test.statistic=\"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: etat\n            Df  Chisq Pr(&gt;Chisq)\n(Intercept)  1 0.3294     0.5660\nage          1 0.0218     0.8826\nmarque       2 1.9307     0.3809\n\n\nIl s’agit cette fois du test du rapport de vraisemblance. Les degrés de liberté manquants sont identiques.\n\nAnova(model,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: etat\n       LR Chisq Df Pr(&gt;Chisq)\nage     0.02189  1     0.8824\nmarque  2.09562  2     0.3507\n\n\n\nLe modèle s’écrit \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1\\mathsf{1}_{x_2=A}+\\beta_2\\mathsf{1}_{x_2=B}.\\]\nLe modèle ajusté ici est \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\gamma_0+\\gamma_1\\mathsf{1}_{x_2=B}+\\gamma_2\\mathsf{1}_{x_2=C}.\\] Par identification on a \\[\\begin{cases}\n\\beta_0+\\beta_1=\\gamma_0 \\\\\n\\beta_0+\\beta_2=\\gamma_0+\\gamma_1 \\\\\n\\beta_0=\\gamma_0+\\gamma_2 \\\\\n\\end{cases}\n\\Longleftrightarrow\n\\begin{cases}\n\\beta_0=\\gamma_0+\\gamma_2 \\\\\n\\beta_1=-\\gamma_2 \\\\\n\\beta_2=\\gamma_1-\\gamma_2 \\\\\n\\end{cases}\n\\Longrightarrow\n\\begin{cases}\n\\widehat\\beta_0=-0.92 \\\\\n\\widehat\\beta_1=1.48 \\\\\n\\widehat\\beta_2=1.05 \\\\\n\\end{cases}\\] On peut retrouver ces résultats avec\n\nglm(etat~C(marque,base=3),data=panne,family=\"binomial\")\n\n\nCall:  glm(formula = etat ~ C(marque, base = 3), family = \"binomial\", \n    data = panne)\n\nCoefficients:\n         (Intercept)  C(marque, base = 3)A  C(marque, base = 3)B  \n             -0.9163                1.4759                1.0498  \n\nDegrees of Freedom: 32 Total (i.e. Null);  30 Residual\nNull Deviance:      45.72 \nResidual Deviance: 43.52    AIC: 49.52\n\n\n\nIl y a interaction si l’age agit différemment sur la panne en fonction de la marque.\nLe modèle ajusté sur R est \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\delta_0+\\delta_1\\mathsf{1}_{x_2=B}+\\delta_2\\mathsf{1}_{x_2=C}+\\delta_3x_1+\\delta_4x_1\\mathsf{1}_{x_2=B}+\\delta_5x_1\\mathsf{1}_{x_2=C}.\\] On obtient ainsi par identification : \\[\\begin{cases}\n\\alpha_0=\\delta_0\\\\\n\\alpha_1=\\delta_3\\\\\n\\beta_0=\\delta_0+\\delta_1\\\\\n\\beta_1=\\delta_3+\\delta_4\\\\\n\\gamma_0=\\delta_0+\\delta_2\\\\\n\\gamma_1=\\delta_3+\\delta_5\n\\end{cases}\\] On peut ainsi en déduire les valeurs des estimateurs que l’on peut retrouver avec la commande :\n\nglm(etat~-1+marque+marque:age,data=panne,family=\"binomial\")\n\n\nCall:  glm(formula = etat ~ -1 + marque + marque:age, family = \"binomial\", \n    data = panne)\n\nCoefficients:\n    marqueA      marqueB      marqueC  marqueA:age  marqueB:age  marqueC:age  \n    0.23512      0.43375     -2.19633      0.05641     -0.05547      0.27228  \n\nDegrees of Freedom: 33 Total (i.e. Null);  27 Residual\nNull Deviance:      45.75 \nResidual Deviance: 42.62    AIC: 54.62\n\n\n\n\n\nExercice 6 (Interprétation)  \n\ndf &lt;- read.csv(\"../donnees/logit_ex6.csv\")\nmod &lt;- glm(Y~.,data=df,family=binomial)\nmod1 &lt;- glm(Y~X1,data=df,family=binomial)\nsummary(mod)\n\n\nCall:\nglm(formula = Y ~ ., family = binomial, data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.57866    0.11926  -4.852 1.22e-06 ***\nX1          -0.19471    0.06556  -2.970  0.00298 ** \nX2           0.31899    0.04404   7.244 4.36e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 499  degrees of freedom\nResidual deviance: 618.26  on 497  degrees of freedom\nAIC: 624.26\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(mod1)\n\n\nCall:\nglm(formula = Y ~ X1, family = binomial, data = df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.001092   0.089499   0.012    0.990\nX1          -0.020467   0.051733  -0.396    0.692\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 499  degrees of freedom\nResidual deviance: 692.99  on 498  degrees of freedom\nAIC: 696.99\n\nNumber of Fisher Scoring iterations: 3\n\n\nOn remarque que la nullité du paramètre associé à X1 est accepté dans le modèle avec uniquement X1 alors qu’elle est refusée lorsqu’on considère X1 et X2 dans le modèle.\n\n\nExercice 7 (Tests à la main)  \n\nLe modèle s’écrit \\[log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x.\\]\nLa log vraisemblance s’obtient avec\n\np &lt;- c(0.76,0.4,0.6,0.89,0.35)\nY &lt;- c(1,0,0,1,1)\nL1 &lt;- log(prod(p^Y*(1-p)^(1-Y)))\nL1\n\n[1] -2.867909\n\n\n\nOn calcule les écart-type des estimateurs\n\nX1 &lt;- c(0.47,-0.55,-0.01,1.07,-0.71)\nX &lt;- matrix(c(rep(1,5),X1),ncol=2)\nW &lt;- diag(p*(1-p))\nSIG &lt;- solve(t(X)%*%W%*%X)\nsig &lt;- sqrt(diag(SIG))\nsig\n\n[1] 1.023252 1.744935\n\n\nOn en déduit les statistiques de test :\n\nbeta &lt;- c(0.4383,1.5063)\nbeta/sig\n\n[1] 0.4283401 0.8632411\n\n\nOn peut faire le test de Wald et du rapport de vraisemblance.\nLa statistique de test vaut 0.8632411, on obtient donc la probabilité critique\n\n2*(1-pnorm(0.8632411))\n\n[1] 0.3880049\n\n\nOn peut également effectuer un test du rapport de vraisemblance. Le modèle null sans X1 a pour log-vraisemblance\n\np0 &lt;- 3/5\nL0 &lt;- log(prod(p0^Y*(1-p0)^(1-Y)))\nL0\n\n[1] -3.365058\n\n\nLa statistique de test vaut donc\n\n2*(L1-L0)\n\n[1] 0.9942984\n\n\net la probabilité critique vaut\n\n1-pchisq(2*(L1-L0),df=1)\n\n[1] 0.3186941\n\n\nOn peut retrouver (aux arrondis près) les résultats de l’exercice avec\n\nX &lt;- c(0.47,-0.55,-0.01,1.07,-0.71)\nY &lt;- c(1,0,0,1,1)\ndf &lt;- data.frame(X,Y)\nmodel &lt;- glm(Y~X,data=df,family=\"binomial\")\nlogLik(model)\n\n'log Lik.' -2.898765 (df=2)\n\nAnova(model,type=3,test.statistic = \"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n            Df  Chisq Pr(&gt;Chisq)\n(Intercept)  1 0.1846     0.6675\nX            1 0.7552     0.3848\n\nAnova(model,type=3,test.statistic = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n  LR Chisq Df Pr(&gt;Chisq)\nX  0.93259  1     0.3342\n\n\n\n\n\n\nExercice 8 (Vraisemblance du modèle saturé)  \n\nLes variables \\((y_t,t=1,\\dots,y_T)\\) étant indépendantes et de loi binomiales \\(B(n_t,p_t)\\), la log-vraisemblance est donnée par \\[\\begin{align*}\n\\mathcal L_{\\text{sat}}(Y,p)= & \\log\\left(\\prod_{t=1}^T\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\np_t^{\\tilde y_t}(1-p_t)^{n_t-\\tilde y_t}\\right) \\\\\n= &\n\\sum_{t=1}^T\\left(\\log\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\n+\\tilde y_t\\log(p_t)+(n_t-\\tilde y_t)\\log(1-p_t)\\right)\n\\end{align*}\\]\nLa dérivée de la log-vraisemblance par rapport à \\(p_t\\) s’écrit \\[\\frac{\\tilde y_t}{p_t}-\\frac{n_t-\\tilde y_t}{1-p_t}.\\] Cette dérivée s’annule pour \\[\\widehat p_t=\\frac{\\tilde y_t}{n_t}.\\]\nOn note \\(\\widehat \\beta\\) l’EMV du modèle logistique et \\(p_{\\widehat\\beta}\\) le vecteur qui contient les valeurs ajustées \\(p_{\\widehat\\beta}(x_t),t=1,\\dots,T\\). On a pour tout \\(\\beta\\in\\mathbb R^p\\) : \\[\\mathcal L(Y,\\beta)\\leq\\mathcal L(Y,\\widehat\\beta)=\\mathcal L_{\\text{sat}}(Y,p_{\\widehat\\beta})\\leq L_{\\text{sat}}(Y,\\widehat p_t).\\]\n\n\n\nExercice 9 (Intervalle de confiance profilé)  \n\n\nartere &lt;- read.table(\"../donnees/artere.txt\",header=T)\nmodele &lt;- glm(chd~age,data=artere,family=binomial)\nB0 &lt;- coef(modele)\nOriginalDeviance &lt;- modele$deviance\n\n\nalpha &lt;- 0.05    \n\n\nstderr &lt;- summary(modele)$coefficients[, \"Std. Error\"]\ndelta &lt;- sqrt(qchisq((1-alpha/4),df=1))* stderr[2] /5\ngrille &lt;- B0[2]+(-10):10*delta\n\nOn a \\[\\begin{align*}\n\\mathcal D_1&=-2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L_{sat})\n  \\end{align*}\\] Pour celle avec l’offset \\(K_i=x_i\\beta_2^*\\) elle vaut \\[\\begin{align*}\n\\mathcal D_o&=-2(\\mathcal L(Y,K,\\hat\\beta_1)-\\mathcal L_{sat})\n  \\end{align*}\\] où \\(\\hat \\beta_1\\) maximise \\(\\mathcal L(Y,K,\\hat\\beta_1)\\) c’est à dire \\(\\mathcal L(Y,K,\\hat\\beta_1)=l(\\beta_2^*)\\) et nous avons donc \\[\\begin{align*}\n\\mathcal D_o - \\mathcal D_1= 2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L(Y,K,\\beta_1)= 2(\\mathcal L(Y,\\hat\\beta)-l(\\beta_2^*))=P(\\beta_2^*).\n  \\end{align*}\\]\n\nprofil2 &lt;- rep(0,length(grille))\nfor (k in 1:length(grille)) {\n  modeleo &lt;- glm(chd~1,family=binomial,offset=artere[,\"age\"]*grille[k],data=artere)\n  profil2[k] &lt;- modeleo$deviance - OriginalDeviance\n}\n\n\nprofil &lt;- sign(-10:10)*sqrt(profil2)\n\n\nspline(x=profil,y=grille,xout=c(-sqrt(qchisq(1-alpha,1)),sqrt(qchisq(1-alpha,1))))$y\n\n[1] 0.0669275 0.1620014\n\n\n\nconfint(modele)\n\n                  2.5 %     97.5 %\n(Intercept) -7.72587162 -3.2461547\nage          0.06693158  0.1620067",
    "crumbs": [
      "Correction des exercices",
      "IV Le modèle linéairé généralisé",
      "11 Régression logistique"
    ]
  },
  {
    "objectID": "code/chap6.html",
    "href": "code/chap6.html",
    "title": "6 Variables qualitatives : ANCOVA et ANOVA",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nplot(ozone[,\"T12\"], ozone[,\"O3\"],pch=as.numeric(ozone[,\"vent\"]),\n     col = as.numeric(ozone[,\"vent\"]))\na1 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"EST\",])\na2 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"NORD\",])\na3 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"OUEST\",])\na4 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"SUD\",])\nabline(a1, col=1)\nabline(a2, col=2)\nabline(a3, col=3)\nabline(a4, col=4)\n\n\n\n\n\n\n\n\n\nmod1b &lt;- lm(formula = O3 ~ -1 + vent + T12:vent, data = ozone)\nsummary(mod1b)\n\n\nCall:\nlm(formula = O3 ~ -1 + vent + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \nventEST        45.6090    13.9343   3.273 0.002133 ** \nventNORD      106.6345    28.0341   3.804 0.000456 ***\nventOUEST      64.6840    24.6208   2.627 0.011967 *  \nventSUD       -27.0602    26.5389  -1.020 0.313737    \nventEST:T12     2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -1.6491     1.6058  -1.027 0.310327    \nventOUEST:T12   0.3407     1.2047   0.283 0.778709    \nventSUD:T12     5.3786     1.1497   4.678 3.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.9773,    Adjusted R-squared:  0.973 \nF-statistic: 226.1 on 8 and 42 DF,  p-value: &lt; 2.2e-16\n\n\n\nmod1 &lt;- lm(formula = O3 ~ vent + T12:vent, data = ozone)\nsummary(mod1)\n\n\nCall:\nlm(formula = O3 ~ vent + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    45.6090    13.9343   3.273  0.00213 ** \nventNORD       61.0255    31.3061   1.949  0.05796 .  \nventOUEST      19.0751    28.2905   0.674  0.50384    \nventSUD       -72.6691    29.9746  -2.424  0.01972 *  \nventEST:T12     2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -1.6491     1.6058  -1.027  0.31033    \nventOUEST:T12   0.3407     1.2047   0.283  0.77871    \nventSUD:T12     5.3786     1.1497   4.678 3.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6212 \nF-statistic: 12.48 on 7 and 42 DF,  p-value: 1.614e-08\n\n\n\nmod2 &lt;- lm(formula = O3 ~ vent + T12, data = ozone)\nmod2b &lt;- lm(formula = O3 ~ -1 + vent + T12, data = ozone)\nmod3 &lt;- lm(formula = O3 ~ vent:T12, data = ozone)\n\n\nanova(mod2,mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent + T12\nModel 2: O3 ~ vent + T12:vent\n  Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     45 12612.0                                \n2     42  9087.4  3    3524.5 5.4298 0.003011 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod3,mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent:T12\nModel 2: O3 ~ vent + T12:vent\n  Res.Df     RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     45 11864.1                              \n2     42  9087.4  3    2776.6 4.2776 0.01008 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplot(rstudent(mod2) ~ fitted(mod2),xlab=\"ychap\",ylab=\"residus\")\n\n\n\n\n\n\n\n\n\nlibrary(lattice)\nxyplot(rstudent(mod2)~fitted(mod2)|vent,data = ozone, ylab=\"residus\")\n\n\n\n\n\n\n\n\n\nmod &lt;- lm(formula = O3 ~ vent + T12 + T12:vent, data = ozone)\n\n\nmod0 &lt;- lm(formula = O3 ~ vent +T12 + T12:vent, data = ozone)\nsummary(mod0)\n\n\nCall:\nlm(formula = O3 ~ vent + T12 + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    45.6090    13.9343   3.273  0.00213 ** \nventNORD       61.0255    31.3061   1.949  0.05796 .  \nventOUEST      19.0751    28.2905   0.674  0.50384    \nventSUD       -72.6691    29.9746  -2.424  0.01972 *  \nT12             2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -4.3971     1.7265  -2.547  0.01462 *  \nventOUEST:T12  -2.4073     1.3614  -1.768  0.08429 .  \nventSUD:T12     2.6306     1.3130   2.004  0.05160 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6212 \nF-statistic: 12.48 on 7 and 42 DF,  p-value: 1.614e-08\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\neucalypt[,\"bloc\"] &lt;- as.factor(eucalypt[,\"bloc\"])\nm.complet &lt;- lm(ht ~ bloc - 1 + bloc:circ, data = eucalypt)\nm.pente &lt;- lm(ht ~ bloc - 1 + circ, data = eucalypt)\nm.ordonne &lt;- lm(ht ~ bloc:circ, data = eucalypt)\nanova(m.pente, m.complet)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ bloc - 1 + circ\nModel 2: ht ~ bloc - 1 + bloc:circ\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   1425 2005.9                           \n2   1423 2005.0  2   0.84752 0.3007 0.7403\n\n\n\nanova(m.ordonne, m.complet)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ bloc:circ\nModel 2: ht ~ bloc - 1 + bloc:circ\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   1425 2009.2                           \n2   1423 2005.0  2    4.1649 1.4779 0.2285\n\n\n\nm.simple &lt;- lm(ht ~ circ, data = eucalypt)\nanova(m.simple, m.pente)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ circ\nModel 2: ht ~ bloc - 1 + circ\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   1427 2052.1                                  \n2   1425 2005.9  2    46.188 16.406 9.031e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nANOVA\n\nmod1 &lt;- lm(O3~vent-1,data=ozone)\nsummary(mod1)\n\n\nCall:\nlm(formula = O3 ~ vent - 1, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \nventEST    103.850      4.963   20.92  &lt; 2e-16 ***\nventNORD    78.289      6.618   11.83 1.49e-15 ***\nventOUEST   71.578      4.680   15.30  &lt; 2e-16 ***\nventSUD     94.343      7.504   12.57  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.9547,    Adjusted R-squared:  0.9508 \nF-statistic: 242.4 on 4 and 46 DF,  p-value: &lt; 2.2e-16\n\n\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: O3\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent       4 382244   95561  242.44 &lt; 2.2e-16 ***\nResiduals 46  18131     394                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nmod2 &lt;- lm(O3 ~ vent, data = ozone)\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: O3\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent       3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals 46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod2)\n\n\nCall:\nlm(formula = O3 ~ vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  103.850      4.963  20.923  &lt; 2e-16 ***\nventNORD     -25.561      8.272  -3.090  0.00339 ** \nventOUEST    -32.272      6.821  -4.731 2.16e-05 ***\nventSUD       -9.507      8.997  -1.057  0.29616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nlm(O3 ~ C(vent,treatment), data = ozone)\n\n\nCall:\nlm(formula = O3 ~ C(vent, treatment), data = ozone)\n\nCoefficients:\n            (Intercept)   C(vent, treatment)NORD  C(vent, treatment)OUEST  \n                103.850                  -25.561                  -32.272  \n  C(vent, treatment)SUD  \n                 -9.507  \n\n\n\nlm(O3 ~ C(vent,base=2), data = ozone)\n\n\nCall:\nlm(formula = O3 ~ C(vent, base = 2), data = ozone)\n\nCoefficients:\n           (Intercept)    C(vent, base = 2)EST  C(vent, base = 2)OUEST  \n                78.289                  25.561                  -6.711  \n  C(vent, base = 2)SUD  \n                16.054  \n\n\n\nII &lt;- length(levels(as.factor(ozone$vent)))\nnI &lt;- table(ozone$vent)\ncontraste&lt;-matrix(rbind(diag(II-1),-nI[-II]/nI[II]),II,II-1)\nmod3 &lt;- lm(O3 ~ C(vent,contraste), data = ozone)\nanova(mod3)\n\nAnalysis of Variance Table\n\nResponse: O3\n                   Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nC(vent, contraste)  3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals          46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod3)\n\n\nCall:\nlm(formula = O3 ~ C(vent, contraste), data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           86.300      2.808  30.737  &lt; 2e-16 ***\nC(vent, contraste)1   17.550      4.093   4.288 9.15e-05 ***\nC(vent, contraste)2   -8.011      5.993  -1.337 0.187858    \nC(vent, contraste)3  -14.722      3.744  -3.933 0.000281 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nmod4 &lt;- lm(O3 ~ C(vent,sum), data = ozone) \nanova(mod4)\n\nAnalysis of Variance Table\n\nResponse: O3\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nC(vent, sum)  3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals    46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod4)\n\n\nCall:\nlm(formula = O3 ~ C(vent, sum), data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     87.015      3.027  28.743  &lt; 2e-16 ***\nC(vent, sum)1   16.835      4.635   3.632 0.000705 ***\nC(vent, sum)2   -8.726      5.573  -1.566 0.124284    \nC(vent, sum)3  -15.437      4.485  -3.442 0.001240 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nresid2 &lt;- resid(mod2)\nozone$vent &lt;- as.factor(ozone$vent)\nplot(resid2~vent,data=ozone, ylab=\"residus\")\n\n\n\n\n\n\n\nplot(resid2 ~ jitter(fitted(mod2)),xlab=\"ychap\",ylab=\"residus\")\n\n\n\n\n\n\n\nxyplot(resid2 ~ I(1:50)|vent, data=ozone,\n       xlab=\"index\", ylab=\"residus\")\n\n\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nwith(ozone, interaction.plot(vent, nebulosite, O3, col=1:2))\nwith(ozone, interaction.plot(nebulosite, vent, O3, col=1:4))\n\n\n\n\n\n\n\n\n\nmod1 &lt;- lm(O3 ~ vent + nebulosite + vent:nebulosite, data = ozone)\nmod2 &lt;- lm(O3 ~ vent + nebulosite, data = ozone)\nanova(mod2, mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent + nebulosite\nModel 2: O3 ~ vent + nebulosite + vent:nebulosite\n  Res.Df   RSS Df Sum of Sq     F Pr(&gt;F)\n1     45 11730                          \n2     42 11246  3    483.62 0.602 0.6173\n\n\n\nmod3 &lt;- lm(O3 ~ vent, data = ozone)\nanova(mod3, mod2)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent\nModel 2: O3 ~ vent + nebulosite\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 18131                                  \n2     45 11730  1    6401.5 24.558 1.066e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(mod3, mod2, mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent\nModel 2: O3 ~ vent + nebulosite\nModel 3: O3 ~ vent + nebulosite + vent:nebulosite\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 18131                                  \n2     45 11730  1    6401.5 23.907 1.523e-05 ***\n3     42 11246  3     483.6  0.602    0.6173    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: O3\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent             3  9859.8  3286.6  12.274 6.689e-06 ***\nnebulosite       1  6401.5  6401.5  23.907 1.523e-05 ***\nvent:nebulosite  3   483.6   161.2   0.602    0.6173    \nResiduals       42 11246.2   267.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Codes R",
      "II Inférence",
      "6 Variables qualitatives : ANCOVA et ANOVA"
    ]
  },
  {
    "objectID": "correction/chap15.html",
    "href": "correction/chap15.html",
    "title": "15 Données déséquilibrées",
    "section": "",
    "text": "library(tidyverse)\n\n\nExercice 1 (Critères pour un exemple de données déséquilibrées)  \n\n\nset.seed(1235)\nn &lt;- 500\nY &lt;- rbinom(n,1,0.05) %&gt;% as.factor()\n\n\nset.seed(12345)\nP1 &lt;- rbinom(n,1,0.005) %&gt;% factor(levels=c(\"0\",\"1\"))\n\n\nset.seed(123)\nP2 &lt;- rep(0,n)\nP2[Y==1] &lt;- rbinom(sum(Y==1),1,0.85) \nP2[Y==0] &lt;- rbinom(sum(Y==0),1,0.1)  \nP2 &lt;- factor(P2,levels=c(\"0\",\"1\"))\n\n\ntable(P1,Y)\n\n   Y\nP1    0   1\n  0 473  24\n  1   3   0\n\ntable(P2,Y)\n\n   Y\nP2    0   1\n  0 432   8\n  1  44  16\n\n\n\nT2 &lt;- table(P2,Y)\nacc &lt;- sum(T2[c(1,4)])/sum(T2)\nrec &lt;- T2[2,2]/sum(T2[,2])\nprec &lt;- T2[2,2]/sum(T2[2,])\nc(acc,rec,prec)\n\n[1] 0.8960000 0.6666667 0.2666667\n\n\n\nF1 &lt;- 2*(rec*prec)/(rec+prec)\nF1\n\n[1] 0.3809524\n\n\n\nrand &lt;- sum(T2[1,])/n*sum(T2[,1])/n+sum(T2[2,])/n*sum(T2[,2])/n\nkappa &lt;- (acc-rand)/(1-rand)\nkappa\n\n[1] 0.3353783\n\n\nRetrouver ces indicateurs à l’aide de la fonction confusionMatrix de caret puis comparer les prévisions P1 et P2.\n\nlibrary(yardstick)\ndf &lt;- data.frame(Y,P1,P2)\nmulti_metric &lt;- metric_set(accuracy,recall,precision,f_meas,kap)\ndf %&gt;% pivot_longer(-Y,names_to = \"algo\",values_to = \"prev\") %&gt;%\n  group_by(algo) %&gt;%\n  multi_metric(truth=Y,estimate = prev,event_level = \"second\") %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;% \n  select(-2)\n\n# A tibble: 5 × 3\n  .metric        P1    P2\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 accuracy   0.946  0.896\n2 recall     0      0.667\n3 precision  0      0.267\n4 f_meas     0      0.381\n5 kap       -0.0108 0.335\n\n\n\n\n\nExercice 2 (Échantillonnage rétrospectif) On remarque d’abord que \\(\\mathbf P(\\tilde y_i=1)=\\mathbf P(y_i=1|s_i=1)\\). De plus \\[\n\\text{logit}\\, p_\\beta(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}\\quad\\text{et}\\quad \\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1|s_i=1)}{\\mathbf P(y_i=0|s_i=1)}.\n\\] Or \\[\n\\mathbf P(y_i=1|s_i=1)=\\frac{\\mathbf P(y_i=1,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=1)\\mathbf P(y_i=1)}{\\mathbf P(s_i=1)}\n\\] et \\[\n\\mathbf P(y_i=0|s_i=1)=\\frac{\\mathbf P(y_i=0,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=0)\\mathbf P(y_i=0)}{\\mathbf P(s_i=1)}.\n\\] Donc \\[\n\\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}+\\log\\frac{\\mathbf P(s_i=1|y_i=1)}{\\mathbf P(s_i=1|y_i=0)}=\\text{logit}\\,p_\\beta(x_i)+\\log\\left(\\frac{\\tau_{1i}}{\\tau_{0i}}\\right).\n\\]\n\n\nExercice 3 (Rééquilibrage)  \n\n\ndf1 &lt;- read.csv(\"../donnees/dd_exo3_1.csv\") %&gt;% mutate(Y=as.factor(Y))\ndf2 &lt;- read.csv(\"../donnees/dd_exo3_2.csv\") %&gt;% mutate(Y=as.factor(Y))\ndf3 &lt;- read.csv(\"../donnees/dd_exo3_3.csv\") %&gt;% mutate(Y=as.factor(Y))\n\n\nsummary(df1$Y)\n\n  0   1 \n559 441 \n\nsummary(df2$Y)\n\n  0   1 \n692 308 \n\nsummary(df3$Y)\n\n  0   1 \n842 158 \n\n\n\nggplot(df1)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\n\n\nggplot(df2)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\n\n\nggplot(df3)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\n\n\n\n\nlibrary(caret)\nset.seed(123)\na1 &lt;- createDataPartition(1:nrow(df1),p=2/3)\na2 &lt;- createDataPartition(1:nrow(df2),p=2/3)\na3 &lt;- createDataPartition(1:nrow(df3),p=2/3)\ntrain1 &lt;- df1[a1$Resample1,]\ntrain2 &lt;- df2[a2$Resample1,]\ntrain3 &lt;- df3[a3$Resample1,]\ntest1 &lt;- df1[-a1$Resample1,]\ntest2 &lt;- df2[-a2$Resample1,]\ntest3 &lt;- df3[-a3$Resample1,]\n\n\nlogit1 &lt;- glm(Y~.^2,data=train1,family=binomial)\nlogit2 &lt;- glm(Y~.^2,data=train2,family=binomial)\nlogit3 &lt;- glm(Y~.^2,data=train3,family=binomial)\np1 &lt;- predict(logit1,newdata=test1,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np2 &lt;- predict(logit2,newdata=test2,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3 &lt;- predict(logit3,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\n\n\nconfusionMatrix(data=p1,reference=test1$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 139  53\n         1  45  95\n\n               Accuracy : 0.7048          \n                 95% CI : (0.6526, 0.7534)\n    No Information Rate : 0.5542          \n    P-Value [Acc &gt; NIR] : 1.282e-08       \n\n                  Kappa : 0.3994          \n\n Mcnemar's Test P-Value : 0.4795          \n\n            Sensitivity : 0.7554          \n            Specificity : 0.6419          \n         Pos Pred Value : 0.7240          \n         Neg Pred Value : 0.6786          \n             Prevalence : 0.5542          \n         Detection Rate : 0.4187          \n   Detection Prevalence : 0.5783          \n      Balanced Accuracy : 0.6987          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(data=p2,reference=test2$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 206  52\n         1  27  47\n\n               Accuracy : 0.762           \n                 95% CI : (0.7125, 0.8068)\n    No Information Rate : 0.7018          \n    P-Value [Acc &gt; NIR] : 0.008642        \n\n                  Kappa : 0.387           \n\n Mcnemar's Test P-Value : 0.006930        \n\n            Sensitivity : 0.8841          \n            Specificity : 0.4747          \n         Pos Pred Value : 0.7984          \n         Neg Pred Value : 0.6351          \n             Prevalence : 0.7018          \n         Detection Rate : 0.6205          \n   Detection Prevalence : 0.7771          \n      Balanced Accuracy : 0.6794          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(data=p3,reference=test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 274  58\n         1   0   0\n\n               Accuracy : 0.8253          \n                 95% CI : (0.7801, 0.8646)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 0.535           \n\n                  Kappa : 0               \n\n Mcnemar's Test P-Value : 7.184e-14       \n\n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.8253          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.8253          \n         Detection Rate : 0.8253          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n\n       'Positive' Class : 0               \n\n\n\nOn remarque que l’accuracy est meilleure pour le 3ème échantillon, contrairement à des indicateurs tels que le \\(\\kappa\\) de Cohen ou le balanced accuracy.\n\nlibrary(UBL)\nset.seed(1243)\ntrain3.over &lt;- RandOverClassif(Y~.,dat=train3)\ntrain3.smote &lt;- SmoteClassif(Y~.,dat=train3)\ntrain3.under &lt;- RandUnderClassif(Y~.,dat=train3)\ntrain3.tomek &lt;- TomekClassif(Y~.,dat=train3)[[1]]\n\n\nggplot(train3.under)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\n\n\nggplot(train3.over)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\n\n\nggplot(train3.smote)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\n\n\nggplot(train3.tomek)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\n\n\n\n\nlogit3.over &lt;- glm(Y~.^2,data=train3.over,family=binomial)\nlogit3.smote &lt;- glm(Y~.^2,data=train3.smote,family=binomial)\nlogit3.under &lt;- glm(Y~.^2,data=train3.under,family=binomial)\nlogit3.tomek &lt;- glm(Y~.^2,data=train3.tomek,family=binomial)\np3.over &lt;- predict(logit3.over,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3.smote &lt;- predict(logit3.smote,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3.under &lt;- predict(logit3.under,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3.tomek &lt;- predict(logit3.tomek,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\n\n\nconfusionMatrix(p3.over,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 182   9\n         1  92  49\n\n               Accuracy : 0.6958          \n                 95% CI : (0.6432, 0.7448)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 1               \n\n                  Kappa : 0.3255          \n\n Mcnemar's Test P-Value : 3.37e-16        \n\n            Sensitivity : 0.6642          \n            Specificity : 0.8448          \n         Pos Pred Value : 0.9529          \n         Neg Pred Value : 0.3475          \n             Prevalence : 0.8253          \n         Detection Rate : 0.5482          \n   Detection Prevalence : 0.5753          \n      Balanced Accuracy : 0.7545          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(p3.smote,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 181   9\n         1  93  49\n\n               Accuracy : 0.6928         \n                 95% CI : (0.6401, 0.742)\n    No Information Rate : 0.8253         \n    P-Value [Acc &gt; NIR] : 1              \n\n                  Kappa : 0.3217         \n\n Mcnemar's Test P-Value : &lt;2e-16         \n\n            Sensitivity : 0.6606         \n            Specificity : 0.8448         \n         Pos Pred Value : 0.9526         \n         Neg Pred Value : 0.3451         \n             Prevalence : 0.8253         \n         Detection Rate : 0.5452         \n   Detection Prevalence : 0.5723         \n      Balanced Accuracy : 0.7527         \n\n       'Positive' Class : 0              \n\n\nconfusionMatrix(p3.under,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 187  11\n         1  87  47\n\n               Accuracy : 0.7048          \n                 95% CI : (0.6526, 0.7534)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 1               \n\n                  Kappa : 0.325           \n\n Mcnemar's Test P-Value : 3.56e-14        \n\n            Sensitivity : 0.6825          \n            Specificity : 0.8103          \n         Pos Pred Value : 0.9444          \n         Neg Pred Value : 0.3507          \n             Prevalence : 0.8253          \n         Detection Rate : 0.5633          \n   Detection Prevalence : 0.5964          \n      Balanced Accuracy : 0.7464          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(p3.tomek,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 274  55\n         1   0   3\n\n               Accuracy : 0.8343          \n                 95% CI : (0.7899, 0.8727)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 0.3641          \n\n                  Kappa : 0.0826          \n\n Mcnemar's Test P-Value : 3.305e-13       \n\n            Sensitivity : 1.00000         \n            Specificity : 0.05172         \n         Pos Pred Value : 0.83283         \n         Neg Pred Value : 1.00000         \n             Prevalence : 0.82530         \n         Detection Rate : 0.82530         \n   Detection Prevalence : 0.99096         \n      Balanced Accuracy : 0.52586         \n\n       'Positive' Class : 0               \n\n\n\nLes indicateurs adaptés aux données déséquilibrées sont améliorés, on détecte mieux les 1 (quitte à faire plus d’erreur sur les 0).\n\n\n\nExercice 4 (Rééquilibrage et information de Fisher)  \n\nVérifier intuitivement que la situation de sous-échantillonnage proposé aléatoire peut être approximée par ce schéma si la taille du sous échantillons des$ Y=0 $(dans lequel on sélectionne) est grande. Si la taille est grande alors un tirage le tirage est approximativement équivalent à un tirage avec remise, ce qui est le schéma de l’exercice.\nEn écrivant la loi conjointe \\(g(s, y|x)\\) comme la densité de \\(s\\) sachant \\(y,x\\) multipliée par la densité de \\(y\\) sachant \\(x\\) \\[\n\\begin{align*}\n  g(y,s|x) = \\mathrm{densite}(s|x, y) \\mathrm{densite}(y|x) =h(s|y)f_\\theta(y|x).\n\\end{align*}\n\\] En passant au log nous avons \\[\n\\begin{align*}\n  \\log g(y,s|x) = \\log h(s|y) + \\log f_\\theta(y|x).\n\\end{align*}\n\\] Écrivons la log-vraisemblance des données complètes: \\[\n  \\begin{align*}\n  \\sum_{i=1}^{n}\\log g(y_{i},s_{i}|x_{i}) = \\sum_{i=1}^{n}\\log h(s_{i}|y_{i}) + \\sum_{i=1}^{n}\\log f_\\theta(y_{i}|x_{i}),\n\\end{align*}\n\\] et notons alors \\(\\mathcal{L^*}(\\theta)\\) la partie gauche et \\(\\mathcal{L}(\\theta)\\) la partie de droite nous retrouvons l’équation demandée.\nEn dérivant 2 fois par rapport à \\(\\theta\\) (on sait que \\(h(s|y)\\) ne dépend pas de \\(\\theta\\)) et en passant à l’opposé on a \\[%\n\\begin{align*}\n- \\sum_{i=1}^{n} \\frac{\\partial^{2} g(y_{i},s_{i}|x_{i})}{\\partial \\theta^{2}} =  - \\sum_{i=1}^{n} \\frac{\\partial^{2} f_\\theta(y_{i}|x_{i})}{\\partial \\theta^{2}}\n\\end{align*}\n\\] Ce qui est bien l’égalité demandée.\nOn reconditionne dans l’autre sens: \\[%\n\\begin{align*}\n  g(y,s|x) = \\mathrm{densité}(y|s, y) \\mathrm{densité}(s|x) =l(y|s,x)m(s|x).\n\\end{align*}\n\\] En passant au log nous avons \\[%\n\\begin{align*}\n  \\log g(y,s|x) = \\log l(y|s,x) + \\log m(s|x).\n\\end{align*}\n\\] Et nous avons la vraisemblance: \\[%\n\\begin{align*}\n  \\sum_{i=1}^{n}\\log g(y_{i},s_{i}|x_{i}) = \\sum_{i=1}^{n}\\log l(y_{i}|s_{i},x_{i}) + \\sum_{i=1}^{n}\\log m(s_{i}|x_{i}).\n\\end{align*}\n\\] Comme \\(s_{i}\\) vaut 0 ou 1 en regroupant nous avons bien \\[%\n\\begin{align*}\n\\mathcal{L^*}(\\theta)&=\\sum_{i\\in\\{i | S_i=1\\}}{\\log l(y|s=1,x)} +\n                       \\sum_{i\\in\\{i | S_i=0\\}}{\\log l(y|s=0,x)}+\n                       \\sum_{i=1}^{n}{\\log m(s_i|x_i)}.\n\\end{align*}\n\\]\nEn dérivant 2 fois par rapport à \\(\\theta\\) et en passant à l’opposé on retrouve l’expression demandée notée aussi \\[%\n\\begin{align*}\n     I^*_n(\\theta)&=I_n(\\theta|S=1)+ I_n(\\theta|S=0) + I_n(\\theta, S).\n   \\end{align*}\n\\]\nParmi toutes les informations de Fisher introduites,\n\nquelle est celle qui correspond à l’échantillon ?\nC’est \\(I_n(\\theta)\\),\nquelle est celle qui correspond à l’échantillon après rééquilibrage ?\nC’est \\(I_n(\\theta|S=1)\\)\n\nComme nous savons que \\[\n\\begin{align*}\n   I_n(\\theta) = I^*_n(\\theta) = I_n(\\theta|S=1)+ I_n(\\theta|S=0) + I_n(\\theta, S)\n\\end{align*}\n\\] On déduit que si chaque information de Fisher empirique converge vers vers une matrice définie positive alors il vaut mieux garder l’information la plus grande, \\(I_n(\\theta) = I^*_n(\\theta)\\) donc tout l’échantillon.",
    "crumbs": [
      "Correction des exercices",
      "IV Le modèle linéairé généralisé",
      "15 Données déséquilibrées"
    ]
  },
  {
    "objectID": "code/chap13.html",
    "href": "code/chap13.html",
    "title": "13 Régularisation de la vraisemblance",
    "section": "",
    "text": "Régressions pénalisées avec glmnet\n\nlibrary(bestglm)\ndata(SAheart)\nSAheart.X &lt;- model.matrix(chd~.,data=SAheart)[,-1]\nSAheart.Y &lt;- SAheart$chd \nlibrary(glmnet)\nridge &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0)\nlasso &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\npar(mfrow=c(2,2))\nplot(ridge,ylim=c(-0.1,0.2))\nplot(lasso,ylim=c(-0.1,0.2))\nplot(ridge,ylim=c(-0.1,0.2),xvar=\"lambda\")\nplot(lasso,ylim=c(-0.1,0.2),xvar=\"lambda\")\n\n\n\n\n\n\n\nplot(ridge,ylim=c(-0.1,0.2),cex.lab=0.5)\nplot(lasso,ylim=c(-0.1,0.2),cex.lab=0.5)\nplot(ridge,ylim=c(-0.1,0.2),xvar=\"lambda\",cex.lab=0.5)\nplot(lasso,ylim=c(-0.1,0.2),xvar=\"lambda\",cex.lab=0.5)\n\n\n\n\n\n\n\n\n\n\nValidation croisée\n\nset.seed(2398)\nm1.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0)\nm1.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\nm2.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0,type.measure=\"class\")\nm2.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1,type.measure=\"class\")\nm3.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0,type.measure=\"auc\")\nm3.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1,type.measure=\"auc\")\n\n\nm1.ridge$lambda.min\n\n[1] 0.01774595\n\nm1.ridge$lambda.1se\n\n[1] 0.2892148\n\n\n\npar(mfrow=c(3,2))\nplot(m1.ridge,main=\"Ridge\")\nplot(m1.lasso,main=\"Lasso\")\nplot(m2.ridge,main=\"Ridge\")\nplot(m2.lasso,main=\"Lasso\")\nplot(m3.ridge,main=\"Ridge\")\nplot(m3.lasso,main=\"Lasso\")\n\n\n\n\n\n\n\n\n\n\nGroup-lasso et elastic net\n\nlibrary(gglasso)\nX1 &lt;- c(rep(\"A\",60),rep(\"B\",90),rep(\"C\",50))\nX2 &lt;- c(rep(\"E\",40),rep(\"F\",60),rep(\"G\",55),rep(\"H\",45))\nset.seed(1298)\nX_3 &lt;- runif(200)\nset.seed(2381)\nY &lt;- round(runif(200))\ndonnees &lt;- data.frame(X1,X2,X_3,Y)\n\n\nD &lt;- model.matrix(Y~.,data=donnees)[,-1]\nlasso &lt;- glmnet(D,Y,alpha=1,lambda=exp(seq(-3,-5,length=100)))\ngroupe &lt;- c(1,1,2,2,2,3)\nlibrary(gglasso)\nY1 &lt;- 2*Y-1 \ng.lasso &lt;- gglasso(D,Y1,group=groupe,loss=\"logit\",lambda=exp(seq(-4.5,-5.5,length=100)))\nplot(lasso,xvar=\"lambda\",lwd=2,main=\"Lasso\")\n\n\n\n\n\n\n\nplot(g.lasso,main=\"Group-lasso\")\n\n\n\n\n\n\n\n\n\nlibrary(caret)\nalpha &lt;- seq(0,1,by=0.1)\nlambda &lt;- exp(seq(-7,2,length=100))\ngrille &lt;- expand.grid(alpha=alpha,lambda=lambda)\nctrl &lt;- trainControl(method=\"cv\")\nSAheart$chd &lt;- as.factor(SAheart$chd)\nset.seed(1234)\nsel &lt;- train(chd~.,data=SAheart,method=\"glmnet\",family=\"binomial\",trControl=ctrl,tuneGrid=grille)\nsel$bestTune\n\n    alpha     lambda\n747   0.7 0.05971442\n\ngetTrainPerf(sel)\n\n  TrainAccuracy TrainKappa method\n1     0.7489362  0.3759712 glmnet\n\n\n\n\nApplication : détection d’images publicitaires\n\nad.data &lt;- read.table(\"../donnees/ad_data.txt\",header=FALSE,sep=\",\",dec=\".\",na.strings = \"?\",strip.white = TRUE)\nnames(ad.data)[ncol(ad.data)] &lt;- \"Y\"\nad.data$Y &lt;- as.factor(ad.data$Y)\n\nad.data1 &lt;- na.omit(ad.data)\ndim(ad.data1)\n\n[1] 2359 1559\n\n\n\nset.seed(1234)\nind.app &lt;- sample(nrow(ad.data1),1800)\ndapp &lt;- ad.data1[ind.app,]\ndtest &lt;- ad.data1[-ind.app,]\n\n\nX.app &lt;- model.matrix(Y~.,data=dapp)[,-1]\nX.test &lt;- model.matrix(Y~.,data=dtest)[,-1]\nY.app &lt;- dapp$Y\nY.test &lt;- dtest$Y\n\n\nlogit &lt;- glm(Y~.,data=dapp,family=\"binomial\") \n\n\nset.seed(123)\nlasso.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\")\nridge.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\",alpha=0,lambda=exp(seq(-8,0,length=100)))\nen.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\",alpha=0.5)\n\n\npar(mfrow=c(1,3))\nplot(lasso.cv,main=\"Lasso\")\nplot(ridge.cv,main=\"Ridge\")\nplot(en.cv,main=\"Elastic net\")\n\n\n\n\n\n\n\n\n\nscore &lt;- data.frame(obs=dtest$Y,logit=predict(logit,newdata=dtest,type=\"response\"),\n                                lasso=as.vector(predict(lasso.cv,newx = X.test,type=\"response\")),\n                                ridge=as.vector(predict(ridge.cv,newx = X.test,type=\"response\")),\n                                en=as.vector(predict(en.cv,newx = X.test,type=\"response\")))\n\n\nlibrary(pROC)\nroc.ad &lt;- roc(obs~logit+lasso+ridge+en,data=score)\n\ncouleur &lt;- c(\"black\",\"red\",\"blue\",\"green\")\nmapply(plot,roc.ad,col=couleur,lty=1:4,add=c(F,T,T,T),lwd=2,legacy.axes=TRUE)\n\n                   logit       lasso       ridge       en         \npercent            FALSE       FALSE       FALSE       FALSE      \nsensitivities      numeric,329 numeric,276 numeric,515 numeric,266\nspecificities      numeric,329 numeric,276 numeric,515 numeric,266\nthresholds         numeric,329 numeric,276 numeric,515 numeric,266\ndirection          \"&lt;\"         \"&lt;\"         \"&lt;\"         \"&lt;\"        \ncases              numeric,461 numeric,461 numeric,461 numeric,461\ncontrols           numeric,98  numeric,98  numeric,98  numeric,98 \nfun.sesp           ?           ?           ?           ?          \nauc                0.8635619   0.9695648   0.9805879   0.969587   \ncall               expression  expression  expression  expression \noriginal.predictor numeric,559 numeric,559 numeric,559 numeric,559\noriginal.response  factor,559  factor,559  factor,559  factor,559 \npredictor          numeric,559 numeric,559 numeric,559 numeric,559\nresponse           factor,559  factor,559  factor,559  factor,559 \nlevels             character,2 character,2 character,2 character,2\npredictor.name     \"logit\"     \"lasso\"     \"ridge\"     \"en\"       \nresponse.name      \"obs\"       \"obs\"       \"obs\"       \"obs\"      \n\nlegend(\"bottomright\",legend=c(\"logit\",\"lasso\",\"ridge\",\"elastic net\"),col=couleur,lty=1:4,lwd=2,cex=0.65)\n\n\n\n\n\n\n\n\n\nsort(round(unlist(lapply(roc.ad,auc)),3),decreasing=TRUE)\n\nridge lasso    en logit \n0.981 0.970 0.970 0.864 \n\n\n\nprev1 &lt;- data.frame(apply(round(score[,-1]),2,factor,labels=c(\"ad.\",\"nonad.\")))\nerr &lt;- apply(sweep(prev1,1,dtest$Y,FUN=\"!=\"),2,mean)\nsort(round(err,3))\n\nridge lasso    en logit \n0.030 0.034 0.036 0.077",
    "crumbs": [
      "Codes R",
      "IV Le modèle linéairé généralisé",
      "13 Régularisation de la vraisemblance"
    ]
  },
  {
    "objectID": "correction/chap13.html",
    "href": "correction/chap13.html",
    "title": "13 Régularisation de la vraisemblance",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nA, B\nC\nA\nC, D\n\n\n\nExercice 2 (Lasso sur des données centrées réduites)  \n\nlibrary(bestglm)\ndata(SAheart)\nSAheart.X &lt;- model.matrix(chd~.,data=SAheart)[,-1]\nSAheart.Y &lt;- SAheart$chd \nlibrary(glmnet)\nmod.lasso &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\n\n\n\nlam.lasso &lt;- mod.lasso$lambda\nlam &lt;- lam.lasso[50]\ncoef(mod.lasso,s=lam)\n\n10 x 1 sparse Matrix of class \"dgCMatrix\"\n                         s1\n(Intercept)    -6.105111733\nsbp             0.006097928\ntobacco         0.077763845\nldl             0.169776425\nadiposity       0.012176169\nfamhistPresent  0.902337702\ntypea           0.037527467\nobesity        -0.050604210\nalcohol         .          \nage             0.045476625\n\n\n\nmu &lt;- apply(SAheart.X,2,mean)\nsig &lt;- apply(SAheart.X,2,sd)\nmu.mat &lt;- matrix(rep(mu,nrow(SAheart.X)),nrow=nrow(SAheart.X),byrow=T)\nsig.mat &lt;- matrix(rep(sig,nrow(SAheart.X)),nrow=nrow(SAheart.X),byrow=T)\nSAheart.X.cr &lt;- (SAheart.X-mu.mat)/sig.mat\nmod.lasso1 &lt;- glmnet(SAheart.X.cr,SAheart.Y,family=\"binomial\",alpha=1)\n\n\nlam1 &lt;- mod.lasso1$lambda[50]\ncoef(mod.lasso1,s=lam1)[-1]/sig\n\n           sbp        tobacco            ldl      adiposity famhistPresent \n   0.006097928    0.077763845    0.169776425    0.012176169    0.902337702 \n         typea        obesity        alcohol            age \n   0.037527467   -0.050604210    0.000000000    0.045476625 \n\n\n\n\n\nExercice 3 (Comparaison de méthodes et courbes ROC)  \n\nOn importe les données et on les sépare en un échantillon d’apprentissage et de test.\n\ndf &lt;- read.csv(\"../donnees/logit_ridge_lasso.csv\")\nset.seed(1254)\nperm &lt;- sample(nrow(df))\ndapp &lt;- df[perm[1:300],]\ndtest &lt;- df[perm[301:500],]\n\nOn construit les modèles demandés sur les données d’apprentissage uniquement.\n\nlogit &lt;- glm(Y~.,data=dapp,family=\"binomial\")\nlogit.step &lt;- step(logit,direction=\"backward\",trace=0)\n\n\nXapp &lt;- model.matrix(Y~.,data=dapp)[,-1]\nXtest &lt;- model.matrix(Y~.,data=dtest)[,-1]\nYapp &lt;- dapp$Y\nYtest &lt;- dtest$Y\n\n\nlasso1 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=1)\nridge1 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=0,lambda=exp(seq(-6,-1,length=100)))\nlasso2 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=1,type.measure = \"auc\")\nridge2 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=0,type.measure = \"auc\",lambda=exp(seq(-3,2,length=100)))\n\n\nlibrary(tidyverse)\nscore &lt;- data.frame(logit=predict(logit,newdata=dtest,type=\"response\"),\n                    step=predict(logit.step,newdata=dtest,type=\"response\"),\n                    lasso1=as.vector(predict(lasso1,type=\"response\",newx=Xtest)),\n                    ridge1=as.vector(predict(ridge1,type=\"response\",newx=Xtest)),\n                    lasso2=as.vector(predict(lasso2,type=\"response\",newx=Xtest)),\n                    ridge2=as.vector(predict(ridge2,type=\"response\",newx=Xtest))) %&gt;% \n  mutate(obs=Ytest) %&gt;% gather(key=\"Methode\",value=\"score\",-obs)\n\n\nlibrary(plotROC)\nggplot(score)+aes(m=score,d=obs,color=Methode)+geom_roc()+theme_classic()\n\n\n\n\n\n\n\n\n\nlibrary(tidymodels)\nscore %&gt;% \n  group_by(Methode) %&gt;% \n  mutate(obs=as.factor(obs)) %&gt;%\n  roc_auc(obs,score,event_level = \"second\") %&gt;%\n  select(Methode,.estimate) %&gt;%\n  arrange(desc(.estimate))\n\n# A tibble: 6 × 2\n  Methode .estimate\n  &lt;chr&gt;       &lt;dbl&gt;\n1 lasso1      0.945\n2 lasso2      0.944\n3 ridge2      0.883\n4 ridge1      0.880\n5 logit       0.816\n6 step        0.772\n\n\n\n\n\nExercice 4 (Surapprentissage)  \n\n\nscore.app &lt;- data.frame(logit=predict(logit,newdata=dapp,type=\"response\"),\n                    step=predict(logit.step,newdata=dapp,type=\"response\"),\n                    lasso1=as.vector(predict(lasso1,type=\"response\",newx=Xapp)),\n                    ridge1=as.vector(predict(ridge1,type=\"response\",newx=Xapp)),\n                    lasso2=as.vector(predict(lasso2,type=\"response\",newx=Xapp)),\n                    ridge2=as.vector(predict(ridge2,type=\"response\",newx=Xapp))) %&gt;% \n  mutate(obs=Yapp) %&gt;% gather(key=\"Methode\",value=\"score\",-obs) \n\nOn prédit 1 si la probabilité que Y soit égale à 1 est supérieure ou égale à 0.5 :\n\nprev.app &lt;- score.app %&gt;% mutate(prev=round(score))\nprev.app %&gt;% group_by(Methode) %&gt;% summarise(Err=mean(prev!=obs)) %&gt;% arrange(Err)\n\n# A tibble: 6 × 2\n  Methode    Err\n  &lt;chr&gt;    &lt;dbl&gt;\n1 logit   0     \n2 step    0     \n3 ridge1  0.0567\n4 ridge2  0.0767\n5 lasso1  0.0967\n6 lasso2  0.11  \n\n\nOn fait de même avec l’échantillon test.\n\nprev.test &lt;- score %&gt;% mutate(prev=round(score))\nprev.test %&gt;% group_by(Methode) %&gt;% summarise(Err=mean(prev!=obs)) %&gt;% arrange(Err)\n\n# A tibble: 6 × 2\n  Methode   Err\n  &lt;chr&gt;   &lt;dbl&gt;\n1 lasso1  0.12 \n2 lasso2  0.13 \n3 ridge1  0.215\n4 ridge2  0.215\n5 step    0.23 \n6 logit   0.24 \n\n\nSur les données d’apprentissage ce sont les modèles logistiques complets et construits avec step qui ont les plus petites erreurs. Ces modèles souffrent de sur-apprentissage : ils ajustent très bien les données d’apprentissage mais ont du mal à bien prédire de nouveaux individus.",
    "crumbs": [
      "Correction des exercices",
      "IV Le modèle linéairé généralisé",
      "13 Régularisation de la vraisemblance"
    ]
  },
  {
    "objectID": "code/chap8.html",
    "href": "code/chap8.html",
    "title": "8 Régularisation des moindres carrés : Ridge, Lasso et elastic net",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names=1)[,-c(11:12)]\nozone.X &lt;- model.matrix(O3 ~ ., data = ozone)[,-1]\nozone.Y &lt;- ozone$O3\n\n\nlibrary(glmnet)\nridge &lt;- glmnet(ozone.X, ozone.Y, alpha = 0)\nlasso &lt;- glmnet(ozone.X, ozone.Y)#par défaut alpha=1\nen &lt;- glmnet(ozone.X, ozone.Y, alpha = 0.5)\n\n\nplot(ridge,main=\"Ridge\",ylim=c(-2,2))\n\n\n\n\n\n\n\nplot(ridge,xvar=\"lambda\",main=\"Ridge\",ylim=c(-2,2))\n\n\n\n\n\n\n\nplot(lasso,main=\"Lasso\",ylim=c(-2,2))\n\n\n\n\n\n\n\nplot(lasso,xvar=\"lambda\",main=\"Lasso\",ylim=c(-2,2))\n\n\n\n\n\n\n\nplot(en,main=\"Elastic net\",ylim=c(-2,2))\n\n\n\n\n\n\n\nplot(en,xvar=\"lambda\",main=\"Elastic net\",ylim=c(-2,2))\n\n\n\n\n\n\n\n\n\nset.seed(1234)\ncv.ridge &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 0)\ncv.lasso &lt;- cv.glmnet(ozone.X, ozone.Y) #alpha=1 par défaut\ncv.en &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 0.5)\n\n\ncv.ridge$lambda.min\n\n[1] 4.619799\n\ncv.ridge$lambda.1se\n\n[1] 29.69641\n\n\n\nplot(cv.ridge, main = \"Ridge\")\n\n\n\n\n\n\n\nplot(cv.lasso, main = \"Lasso\")\n\n\n\n\n\n\n\nplot(cv.en, main = \"Elastic net\")\n\n\n\n\n\n\n\n\n\nxnew &lt;- ozone.X[c(25,30),]\nrownames(xnew) &lt;- NULL\n\n\nxnew\n\n      T12  T15 Ne12 N12 S12 E12 W12   Vx   O3v\n[1,] 13.6 14.4    1   0   0   1   0 3.55  97.8\n[2,] 21.8 23.6    6   4   0   0   0 2.50 112.0\n\npredict(cv.ridge,newx=xnew)\n\n     lambda.1se\n[1,]   90.41122\n[2,]   90.65036\n\n\n\nozone&lt;-read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names = 1)\nozone.X &lt;- model.matrix(O3~.,data=ozone)\nozone.Y &lt;- ozone$O3\ncv.defaut &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassodefaut&lt;-glmnet(ozone.X,ozone.Y,lambda=cv.defaut$lambda.min)\n\n\nozone$vent &lt;- as.factor(ozone$vent)\nozone$vent &lt;- relevel(ozone$vent,ref=\"NORD\")\nozone.X &lt;- model.matrix(O3~.,data=ozone)\ncv.nord &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassonord &lt;- glmnet(ozone.X,ozone.Y,lambda=cv.nord$lambda.min)\n\n\npredict(lassodefaut,ozone.X[1:4,])\n\n               s0\n19960422 73.05686\n19960429 92.67573\n19960506 69.26897\n19960514 80.69977\n\npredict(lassonord,ozone.X[1:4,])\n\n               s0\n19960422 79.43741\n19960429 90.58414\n19960506 74.36556\n19960514 75.06648\n\n\n\nozone.X &lt;- model.matrix(O3~.-vent-nebulosite+C(vent,sum)+\n                          C(nebulosite,sum),data=ozone)\ncv.sum &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassosum &lt;- glmnet(ozone.X,ozone.Y,lambda=cv.sum$lambda.min)\npredict(lassosum,ozone.X[1:4,])\n\n               s0\n19960422 78.05238\n19960429 89.87258\n19960506 75.07092\n19960514 73.93502",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "8 Régularisation des moindres carrés : Ridge, Lasso et elastic net"
    ]
  },
  {
    "objectID": "code/chap12.html",
    "href": "code/chap12.html",
    "title": "12 Régression de Poisson",
    "section": "",
    "text": "Le modèle de Poisson\n\nMalaria &lt;- read.table(\"../donnees/poissonData3.csv\", sep=\",\", header=T)\nsummary(Malaria)\n\n     Sexe                Age            Altitude     Prevention       \n Length:1627        Min.   :  10.0   Min.   :1129   Length:1627       \n Class :character   1st Qu.: 220.0   1st Qu.:1266   Class :character  \n Mode  :character   Median : 361.0   Median :1298   Mode  :character  \n                    Mean   : 419.4   Mean   :1295                     \n                    3rd Qu.: 555.0   3rd Qu.:1320                     \n                    Max.   :1499.0   Max.   :1515                     \n                                     NA's   :105                      \n     Duree          N.malaria     \n Min.   :   0.0   Min.   : 0.000  \n 1st Qu.: 172.0   1st Qu.: 1.000  \n Median : 721.0   Median : 4.000  \n Mean   : 619.3   Mean   : 4.687  \n 3rd Qu.:1011.0   3rd Qu.: 7.000  \n Max.   :1464.0   Max.   :26.000  \n                                  \n\n\n\nmodP &lt;- glm(N.malaria ~ Duree, data = Malaria, family = poisson)\nmodP\n\n\nCall:  glm(formula = N.malaria ~ Duree, family = poisson, data = Malaria)\n\nCoefficients:\n(Intercept)        Duree  \n   0.429459     0.001508  \n\nDegrees of Freedom: 1626 Total (i.e. Null);  1625 Residual\nNull Deviance:      5710 \nResidual Deviance: 3325     AIC: 8125\n\n\n\nplot(N.malaria ~ Duree, data = Malaria,pch=20,cex=0.5)\nmod.lin &lt;- lm(N.malaria ~ Duree, data = Malaria)\nabline(a=coef(mod.lin)[1],b=coef(mod.lin)[2],lwd=2)\nx &lt;- seq(0,1500,by=1)\ny &lt;- exp(coef(modP)[1]+coef(modP)[2]*x)\nlines(x,y,col=\"red\",lty=2,lwd=2.5)\n\n\n\n\n\n\n\n\n\nmodP3 &lt;- glm( N.malaria ~ Duree + Sexe + Prevention, \n              data = Malaria,family = poisson )\n\n\n\nTests et intervalles de confiance\n\nMalaria$Prevention &lt;- as.factor(Malaria$Prevention)\nMalaria$Prevention &lt;- relevel(Malaria$Prevention,ref=\"Rien\")\nmodP3 &lt;- glm( N.malaria ~ Duree + Sexe + Prevention, data = Malaria,\n             family = poisson )\nsummary(modP3)\n\n\nCall:\nglm(formula = N.malaria ~ Duree + Sexe + Prevention, family = poisson, \n    data = Malaria)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                0.3878929  0.0389800   9.951   &lt;2e-16 ***\nDuree                      0.0015101  0.0000343  44.031   &lt;2e-16 ***\nSexeM                      0.0550890  0.0229690   2.398   0.0165 *  \nPreventionAutre           -0.2255828  0.1781379  -1.266   0.2054    \nPreventionMoustiquaire     0.0176850  0.0255967   0.691   0.4896    \nPreventionSerpentin/Spray  0.0196420  0.0590690   0.333   0.7395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5710.4  on 1626  degrees of freedom\nResidual deviance: 3317.3  on 1621  degrees of freedom\nAIC: 8124.6\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nmodP2 &lt;- glm( N.malaria ~ Duree + Sexe, data = Malaria, family = poisson)\n-2*(logLik(modP2)-logLik(modP3))\n\n'log Lik.' 2.448823 (df=3)\n\nqchisq(0.95,df=3)\n\n[1] 7.814728\n\n\n\nanova(modP2,modP3,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: N.malaria ~ Duree + Sexe\nModel 2: N.malaria ~ Duree + Sexe + Prevention\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1      1624     3319.8                     \n2      1621     3317.3  3   2.4488   0.4846\n\n\n\nlibrary(car)\nAnova(modP2,test=\"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: N.malaria\n      LR Chisq Df Pr(&gt;Chisq)    \nDuree  2386.56  1    &lt; 2e-16 ***\nSexe      5.45  1    0.01961 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nround(confint.default(modP3),3)\n\n                           2.5 % 97.5 %\n(Intercept)                0.311  0.464\nDuree                      0.001  0.002\nSexeM                      0.010  0.100\nPreventionAutre           -0.575  0.124\nPreventionMoustiquaire    -0.032  0.068\nPreventionSerpentin/Spray -0.096  0.135\n\nround(confint(modP3),3)\n\n                           2.5 % 97.5 %\n(Intercept)                0.311  0.464\nDuree                      0.001  0.002\nSexeM                      0.010  0.100\nPreventionAutre           -0.596  0.105\nPreventionMoustiquaire    -0.032  0.068\nPreventionSerpentin/Spray -0.098  0.134\n\n\n\n\nSélection de variables\n\nMalaria &lt;- read.table(\"../donnees/poissonData.csv\", sep=\",\", header=T)\nMalaria1 &lt;- na.omit(Malaria)\nMalaria1$Prevention &lt;- as.factor(Malaria1$Prevention)\nMalaria1$Sexe &lt;- as.factor(Malaria1$Sexe)\nlibrary(bestglm)\nmod_sel &lt;- bestglm(Malaria1,family=poisson)\nmod_sel$BestModels\n\n   Sexe   Age Altitude Prevention Duree Criterion\n1 FALSE  TRUE     TRUE      FALSE  TRUE  7384.946\n2 FALSE FALSE     TRUE      FALSE  TRUE  7387.814\n3  TRUE  TRUE     TRUE      FALSE  TRUE  7390.053\n4  TRUE FALSE     TRUE      FALSE  TRUE  7393.119\n5 FALSE  TRUE    FALSE      FALSE  TRUE  7401.021",
    "crumbs": [
      "Codes R",
      "IV Le modèle linéairé généralisé",
      "12 Régression de Poisson"
    ]
  },
  {
    "objectID": "correction/chap3.html",
    "href": "correction/chap3.html",
    "title": "3 Validation du modèle",
    "section": "",
    "text": "Exercice 1 (Questions de cours) C si \\(\\mathbf{1}\\) fait partie des variables ou si \\(\\mathbf{1} \\in \\Im(X)\\), A, C, C, A.\n\n\nExercice 2 (Propriétés d’une matrice de projection) La trace d’un projecteur vaut la dimension de l’espace sur lequel s’effectue la projection, donc \\(\\text{tr}(P_X)=p\\). Le second point découle de la propriété \\(P^2=P\\).\nLes matrices \\(P_X\\) et \\(P_XP_X\\) sont égales, nous avons que \\((P_X)_{ii}\\) vaut \\((P_XP_X)_{ii}\\). Cela s’écrit \\[\\begin{eqnarray*}\nh_{ii} &=&  \\sum_{k=1}^n h_{ik} h_{ki}\\\\\n&=& h_{ii}^2 + \\sum_{k=1, k \\neq i}^n h_{ik}^2\\\\\nh_{ii}(1-h_{ii}) &=& \\sum_{k=1, k \\neq i}^n h_{ik}^2.\n\\end{eqnarray*}\\] La dernière quantité de droite de l’égalité est positive et donc le troisième point est démontré. En nous servant de cet écriture les deux derniers points sont aussi démontrés.\nNous pouvons écrire \\[\\begin{eqnarray*}\nh_{ii}(1-h_{ii}) &=& h_{ij}^2 + \\sum_{k=1, k \\neq i ,j }^n h_{ik}^2.\n\\end{eqnarray*}\\] La quantité de gauche est maximum lorsque \\(h_{ii}=0.5\\) et vaut alors \\(0.25\\). Le quatrième point est démontré.\n\n\nExercice 3 (Lemme d’invertion matricielle) Commençons par effectuer les calculs en notant que la quantité \\(u'M^{-1}v\\) est un scalaire que nous noterons \\(k\\). Nous avons \\[\\begin{eqnarray*}\n\\left(M+uv'\\right)\\left(M^{-1}-\\frac{M^{-1}uv'M^{-1}}{1+u'M^{-1}v}\\right)\n&=&MM^{-1}-\\frac{MM^{-1}uv'M^{-1}}{1+k}+uv'M^{-1}\n-\\frac{uv'M^{-1}uv'M^{-1}}{1+k}\\\\\n&=&I+\\frac{-uv'M^{-1}+uv'M^{-1}+kuv'M^{-1}-ukv'M^{-1}}{1+k}.\n\\end{eqnarray*}\\] Le résultat est démontré.\n\n\nExercice 4 (Résidus studentisés)  \n\nIl suffit d’utiliser la définition du produit matriciel et de la somme matricielle et d’identifier les 2 membres des égalités.\nEn utilisant maintenant l’égalité de l’exercice précédent sur les inverses, avec \\(u=-x_i\\) et \\(v=x_i'\\), nous avons \\[\\begin{eqnarray*}\n(X'_{(i)}X_{(i)})^{-1}=(X'X-x_{i}x_{i}')^{-1}=(X'X)^{-1}+\n\\frac{(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}}\n{1-x_{i}'(X'X)^{-1}x_{i}}.%\\label{eq:hiieta}\n\\end{eqnarray*}\\] La définition de \\(h_{ii}=x_{i}'(X'X)^{-1}x_{i}\\) donne le résultat.\nCalculons la prévision où \\(\\hat \\beta_{(i)}\\) est l’estimateur de \\(\\beta\\) obtenu sans la \\(i^e\\) observation\n\n\\[\\begin{eqnarray*}\n\\hat y_i^p\n= x_{i}'\\hat \\beta_{(i)}\n&=& x_{i}' (X'_{(i)}X_{(i)})^{-1}X'_{(i)}Y_{(i)}\\\\\n&=& x_{i}'\\left[(X'X)^{-1}\n+ \\frac{(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}}{1-h_{ii}}\n\\right]\\left(X'Y-x_{i}'y_i\\right)\\\\\n&=& x_{i}' \\hat \\beta + \\frac{h_{ii}}{1-h_{ii}}x_{i}' \\hat \\beta\n- h_{ii}y_i -\\frac{h_{ii}^2}{1-h_{ii}}y_i\\\\\n&=& \\frac{1}{1-h_{ii}}\\hat y_i - \\frac{h_{ii}}{1-h_{ii}}y_i.\n\\end{eqnarray*}\\]\n\nCe dernier résultat donne \\[\\begin{eqnarray*}\n\\hat \\varepsilon_i = (1-h_{ii})(y_i-\\hat y^p_i).\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\nt^*_i &=& \\frac{\\hat \\varepsilon_i}{\\hat \\sigma_{(i)}\\sqrt{1-h_{ii}}}\\\\\n&=&\\frac{\\sqrt{(1-h_{ii})}(y_i - \\hat y_i^p)}{\\hat \\sigma_{(i)}}.\n\\end{eqnarray*}\\] Pour terminer, remarquons qu’en multipliant l’égalité de la question 3 à gauche par \\(x_{i}'\\) et à droite par \\(x_{i}\\) \\[\\begin{eqnarray*}\nx_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}\n&=& h_{ii}+ \\frac{h_{ii}^2}{1-h_{ii}}.\\\\\n1+x_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}\n&=& 1 +\\frac{h_{ii}}{1-h_{ii}}=\\frac{h_{ii}}{1-h_{ii}}.\n\\end{eqnarray*}\\]\nUtilisons l’expression \\[\\begin{eqnarray*}\nt^*_i=\\frac{y_i-\\hat y_i^p }\n{\\hat \\sigma_{(i)}\\sqrt{1+x_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}}}.\n\\end{eqnarray*}\\] Nous pouvons alors appliquer la preuve de la proposition 5.4 page 97, en constatant que la \\(i^e\\) observation est une nouvelle observation. Nous avons donc \\(n-1\\) observations pour estimer les paramètres, cela donne donc un Student à \\(n-1-p\\) paramètres.\n\n\n\nExercice 5 (Distance de Cook)  \n\nNous reprenons une partie des calculs de l’exercice précédent : \\[\\begin{eqnarray*}\n\\hat \\beta_{(i)} &=& (X'_{(i)}X_{(i)})^{-1}X'_{(i)}Y_{(i)}\\\\\n&=& (X'X)^{-1}[X'Y-x_{i}y_i]+\\frac{1}{1-h_{ii}}\n(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}[X'Y-x_{i}y_i]\\\\\n&=& \\hat \\beta - (X'X)^{-1}x_{i}y_i + \\frac{1}{1-h_{ii}}\n(X'X)^{-1}x_{i}x_{i}'\\hat \\beta - \\frac{h_{ii}}{1-h_{ii}}\n(X'X)^{-1}x_{i}y_i,\n\\end{eqnarray*}\\] d’où le résultat.\nPour obtenir la seconde écriture de la distance de Cook, nous écrivons d’abord que \\[\\begin{eqnarray*}\n\\hat \\beta_{(i)} - \\hat \\beta = \\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\n(X'X)^{-1}x_{i}.\n\\end{eqnarray*}\\] Puis nous développons \\[\\begin{eqnarray*}\nC_i &=& \\frac{1}{p \\hat \\sigma^2}(\\hat \\beta_{[i]}-\\hat \\beta)'\nX'X(\\hat \\beta_{(i)}-\\hat \\beta)\\\\\n&=& \\frac{1}{p \\hat \\sigma^2} \\left(\\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\\right)^2 x_{i}' (X'X)^{-1}(X'X)(X'X)^{-1}x_{i}.\n\\end{eqnarray*}\\] Le résultat est démontré.\n\n\n\nExercice 6 (Régression partielle) Nous avons le modèle suivant : \\[\\begin{eqnarray*}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\beta_jP_{X_{\\bar{j}}^\\perp} X_j + \\eta.\n\\end{eqnarray*}\\] L’estimateur des moindres carrés \\(\\tilde\\beta_j\\) issu de ce modèle vaut \\[\\begin{eqnarray*}\n\\tilde \\beta_j = (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y.\n\\end{eqnarray*}\\] La projection de \\(Y\\) sur \\(\\Im(X_{\\bar{j}})\\) (i.e. la prévision par le modèle sans la variable \\(X_j\\)) peut s’écrire comme la projection \\(Y\\) sur \\(\\Im(X)\\) qui est ensuite projetée sur \\(\\Im(X_{\\bar{j}})\\), puisque \\(\\Im(X_{\\bar{j}})\\subset \\Im(X)\\). Ceci s’écrit \\[\\begin{eqnarray*}\nP_{X_{\\bar{j}}}Y&=&P_{X_{\\bar{j}}}P_{X}Y=P_{X_{\\bar{j}}}X\\hat{\\beta}\n=P_{X_{\\bar{j}}}(X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jX_j)\n=X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jP_{X_{\\bar{j}}}X_j,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\nX_{\\bar{j}}\\hat\\beta_{\\bar{j}} = P_{X_{\\bar{j}}} Y -\n\\hat\\beta_jP_{X_{\\bar{j}}}X_j.\n\\end{eqnarray*}\\] Récrivons les résidus \\[\\begin{eqnarray*}\n\\hat{\\varepsilon}&=&P_{X^\\perp} Y=Y-X\\hat\\beta\n=Y-X_{\\bar{j}}\\hat\\beta_{\\bar{j}}-\\hat\\beta_jX_j\\nonumber\\\\\n&=&Y-P_{X_{\\bar{j}}}Y + \\hat\\beta_jP_{X_{\\bar{j}}}X_j  -\\hat\\beta_j X_j\\nonumber\\\\\n&=&(I-P_{X_{\\bar{j}}})Y - \\hat\\beta_j(I-P_{X_{\\bar{j}}})X_j\\nonumber\\\\\n&=&P_{X_{\\bar{j}}^\\perp} Y-\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j.%\\label{eq:origine:residpartiel}\n\\end{eqnarray*}\\] En réordonnant cette dernière égalité, nous pouvons écrire \\[\\begin{eqnarray}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon}.\\nonumber\n\\end{eqnarray}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\tilde\\beta_j &=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y\\\\\n&=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j(\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon})\\\\\n&=& \\hat\\beta_j +(X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1} X'_j\\hat{\\varepsilon}).\n\\end{eqnarray*}\\] Le produit scalaire \\(X'_j\\hat{\\varepsilon} = \\langle X_j,\\hat{\\varepsilon} \\rangle\\) est nul car les deux vecteurs appartiennent à des sous-espaces orthogonaux, d’où le résultat.\n\n\nExercice 7 (TP : Résidus partiels)  \n\nImportation\n\ndon &lt;- read.table(\"../donnees/tprespartiel.dta\", header=TRUE, sep=\";\")\n\nEstimation du modèle\n\nmod &lt;- lm(Y~X1+X2+X3+X4, data=don)\n\nAnalyse des résidus partiels. Commençons par calculer les résidus partiels (matrice \\(n\\times p\\))\n\nrpartiel &lt;- residuals(mod, type=\"partial\")\n\nCréons un data-frame à 3 colonnes et \\(np\\) lignes: la colonne de nom de la variable (X1 répété \\(n\\) fois, X2 répété \\(n\\) fois, X3 répété \\(n\\) fois et X4 répété \\(n\\) fois), les variables X1, X2, X3 et X4 et les résidus partiels.\n\nnoms &lt;- rep(names(don)[1:4], each=nrow(don))\nX &lt;-  as.vector(data.matrix(don[,1:4]))\ndonlong &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\n\nLa représentation avec est donnée par\n\nlibrary(ggplot2)\nggplot(donlong, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\n\n\n\n\nLes 3 premières variables montrent des tendances linéaires (ou aucune pour la troisième) alors que la troisième semble montrer plutôt une tendance quadratique.\nRefaisons le modèle avec X5 :\n\ndon &lt;- cbind.data.frame(don, X5=don$X4^{2})\nmod2 &lt;- lm(Y~X1+X2+X3+X5, data=don)\nrpartiel &lt;- residuals(mod2, type=\"partial\")\nnoms &lt;- rep(names(don)[c(1:3,6)], each=nrow(don))\nX &lt;-  as.vector(data.matrix(don[,c(1:3,6)]))\ndonlong2 &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\nggplot(donlong2, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\n\n\n\n\net nous constatons que les résidus partiels sont tous à tendance linéaire. Les 2 modèles ayant le même nombre de variables nous pouvons les comparer via leur \\(\\mathop{\\mathrm{R^2}}\\) qui valent 0.986 et 0.9966. La seconde modélisation est la meilleure tant pour la qualité globale que pour l’analyse des résidus.\nAvec le second jeu de données\n\ndonbis &lt;- read.table(\"../donnees/tpbisrespartiel.dta\", header=TRUE, sep=\";\")\nmodbis &lt;- lm(Y~X1+X2+X3+X4, data=donbis)\nrpartiel &lt;- residuals(modbis, type=\"partial\")\nnoms &lt;- rep(names(donbis)[1:4], each=nrow(donbis))\nX &lt;-  as.vector(data.matrix(donbis[,1:4]))\ndonlongbis &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\nggplot(donlongbis, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\n\n\n\n\nNous voyons clairement une sinusoïde de type \\(\\sin(-2\\pi X_4)\\) sur le dernier graphique. Changeons X4\n\ndonbis &lt;- cbind.data.frame(donbis, X5=sin(-2*pi*donbis$X4))\nmodbis2 &lt;- lm(Y~X1+X2+X3+X5, data=donbis)\nrpartiel &lt;- residuals(modbis2, type=\"partial\")\nnoms &lt;- rep(names(donbis)[c(1:3,6)], each=nrow(donbis))\nX &lt;-  as.vector(data.matrix(donbis[,c(1:3,6)]))\ndonlongbis2 &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\nggplot(donlongbis2, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\n\n\n\n\nLes résidus partiels sont tous à tendance linéaire et donc corrects. La qualité globale analysée par \\(\\mathop{\\mathrm{R^2}}\\) augmente elle aussi de 0.8106 à 0.9985.",
    "crumbs": [
      "Correction des exercices",
      "I Introduction au modèle linéaire",
      "3 Validation du modèle"
    ]
  },
  {
    "objectID": "code/chap9.html",
    "href": "code/chap9.html",
    "title": "9 Régression sur composantes : PCR et PLS",
    "section": "",
    "text": "Régression MCO et choix de variables\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names=1)\nmodeleinit &lt;- lm(O3 ~ ., data = ozone[,1:10])\nround(coefficients(modeleinit),2)\n\n(Intercept)         T12         T15        Ne12         N12         S12 \n      54.73       -0.35        1.50       -4.19        1.28        3.17 \n        E12         W12          Vx         O3v \n       0.53        2.47        0.61        0.25 \n\nBIC(modeleinit)\n\n[1] 431.8923\n\n\n\nlibrary(leaps)\nchoix &lt;- regsubsets(O3 ~ .,nbest=1,nvmax=10,data=ozone[,1:10])\nresume &lt;- summary(choix)\nindmin &lt;- which.min(resume$bic)\nnomselec &lt;- colnames(resume$which)[resume$which[indmin,]][-1]\nformule &lt;- formula(paste(\"O3~\",paste(nomselec,collapse=\"+\")))\nmodeleBIC &lt;- lm(formule,data=ozone[,1:10])\nround(coefficients(modeleBIC),2)\n\n(Intercept)         T15        Ne12          Vx         O3v \n      61.83        1.06       -3.99        0.31        0.26 \n\nBIC(modeleBIC)\n\n[1] 415.8866\n\n\n\n\nMise en place des données centrées réduites\n\nX &lt;- ozone[,2:10]\nXbar  &lt;- apply(X, 2, mean)\nstdX &lt;- sqrt(apply(X, 2, var))\nXcr &lt;- scale(X, center = Xbar, scale = stdX)\n\n\n\nPCR\n\nlibrary(pls)\nset.seed(87)\ncvseg &lt;- cvsegments(nrow(ozone), k = 4, type = \"random\")\nn.app &lt;- nrow(ozone)\nmodele.pcr &lt;- pcr(O3 ~ ., ncomp=9, data=ozone[,1:10], scale=T,\n                   validation = \"CV\", segments = cvseg)\nmsepcv.pcr &lt;- MSEP(modele.pcr ,estimate=c(\"train\",\"CV\")) \nmsepcv.pcr\n\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\ntrain        559.8    188.7    186.5    185.2    179.8    158.8    152.0\nCV           582.9    260.4    260.6    278.2    271.3    239.3    248.1\n       7 comps  8 comps  9 comps\ntrain    143.9    142.5    139.7\nCV       242.1    244.0    239.4\n\n\n\nnpcr &lt;- which.min(msepcv.pcr$val[\"CV\",,])-1\nmodele.pcr.fin &lt;- pcr(O3 ~ ., ncomp = npcr, scale = TRUE,data = ozone[,1:10])\n\n\nX &lt;- ozone[,2:10]\nY &lt;- ozone[,1]\nn &lt;- nrow(X)\nXbar  &lt;- apply(X,2,mean)\nstdX &lt;- sqrt(apply(X,2,var)*(n-1)/n)\nYbar &lt;- mean(Y)\nmodele.pcr.fin &lt;- pcr(O3~.,ncomp=1,scale=TRUE,data =ozone[,1:10])\nbetafinpcr &lt;- matrix(coefficients(modele.pcr.fin),ncol=1)/stdX\nmu &lt;- mean(Y)-Xbar%*%betafinpcr\nc(mu,betafinpcr)\n\n [1] 48.3373569  0.7884931  0.8236705 -1.7618991 -0.6760911  0.2126764\n [7]  1.6099503 -1.4694047  0.3118176  0.1662505\n\n\n\n\nPLS\n\nset.seed(87)\ncvseg &lt;- cvsegments(nrow(ozone), k = 4, type = \"random\")\nn.app &lt;- nrow(ozone)\nmodele.pls &lt;- plsr(O3 ~ ., ncomp=9, data = ozone[,1:10], scale=T,validation = \"CV\", segments = cvseg)\nmsepcv.pls &lt;- MSEP(modele.pls ,estimate=c(\"train\",\"CV\")) \nmsepcv.pls\n\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\ntrain        559.8    173.9    150.9    146.9    144.2    142.1    141.4\nCV           582.9    251.9    248.4    245.3    234.6    241.7    243.6\n       7 comps  8 comps  9 comps\ntrain    140.9    139.8    139.7\nCV       234.7    239.6    239.4\n\n\n\nnpls &lt;- which.min(msepcv.pls$val[\"CV\",,])-1\nmodele.pls.fin &lt;- plsr(O3~ . , ncomp  =npls, scale = TRUE,data = ozone[,1:10])\n\n\nX &lt;- ozone[,2:10]\nY &lt;- ozone[,1]\nn &lt;- nrow(X)\nXbar  &lt;- apply(X,2,mean)\nstdX &lt;- sqrt(apply(X,2,var)*(n-1)/n)\nYbar &lt;- mean(Y)\nmodele.pls.fin &lt;- plsr(O3~.,ncomp=1,scale=TRUE,data =ozone[,1:10])\nbetafinpls &lt;- matrix(coefficients(modele.pls.fin),ncol=1)/stdX\nmu &lt;- mean(Y)-Xbar%*%betafinpls\nc(mu,betafinpls)\n\n [1] 47.1060475  0.8151834  0.8471154 -2.1910455 -0.5175855  0.3563688\n [7]  1.3005636 -1.2552108  0.2805513  0.1920499",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "9 Régression sur composantes : PCR et PLS"
    ]
  },
  {
    "objectID": "correction/chap8.html",
    "href": "correction/chap8.html",
    "title": "8 Régularisation des moindre carrés : Ridge, Lasso et elastic net",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, B, B, B, A (pour un bon choix de \\(\\lambda\\)) et B, A, C et D.\n\n\nExercice 2 (Projection et régression ridge) L’ajustement avec la méthode ridge est \\[\n\\begin{align*}\n\\hat Y_{\\mathrm{ridge}}&= X(X'X - \\kappa I)^{-1}X'Y=H^*(\\kappa)Y.\n\\end{align*}\n\\] Nous avons que \\(H^*(\\kappa)H^*(\\kappa) \\neq H^*(\\kappa)\\) sauf si \\(\\kappa=0\\) (cas des MC) et donc ce n’est pas un projecteur.\n\n\nExercice 3 (Variance des valeurs ajustées avec une régression ridge) \\(X\\) de rang \\(p\\). Soit la décomposition en valeurs singulières de \\(X\\)~: \\[\n\\begin{align*}\nX &=U\\Lambda V'\n\\end{align*}\n\\] avec \\(U\\) de dimension \\(n\\times p\\) (avec \\(U'U=I_p\\) mais nous n’avons pas \\(UU'\\)…), \\(V\\) \\(p\\times p\\) (avec \\(V'V=VV'=I_p\\)) et \\(\\Lambda=\\text{diag}(\\lambda_j)\\).\nDe cette décomposition et de ses propriétés nous déduisons \\[\n\\begin{align*}\nX' &=V\\Lambda U'\\\\\nX'X &=V\\Lambda^2 V'\n(X'X)^{-1} &=V\\Lambda^{-2} V'\\\\\n(X'X + \\kappa I)^{-1} &=(V\\Lambda^2 V' + \\kappa VV')^{-1} = V(\\text{diag}(\\lambda_j^2 + \\kappa)^{-1} V'\n\\end{align*}\n\\] L’ajustement avec la méthode ridge est \\[\n\\begin{align*}\n  \\hat Y_{\\mathrm{ridge}}&= X(X'X - \\kappa I)^{-1}X'Y=U\\Lambda V'\n  V(\\text{diag}(1/(\\lambda_j^2 + \\kappa))V'V\\Lambda U'Y = U\\Lambda \\text{diag}(1/(\\lambda_j^2 + \\kappa)) \\Lambda U'Y\n  =U\\text{diag}(\\lambda_j^2/(\\lambda_j^2 + \\kappa))U'Y\n\\end{align*}\n\\] et celui des MCO est \\[\n\\begin{align*}\n  \\hat Y&= X(X'X)^{-1}X'Y=U\\Lambda V'\n  V(\\Lambda^{-2}) V'V\\Lambda U'Y = UU'Y.\n\\end{align*}\n\\]\nCalculons leur norme carrée \\[\n\\begin{align*}\n  \\|\\hat Y_{\\mathrm{ridge}}\\|^2&=\\hat Y_{\\mathrm{ridge}}'\\hat Y_{\\mathrm{ridge}}\n  =Y'U\\text{diag}(\\lambda_j^2/(\\lambda_j^2 + \\kappa))U'U\\text{diag}(\\lambda_j^2/(\\lambda_j^2 + \\kappa))U'Y=Y'U\\text{diag}(\\lambda_j^4/(\\lambda_j^2 + \\kappa)^2)U'Y\\\\\n\\|\\hat Y\\|^2&=\\hat Y\\hat Y\n  =Y'UU'UU'Y=Y'UU'Y\\\\\n\\end{align*}\n\\] Comme \\(X\\) est de rang \\(p\\) on a \\(\\lambda_j&gt;0\\) pour tout \\(j\\). Si on choisit \\(\\kappa\\) tel que \\(\\lambda_j^4/(\\lambda_j^2 + \\kappa)^2&lt;1\\) pour toutes les valeurs \\(j\\) on obtient le résultat.\n\\(X\\) de rang \\(r&lt;p\\). Nous avons toujours la décomposition en valeurs singulières de \\(X\\). Nous pouvons l’écrire comme \\[\n\\begin{align*}\nX &=U\\Lambda V'\n\\end{align*}\n\\] (avec \\(U'U=I_p\\) mais nous n’avons pas \\(UU'\\)…), \\(V\\) \\(p\\times p\\) (avec \\(V'V=VV'=I_p\\)) et \\(\\Lambda=\\text{diag}(\\lambda_1,\\dotsc,\\lambda_r,0,\\dotsc, 0)\\) mais après le rang \\(r\\) (avec \\(r&lt;p\\)) les valeurs singulières sont nulles.\nIci \\(\\Lambda\\) n’est plus inversible ou encore \\(X'X\\) n’est pas inversible. On ne peut donc écrire l’ajustement par MCO comme \\(X(X'X)^{-1}X'Y\\) car cela n’a pas de sens, mais on peut prendre l’inverse généralisé de Moore-Penrose à la place de l’inverse: \\[\n\\begin{align*}\n  \\hat Y&=P_X Y=  X(X'X)^{+}X'Y\n\\end{align*}\n\\] Techniquement pour calculer cet inverse il suffit d’inverser les valeurs propres non nulles (ou les carrés des valeurs singulières) et de laisser les autres à zéros: \\[\n\\begin{align*}\n  (X'X)^{+}&=V\\text{diag}(\\frac{1}{\\lambda^2_1},\\dotsc,\\frac{1}{\\lambda^2_r},0,\\dotsc,0)V'\n\\end{align*}\n\\] Si on note (avec un abus de notation) \\(\\Lambda^{-2}=\\text{diag}(\\frac{1}{\\lambda^2_1},\\dotsc,\\frac{1}{\\lambda^2_r},0,\\dotsc,0)\\) on a alors les mêmes formules pour les ajustements et les normes et donc la même conclusion.\n\n\nExercice 4 (Nombre effectif de paramètres de la régression ridge)  \n\nRappelons que pour une valeur \\(\\kappa\\) donnée, le vecteur de coefficients de la régression ridge s’écrit \\[\n\\hat \\beta_{\\mathrm{ridge}}(\\kappa) = (X'X + \\kappa I)^{-1}X'Y.\n\\] et donc l’ajustement par la régression ridge est \\[\n\\hat Y_{\\mathrm{ridge}}(\\kappa)=X(X'X + \\kappa I)^{-1}X'Y=H^*(\\kappa)Y\n\\]\nSoit \\(U_i\\) le vecteur propre de \\(A\\) associé à la valeur propre \\(d^2_i\\). Nous avons donc par définition que \\[\n\\begin{eqnarray*}\nAU_i&=&d^2_iU_i\\\\\nAU_i+\\lambda U_i&=&d^2_iU_i+\\lambda U_i=(d^2_i+\\lambda) U_i\\\\\n(A+\\lambda I_p)U_i&=&(d^2_i+\\lambda) U_i,\n\\end{eqnarray*}\n\\] c’est-à-dire que \\(U_i\\) est aussi vecteur propre de \\(A+\\lambda I_p\\) associé à la valeur propre \\(\\lambda+d^2_i\\).\nNous savons que \\(X=QD P'\\) avec \\(Q\\) et \\(P\\) matrices orthogonales et \\(D=\\text{diag}(d_1,\\dotsc,d_p)\\). Puisque \\(Q\\) est orthogonale, nous avons, par définition, \\(Q'Q=I\\). Nous avons donc que \\(X'X=(QD P')'QD P'=PDQ'QDP'=PD^2P'\\). Puisque \\(P\\) est orthogonale \\(P'P=I_p\\) et \\(P^{-1}=P\\). \\[\n\\begin{eqnarray*}\n\\text{tr}(X(X'X+\\lambda I_p)^{-1}X')&=&\\text{tr}((X'X+\\lambda I_p)^{-1}X'X)\\\\\n&=&\\text{tr}((PD^2P'+\\lambda PP')^{-1}PD^2P')\\\\\n&=&\\text{tr}((P(D+\\lambda I_p )P')^{-1}PD^2P').\n\\end{eqnarray*}\n\\] Ainsi \\[\n\\begin{eqnarray*}\n\\text{tr}(X(X'X+\\lambda I_p)^{-1}X')&=&\\text{tr}( (P')^{-1}(D+\\lambda I_p )^{-1} P^{-1} PD^2P')\\\\\n&=&\\text{tr}( (P')^{-1}(D+\\lambda I_p )^{-1} D^2P')\\\\\n&=&\\text{tr}( (D+\\lambda I_p )^{-1} D^2).\n\\end{eqnarray*}\n\\] Selon la définition de \\(H^*(\\kappa)\\), nous savons que sa trace vaut donc \\[\n\\begin{eqnarray*}\n\\text{tr}( (D+\\kappa I_p )^{-1} D^2).\n\\end{eqnarray*}\n\\] Comme \\(D\\) et \\(I_p\\) sont des matrices diagonales, leur somme et produit sont simplement leur somme et produit terme à terme des éléments de la diagonale, et donc cette trace (somme des éléments de la diagonale) vaut \\[\n\\sum_{i=1}^{p}{\\frac{d_j^2}{d_j^2+\\kappa}}.\n\\]\n\n\n\nExercice 5 (Estimateurs à rétrecissement - shrinkage)  \n\nSoit le modèle de régression \\[\nY=X\\beta+\\varepsilon.\n\\] En le pré-multipliant par \\(P\\), nous avons \\[\nZ=PY=PX\\beta+P\\varepsilon=DQ\\beta+\\eta=D\\gamma+\\eta.\n\\] Puisque \\(\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2 I_n)\\) et \\(P\\) fixé, nous avons que \\(\\eta=P\\varepsilon\\) suit une loi normale de moyenne \\(\\mathbf E(\\eta)=P\\mathbf E(\\varepsilon)=0\\) et de variance \\(\\mathop{\\mathrm{V}}(\\eta)=P\\mathop{\\mathrm{V}}(\\varepsilon)P'=\\sigma^2PP'=\\sigma^2I_n\\).\nPar définition, \\(Z\\) vaut \\(PY\\) et nous savons que \\(Y\\sim\\mathcal{N}(X\\beta,\\sigma^2 I_n)\\), donc \\(Z\\sim\\mathcal{N}(PX\\beta,\\sigma^2 PP')\\), c’est-à-dire \\(Z\\sim\\mathcal{N}(DQ\\beta,\\sigma^2 I_n)\\) ou encore \\(Z\\sim\\mathcal{N}(D\\gamma,\\sigma^2 I_n)\\). En utilisant la valeur de \\(D\\) nous avons \\[    \n\\begin{eqnarray*}\nD\\gamma&=&\n\\begin{pmatrix}\n  \\Delta \\gamma\\\\\n0\n\\end{pmatrix}.\n\\end{eqnarray*}\n\\] Donc \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2I_p)\\).\nSoit un estimateur de \\(\\beta\\) linéaire en \\(Y\\)~: \\(\\hat \\beta=AY\\). Soit l’estimateur de \\(\\gamma=Q\\beta\\) linéaire en \\(Y\\)~: \\(\\hat\\gamma=Q AY\\). Pour calculer leur matrice de l’EQM, nous devons calculer leur biais et leur variance. Le biais de \\(\\hat \\beta\\) est \\[\nB(\\hat \\beta)=\\mathbf E(\\hat \\beta)-\\beta=\\mathbf E(AY)-\\beta=A\\mathbf E(Y)-\\beta=AX\\beta-\\beta.\n\\] Le biais de \\(\\hat\\gamma\\) s’écrit \\[\nB(\\hat\\gamma)=\\mathbf E(\\hat \\gamma)-\\gamma=\\mathbf E(Q\\hat \\beta)-\\gamma=Q\\mathbf E(\\hat \\beta)-\\gamma=QAX\\beta-\\gamma.\n\\] Comme \\(\\gamma=Q\\beta\\) et \\(Q'Q=I_p\\) nous avons \\[\nB(\\hat\\gamma)=QAXQ'\\gamma-\\gamma.\n\\] La variance de \\(\\hat \\beta\\) s’écrit \\[\n\\mathop{\\mathrm{V}}(\\hat \\beta)=\\mathop{\\mathrm{V}}(AY)=A\\mathop{\\mathrm{V}}(Y)A'=\\sigma^2 AA',\n\\] et celle de \\(\\hat \\gamma\\) est \\[\n\\mathop{\\mathrm{V}}(\\hat\\gamma)=\\mathop{\\mathrm{V}}(Q\\hat \\beta)=Q\\mathop{\\mathrm{V}}(\\hat \\beta)Q'=\\sigma^2 QAA'Q'.\n\\] Nous en déduisons que les matrices des EQM sont respectivement \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta)&=&(AX\\beta-\\beta)(AX\\beta-\\beta)'+\\sigma^2 AA',\\\\\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma)&=&(QAXQ'\\gamma-\\gamma)(QAXQ'\\gamma-\\gamma)' + \\sigma^2 QAA'Q',\n\\end{eqnarray*}\n\\] et enfin les traces de ces matrices s’écrivent \\[\n\\begin{eqnarray*}\n\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\beta))&=&(AX\\beta-\\beta)'(AX\\beta-\\beta)+\\sigma^2\\text{tr}(AA'),\\\\\n\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\gamma))&=&(QAXQ'\\gamma-\\gamma)'(QAXQ'\\gamma-\\gamma)+ \\sigma^2\\text{tr}(AA').\\\\\n\\end{eqnarray*}\n\\] Rappelons que \\(\\gamma=Q\\beta\\) et que \\(Q'Q=I_p\\), nous avons donc \\[\n\\begin{eqnarray*}\n\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\gamma))&=&\\gamma'(QAXQ'-I_p)'(QAXQ'-I_p)\\gamma+ \\sigma^2\\text{tr}(AA')\\\\\n&=&\\beta'(QAX - Q)'(QAX - Q)\\beta+ \\sigma^2\\text{tr}(AA')\\\\\n&=&\\beta'(AX-I_p)Q'Q(AX-I_p)\\beta+ \\sigma^2\\text{tr}(AA')\\\\\n&=&\\beta'(AX-I_p)(AX-I_p)\\beta+ \\sigma^2\\text{tr}(AA')=\\text{tr}(\\mathop{\\mathrm{EQM}}(\\hat \\beta)).\n\\end{eqnarray*}\n\\] En conclusion, que l’on s’intéresse à un estimateur linéaire de \\(\\beta\\) ou à un estimateur linéaire de \\(\\gamma\\), dès que l’on passe de l’un à l’autre en multipliant par \\(Q\\) ou \\(Q'\\), matrice orthogonale, la trace de l’EQM est identique, c’est-à-dire que les performances globales des 2 estimateurs sont identiques.\nNous avons le modèle de régression suivant : \\[\nZ_{1:p}=\\Delta\\gamma+\\eta_{1:p},\n\\] et donc, par définition de l’estimateur des MC, nous avons \\[\n\\hat \\gamma_{\\mathrm{MC}}=(\\Delta'\\Delta)^{-1}\\Delta'Z_{1:p}.\n\\] Comme \\(\\Delta\\) est une matrice diagonale, nous avons \\[\n\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-2}\\Delta'Z_{1:p}=\\Delta^{-1}Z_{1:p}.\n\\] Cet estimateur est d’expression très simple et il est toujours défini de manière unique, ce qui n’est pas forcément le cas de \\(\\hat \\beta_{\\mathrm{MC}}\\).\nComme \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p)\\) nous avons que \\(\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-1}Z_{1:p}\\) suit une loi normale d’espérance \\(\\mathbf E(\\Delta^{-1}Z_{1:p})=\\Delta^{-1}\\mathbf E(Z_{1:p})=\\gamma\\) et de variance \\(\\mathop{\\mathrm{V}}(\\hat \\gamma_{\\mathrm{MC}})=\\sigma^2\\Delta^{-2}\\). Puisque \\(\\hat \\gamma_{\\mathrm{MC}}\\) est un estimateur des MC, il est sans biais, ce qui est habituel.\nL’EQM de \\(\\hat \\gamma_{\\mathrm{MC}}\\), estimateur sans biais, est simplement sa variance. Pour la \\(i^e\\) coordonnée de \\(\\hat \\gamma_{\\mathrm{MC}}\\), l’EQM est égal à l’élément \\(i,i\\) de la matrice de variance \\(\\mathop{\\mathrm{V}}(\\hat \\gamma_{\\mathrm{MC}})\\), c’est-à-dire \\(\\sigma^2/\\delta_i^2\\). La trace de l’EQM est alors simplement la somme, sur toutes les coordonnées \\(i\\), de cet EQM obtenu.\nPar définition \\(\\hat \\gamma(c)=\\text{diag}(c_i)Z_{1:p}\\) et nous savons que \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p).\\) Nous obtenons que \\(\\hat \\gamma(c)\\) suit une loi normale d’espérance \\(\\mathbf E(\\text{diag}(c_i)Z_{1:p})=\\text{diag}(c_i)\\Delta\\gamma\\) et de variance \\[\n\\mathop{\\mathrm{V}}(\\hat \\gamma(c))= \\text{diag}(c_i)\\mathop{\\mathrm{V}}(Z_{1:p})\\text{diag}(c_i)'= \\sigma^2\\text{diag}(c_i^2).\n\\] La loi de \\(\\hat \\gamma(c)\\) étant une loi normale de matrice de variance diagonale, nous en déduisons que les coordonnées de \\(\\hat \\gamma(c)\\) sont indépendantes entre elles.\nCalculons l’EQM de la \\(i^e\\) coordonnée de \\(\\hat \\gamma(c)\\) \\[\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)=\\mathbf E(\\hat \\gamma(c)_i -\\gamma)^2=\\mathbf E(\\hat \\gamma(c)_i^2)+\n\\mathbf E(\\gamma_i^2)-2\\mathbf E(\\hat \\gamma(c)_i \\gamma_i).\n\\] Comme \\(\\gamma_i\\) et que \\(\\mathbf E(\\hat \\gamma(c)_i^2)=\\mathop{\\mathrm{V}}(\\hat \\gamma(c)_i^2)+\\{\\mathbf E(\\hat \\gamma(c)_i^2)\\}^2\\), nous avons \\[\n\\begin{align*}\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\gamma_i\\mathbf E(\\hat \\gamma(c)_i)\\\\\n&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\sigma^2 c_i\\delta_i\\gamma_i= \\sigma^2c_i^2+\\gamma_i^2(c_i\\delta_i -1)^2.\n\\end{align*}\n\\]\nDe manière évidente si \\(\\gamma_i^2\\) diminue, alors l’EQM de \\(\\hat \\gamma(c)_i\\) diminue aussi. Calculons la valeur de l’EQM quand \\(\\gamma_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\). Nous avons, grâce à la question précédente, \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)&=&\\sigma^2 c_i^2+(c_i\\delta_i -1)^2\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)^2\\frac{1+\\delta_ic_i}{1-\\delta_ic_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)(1+\\delta_ic_i)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1-\\delta_i^2c_i^2)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}-\\sigma^2c_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\\\\n&=&\\mathop{\\mathrm{EQM}}(\\hat \\gamma_{\\mathrm{MC}}),\n\\end{eqnarray*}\n\\] d’où la conclusion demandée.\nPar définition de \\(\\hat \\gamma(c)\\), nous avons \\[\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&\\text{diag}(c_i)Z_{1:p}=\\text{diag}(\\frac{\\delta_i}{\\delta_i^2+\\kappa})Z_{1:p}\\\\\n&=&(\\Delta'\\Delta + \\kappa I_p)^{-1}\\Delta'Z_{1:p},\n\\end{eqnarray*}\n\\] puisque \\(\\Delta\\) est diagonale. De plus nous avons \\[\nD =\n\\bigl( \\begin{smallmatrix}\n  \\Delta\\\\\n0\n\\end{smallmatrix}\\bigr),\n\\] ce qui entraîne que \\(D'D=\\Delta'\\Delta\\) et \\(D'Z=\\Delta' Z_{1:p}\\). Nous obtenons donc \\[\n\\hat \\gamma(c)=(D'D+\\kappa I_p)^{-1}D'Z.\n\\] Rappelons que \\(D=PXQ'\\) avec \\(P\\) et \\(Q\\) matrices orthogonales, nous avons alors \\[\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&(QX'P'PXQ' + \\kappa I_p)^{-1} D'Z=(QX'XQ' + \\kappa QQ')^{-1}D'Z\\\\\n&=&(Q(X'X  + \\kappa I_p)Q')^{-1}D'Z=(Q')^{-1}(X'X  + \\kappa I_p)^{-1}(Q)^{-1}D'Z\\\\\n&=&Q(X'X  + \\kappa I_p)^{-1}Q'D'Z.\n\\end{eqnarray*}\n\\] Comme \\(Z=PY\\) et \\(D=PXQ'\\), nous avons \\[\n\\hat \\gamma(c)=Q(X'X  + \\kappa I_p)^{-1}Q' QX'P' PY=Q(X'X  + \\kappa I_p)^{-1}XY.\n\\] Enfin, nous savons que \\(Q\\hat\\gamma=\\hat \\beta\\), nous en déduisons que \\(\\hat\\gamma=Q'\\hat \\beta\\) et donc que dans le cas particulier où \\(c_i=\\frac{\\delta_i}{\\delta_i^2+\\kappa}\\) nous obtenons \\[\n\\hat \\beta=Q\\hat \\gamma(c)=(X'X  + \\kappa I_p)^{-1}XY,\n\\] c’est-à-dire l’estimateur de la régression ridge.\n\n\n\nExercice 6 (Coefficient constant et régression sous contraintes)  \n\nComme chaque variable \\(X_j\\), \\(1\\leq j \\leq (p-1)\\), est centrée on a \\[\n\\begin{align*}\n  \\bar X_j =0 =\\frac{1}{n}&lt;\\mathbf{1}_n, X_j&gt;\n\\end{align*}\n\\] donc \\(\\forall z\\in\\mathsf{Vect}(X_\\xi)\\) on a \\(z=X_\\xi\\alpha\\) avec \\(\\alpha\\in\\mathbb R^{p-1}\\) et donc \\[%\n\\begin{align*}\n  &lt;\\mathbf{1}, z&gt;&=\\mathbf{1}' X_\\xi\\alpha = 0\n\\end{align*}\n\\] ou écrit autrement \\(\\mathsf{Vect}(\\mathbf{1}) \\stackrel{\\perp}{\\oplus} \\mathsf{Vect}(X_{\\xi})\\).\nLa fonction à minimiser est \\(\\|Y-X\\beta\\|^2+\\lambda J(\\beta_\\xi)\\) et on a la décomposition unique de \\(Y\\): \\(Y=P_X Y + P_{X^\\perp} Y\\) que l’on peut insérer dans la fonction à minimiser: \\[%\n\\begin{align*}\n\\|Y-X\\beta\\|^2+\\lambda J(\\beta_\\xi)&=\\|P_X Y + P_{X^\\perp} Y-X\\beta\\|^2+\\lambda J(\\beta_\\xi)\n\\end{align*}\n\\] Comme \\(X\\beta\\) est dans \\(\\mathsf{Vect}(X)\\) donc par Pythagore on a \\[%\n\\begin{align*}\n\\|Y-X\\beta\\|^2+\\lambda J(\\beta_\\xi)&=\\|P_{X^\\perp} Y\\|^2 + \\|P_X Y-X\\beta\\|^2+\\lambda J(\\beta_\\xi)\n\\end{align*}\n\\] L’argument du minimum (sur \\(\\beta\\)) ne dépend pas de \\(\\|P_{X^\\perp} Y\\|^2\\) (qui ne dépend pas de \\(\\beta\\)) et donc nous pouvons donc minimiser la fonction \\[\n\\begin{align*}\n\\|P_X Y-X\\beta\\|^2+\\lambda J(\\beta_\\xi)\n\\end{align*}\n\\]\nDe la question 1. nous avons que \\(P_X Y= P_\\mathbf{1}Y + P_{X_\\xi} Y\\) et donc par Pythagore nous pouvons minimiser la fonction \\[\n\\begin{align*}\n\\|P_X Y-X\\beta\\|^2+\\lambda J(\\beta_\\xi) = \\|P_\\mathbf{1}Y - \\mathbf{1}\\beta_p \\|^2 +  \\|P_{X_\\xi} Y-X_\\xi\\beta_\\xi\\|^2+\\lambda J(\\beta_\\xi)\n   \\end{align*}\n\\] Comme \\(P_\\mathbf{1}Y = \\mathbf{1}(\\mathbf{1}'\\mathbf{1})^{-1}\\mathbf{1}' Y = \\bar y \\mathbf{1}\\) on a donc que \\(\\hat\\beta_p =\\bar y\\) et que les autres coefficients sont obtenus en minimisant \\[\n\\begin{align*}\n\\|P_{X_\\xi} Y-X_\\xi\\beta_\\xi\\|^2+\\lambda J(\\beta_\\xi)\n\\end{align*}\n\\] ou, en refaisant le même raisonnement qu’en question 2 avec \\(Y=P_{X_\\xi} Y + P_{X_\\xi^\\perp}Y\\) et Pythagore, on peut aussi minimiser \\[\n\\begin{align*}\n\\|Y-X_\\xi\\beta_\\xi\\|^2+\\lambda J(\\beta_\\xi).\n\\end{align*}\n\\]\n\n\n\nExercice 7 (Unicité pour la régression lasso, Giraud (2014))  \n\nPrenons deux solutions du lasso \\(\\hat\\beta_1\\) et \\(\\hat\\beta_2\\), on a donc qu’ils réalisent le minimum: \\[\n\\begin{align*}\n\\|Y-X\\hat\\beta_1\\|^2+\\lambda \\|\\hat\\beta_1\\|_1=\\|Y-X\\hat\\beta_2\\|^2+\\lambda \\|\\hat\\beta_2\\|_1 = \\mathrm{mini}\n\\end{align*}\n\\] Prenons le milieu \\(\\hat \\beta = (\\hat\\beta_1 + \\hat\\beta_2)/2\\) on a donc \\[\n\\begin{align*}\n  \\|Y-X\\hat\\beta\\|^2+\\lambda \\|\\hat\\beta\\|_1&=\n  \\|Y-X\\hat\\beta_1/2 -X\\hat\\beta_2/2 \\|^2 + \\lambda \\|\\hat\\beta_1/2 + \\hat\\beta_2/2\\|_1\n\\end{align*}\n\\] Comme \\(\\|.\\|_1\\) est convexe on a donc que \\[\n\\begin{align*}\n\\lambda \\|\\hat\\beta_1/2 + \\hat\\beta_2/2\\|_1\\leq \\lambda \\|\\hat\\beta_1/2\\|_1 + \\|\\hat\\beta_2/2\\|_1 = \\lambda/2 (\\|\\hat\\beta_1\\|_1 + \\|\\hat\\beta_2\\|_1 )\n\\end{align*}\n\\] D’où l’inégalité \\[\n\\begin{align*}\n  \\|Y-X\\hat\\beta\\|^2+\\lambda \\|\\hat\\beta\\|_1&\\leq\n  \\|Y/2 -X\\hat\\beta_1/2 + Y/2 -X\\hat\\beta_2/2 \\|^2 + \\lambda/2 (\\|\\hat\\beta_1\\|_1 + \\|\\hat\\beta_2\\|_1 )\n\\end{align*}\n\\] Maintenant comme \\(Z\\in\\mathbb R^n, Z\\mapsto \\|Y-Z\\|^2\\) est une fonction strictement convexe on a l’inégalité stricte (si \\(X\\hat\\beta_1\\neq X\\hat\\beta_2\\)) \\[\n\\begin{align*}\n  \\|Y-X\\hat\\beta\\|^2+\\lambda \\|\\hat\\beta\\|_1&&lt;\n  \\|Y/2-X\\hat\\beta_1/2\\|^2 + \\|Y/2-X\\hat\\beta_2/2 \\|^2 + \\lambda/2 (\\|\\hat\\beta_1\\|_1 + \\|\\hat\\beta_2\\|_1 )\\\\\n  &=\\frac{1}{2}(\\|Y-X\\hat\\beta_1\\|^2  \\lambda\\|\\hat\\beta_1\\|_1) +\n  \\frac{1}{2}(\\|Y-X\\hat\\beta_2\\|^2  \\lambda\\|\\hat\\beta_2\\|_1) = \\mathrm{mini}\n\\end{align*}\n\\] ce qui contredit l’hypothèse du minimum atteint en \\(\\hat\\beta_1\\) ou \\(\\hat\\beta_2\\), donc \\(X\\hat\\beta_1= X\\hat\\beta_2\\)\nEn utilisant la question précédente on ne peut pas avoir deux minimiseurs globaux du lasso avec \\(X\\hat\\beta_1\\neq X\\hat\\beta_2\\) donc on note la valeur unique \\(X\\hat\\beta\\).\nSi on a 2 vecteurs \\(z_1\\) et \\(z_2\\) dans les sous-différentiels on a \\[\n\\begin{align}\n  -2X'(Y-\\mu \\mathbf{1}-X\\hat\\beta)+ z_1&=0\\\\\n  -2X'(Y-\\mu \\mathbf{1}-X\\hat\\beta)+ z_2&=0\n\\end{align}\n\\] Comme \\(X\\hat\\beta\\) est unique, en faisant la différence on obtient que \\(z_1-z_2=0\\), on a donc un vecteur \\(z=z_1=z_2\\) unique.\nPour une valeur quelconque réalisant le minimum du lasso \\(X\\hat\\beta\\) est unique on a le vecteur unique \\(z\\) qui s’écrit \\[%\n\\begin{align}\n  z&=2X'(Y-\\mu \\mathbf{1}-X\\hat\\beta)\n\\end{align}\n\\] Il en est de même pour l’ensemble des coordonnées de \\(z\\) valant \\(\\pm \\lambda\\).\nComme on a nécessairement que quand \\(\\hat\\beta_j\\neq 0\\) alors \\(z_j=\\pm\\lambda\\) on a donc que l’ensemble \\(\\xi\\) contient les coordonnées non nulles de \\(\\hat\\beta\\).\nLa condition nécessaire et suffisante est \\[%\n\\begin{align}\n  -2X'(Y-\\mu \\mathbf{1}-X\\hat\\beta)+ z&=0\n\\end{align}\n\\] Comme \\(\\xi\\) contient les coordonnées non nulles on se restreint à \\(\\xi\\) et on a \\[%\n\\begin{align}\n  -2X_\\xi'(Y-\\mu \\mathbf{1}-X_\\xi\\hat\\beta_\\xi)+ z&=0\\\\\n  2X_\\xi'X_\\xi\\hat\\beta_\\xi &= 2X_\\xi'(Y-\\mu \\mathbf{1}) - z_\\xi &= \\\\\n\\end{align}\n\\] et si \\({\\mathcal{H}}_1'\\) (\\(X_\\xi\\) est de \\[\\begin{align*}\n  \\|Y-X\\hat\\beta\\|^2+\\lambda \\|\\hat\\beta\\|_1&&lt;\n  \\|Y/2-X\\hat\\beta_1/2\\|^2 + \\|Y/2-X\\hat\\beta_2/2 \\|^2 + \\lambda/2 (\\|\\hat\\beta_1\\|_1 + \\|\\hat\\beta_2\\|_1 )\\\\\n  &=\\frac{1}{2}(\\|Y-X\\hat\\beta_1\\|^2  \\lambda\\|\\hat\\beta_1\\|_1) +\n  \\frac{1}{2}(\\|Y-X\\hat\\beta_2\\|^2  \\lambda\\|\\hat\\beta_2\\|_1) = \\mathrm{mini}\n\\end{align*}\\] plein rang) alors \\(X_\\xi'X_\\xi\\) inversible et donc \\[\n\\begin{align}\n  \\hat\\beta_\\xi&= (X_\\xi'X_\\xi)^{-1} (X_\\xi'(Y-\\mu \\mathbf{1}) - z_\\xi/2)\n\\end{align}\n\\] unique.\n\n\n\nExercice 8  \n\n\nlibrary(tidyverse)\nsignal &lt;- read_csv(\"../donnees/courbe_lasso.csv\")\ndonnees &lt;- read_csv(\"../donnees/echan_lasso.csv\")\nggplot(signal)+aes(x=x,y=y)+geom_line()+\n  geom_point(data=donnees,aes(x=X,y=Y))\n\n\n\n\n\n\n\n\nNous cherchons à reconstruire le signal à partir de l’échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme \\[\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n\\] n’est pas approprié. De nombreuses approches en traitement du signal proposent d’utiliser une base ou dictionnaire représentée par une collection de fonctions \\(\\{\\psi_j(x)\\}_{j=1,\\dots,K}\\) et de décomposer le signal dans cette base : \\[\nm(x)\\approx \\sum_{j=1}^K \\beta_j\\psi_j(x).\n\\] Pour un dictionnaire donné, on peut alors considérer un modèle linéaire \\[\n  y_i=\\sum_{j=1}^K \\beta_j\\psi_j(x_i)+\\varepsilon_i.\n\\tag{1}\\] Le problème est toujours d’estimer les paramètres \\(\\beta_j\\) mais les variables sont maintenant définies par les éléments du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par \\[\n\\psi_0(x)=1,\\quad \\psi_{2j-1}(x)=\\cos(2j\\pi x)\\quad\\text{et}\\quad \\psi_{2j}(x)=\\sin(2j\\pi x),\\quad j=1,\\dots,K.\n\\]\n\nmat.dict &lt;- function(K,x){\n    res &lt;- matrix(0,nrow=length(x),ncol=2*K) |&gt; as_tibble()\n    for (j in 1:K){\n      res[,2*j-1] &lt;- cos(2*j*pi*x)\n      res[,2*j] &lt;- sin(2*j*pi*x)\n    }\n    return(res)\n}\n\nIl suffit d’ajuster le modèle linéaire où les variables explicatives sont données par le dictionnaire :\n\nD25 &lt;- mat.dict(25,donnees$X) |&gt; mutate(Y=donnees$Y)\nmod.lin &lt;- lm(Y~.,data=D25)\nS25 &lt;- mat.dict(25,signal$x)\nprev.MCO &lt;- predict(mod.lin,newdata = S25)\nsignal1 &lt;- signal |&gt; mutate(MCO=prev.MCO) |&gt; rename(signal=y)\nsignal2 &lt;- signal1 |&gt; pivot_longer(-x,names_to=\"meth\",values_to=\"y\")\nggplot(signal2)+aes(x=x,y=y)+geom_line(aes(color=meth))+\n  scale_y_continuous(limits = c(-2,2))+geom_point(data=donnees,aes(x=X,y=Y))\n\n\n\n\n\n\n\n\nLe signal estimé a tendance à surajuster les données. Cela vient du fait qu’on estime 51 paramètres avec seulement 60 observations.\nOn regarde tout d’abord le chemin de régularisation des estimateurs lasso\n\nlibrary(glmnet)\nX.25 &lt;- model.matrix(Y~.,data=D25)[,-1]\nlasso1 &lt;- glmnet(X.25,D25$Y,alpha=1)\nplot(lasso1)\n\n\n\n\n\n\n\n\nIl semble que quelques coefficients quittent la valeur 0 bien avant les autres. On effectue maintenant la validation croisée pour sélectionner le paramètre \\(\\lambda\\).\n\nset.seed(1234)\nlasso.cv &lt;- cv.glmnet(X.25,D25$Y,alpha=1)\nplot(lasso.cv)\n\n\n\n\n\n\n\n\nOn calcule les prévisions et on trace le signal.\n\nprev.lasso &lt;- as.vector(predict(lasso.cv,newx=as.matrix(S25)))\nsignal1$lasso &lt;- prev.lasso\nsignal2 &lt;- signal1 |&gt; pivot_longer(-x,names_to=\"meth\",values_to=\"y\")\nggplot(signal2)+aes(x=x,y=y)+geom_line(aes(color=meth))+\n  scale_y_continuous(limits = c(-2,2))+geom_point(data=donnees,aes(x=X,y=Y))\n\n\n\n\n\n\n\n\nL’algorithme lasso a permis de corriger le problème de sur-apprentissage. Les coefficients lasso non nuls sont les suivants\n\nv.sel &lt;- which(coef(lasso.cv)!=0)\nv.sel\n\n [1]  1  2  4  5  6  8 21 28 30 38 40",
    "crumbs": [
      "Correction des exercices",
      "III Réduction de dimension",
      "8 Régularisation des moindre carrés : Ridge, Lasso et elastic net"
    ]
  },
  {
    "objectID": "correction/chap17.html",
    "href": "correction/chap17.html",
    "title": "17 Estimateurs à noyau et \\(k\\) plus proches voisins",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nA\nB\nA\n\n\n\nExercice 2 (Estimateur de Nadaraya-Watson) Il suffit de dériver la quantité à minimiser par rapport à \\(\\beta_1\\) et la valeur de \\(\\beta_1\\) qui annule cette dérivée : \\[-2\\sum_{i=1}^n(y_i-\\beta_1)p_i(x)=0\\quad\\Longleftrightarrow \\quad\\widehat\\beta_1(x)=\\frac{\\sum_{i=1}^ny_ip_i(x)}{\\sum_{i=1}^np_i(x)}.\\]\n\n\nExercice 3 (Polynômes locaux) Notons \\(P=\\text{diag}(p_1(x), \\dots, p_n(x))\\) et nous avons donc à minimiser la fonction \\[\n\\begin{eqnarray*}\n\\min_{\\beta} \\sum_{i=1}^n (y_i-\\beta_1-\\beta_2(x_i-x))^2 p_i(x) = (Y - X\\beta)'P(Y - X\\beta)\n\\end{eqnarray*}\n\\] avec \\(\\beta=(\\beta_1,\\beta_2)'\\) et \\[\n\\begin{align*}\n  X= \\begin{pmatrix}\n    1&x_1\\\\\n    1&x_2\\\\\n    \\vdots&\\vdots\\\\\n    1&x_n\\\\\n  \\end{pmatrix}.\n\\end{align*}\n\\] Le minimum (voir régression pondérée au chapitre 4 (p. 81) avec ici \\(\\omega_i^2=p_i(x)\\)) est atteint pour \\[\n\\begin{eqnarray}\n\\hat \\beta=(X'PX)^{-1}X'PY\n\\end{eqnarray}\n\\tag{1}\\] Calculons \\((X'PX)\\): \\[\n\\begin{align*}\n  X'PX=\\begin{pmatrix}\n  1&1&\\cdots &1\\\\\n  x_1&x_2&\\cdots&x_n\\\\\n  \\end{pmatrix}\n\\begin{pmatrix}\n  p_1(x)&0&\\cdots &0\\\\\n  0&p_2(x)&&0 \\\\\n  \\vdots&&\\ddots&\\vdots \\\\\n  0&\\cdots&0&p_n(x) \\\\\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    1&x_1\\\\\n    1&x_2\\\\\n    \\vdots&\\vdots\\\\\n    1&x_n\\\\\n  \\end{pmatrix}=\n  \\begin{pmatrix}\n    S_0&S_1\\\\\n    S_1&S_2\n  \\end{pmatrix}\n\\end{align*}\n\\] avec \\[\n\\begin{eqnarray*}\nS_{0} &=& \\sum_{i=1}^n p_i(x)\\\\\nS_{1} &=& \\sum_{i=1}^n p_i(x)(x_i-x)\\\\\nS_{2} &=& \\sum_{i=1}^n p_i(x)(x_i-x)^2.\n\\end{eqnarray*}\n\\] Inversons cette matrice \\[\n\\begin{align*}\n  (X'PX)^{-1}=\\frac{1}{S_2S_0 - S_1^2}\\begin{pmatrix}\n    S_2&-S_1\\\\\n    -S_1&S_0\n  \\end{pmatrix}\n\\end{align*}\n\\] Remarquons que \\[\n\\begin{align*}\n\\sum_{i=1}^n q_i(x) =\\sum_{i=1}^n p_i(x)(S_{2}-(x_i-x)S_{1}) = S_2 \\sum_{i=1}^n p_i(x) - S_{1}\\sum_{i=1}^n p_i(x)(x_i-x)= S_2S_0 - S_1^2\n\\end{align*}\n\\] Ensuite calculons \\[\n\\begin{align*}\nX'PY=\\begin{pmatrix}\n  1&1&\\cdots &1\\\\\n  x_1&x_2&\\cdots&x_n\\\\\n  \\end{pmatrix}\n\\begin{pmatrix}\n  p_1(x)&0&\\cdots &0\\\\\n  0&p_2(x)&&0 \\\\\n  \\vdots&&\\ddots&\\vdots \\\\\n  0&\\cdots&0&p_n(x) \\\\\n  \\end{pmatrix}\n  \\begin{pmatrix}\n    y_1\\\\\n    y_2\\\\\n    \\vdots\\\\\n    y_n\\\\\n  \\end{pmatrix}\n  =\n   \\begin{pmatrix}\n    \\sum_{i=1}^n p_i(x)y_i\\\\\n    \\sum_{i=1}^n(x_i-x)p_i(x)y_i\n  \\end{pmatrix}\n\\end{align*}\n\\] En utilisant dans (équation 1) les différentes parties calculées ci-dessus nous obtenons enfin \\[\n\\begin{align*}\n\\hat\\beta=\\frac{1}{\\sum_{i=1}^n q_i(x)}\\begin{pmatrix}\n    S_2&-S_1\\\\\n    -S_1&S_0\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\sum_{i=1}^n p_i(x)y_i\\\\\n    \\sum_{i=1}^n(x_i-x)p_i(x)y_i\n\\end{pmatrix}=\n=\\frac{1}{\\sum_{i=1}^n q_i(x)}\\begin{pmatrix}\n  S_2\\sum_{i=1}^n p_i(x)y_i - S_1\\sum_{i=1}^n(x_i-x)p_i(x)y_i\\\\\n  -S_1\\sum_{i=1}^n p_i(x)y_i + S_0\\sum_{i=1}^n(x_i-x)p_i(x)y_i\n\\end{pmatrix}\n\\end{align*}\n\\] Comme nous cherchons la première coordonnée cela donne \\[\n\\begin{align*}\n\\hat\\beta_1=\\frac{\\sum_{i=1}^n y_i p_i(x)(S_2 - (x_i-x)S_1)}{\\sum_{i=1}^n q_i(x)} = \\frac{\\sum_{i=1}^n y_i q_i(x)}{\\sum_{i=1}^n q_i(x)}.\n\\end{align*}\n\\]\n\n\nExercice 4 (Estimateur à noyau uniforme dans \\(\\mathbb R^p\\))  \n\nEn annulant la dérivée par rapport à \\(a\\), on obtient \\[\\widehat m(x)=\\frac{\\sum_{i=1}^ny_iK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\nOn a \\[\\mathop{\\mathrm{V}}[\\widehat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}\\] et \\[\\mathbf E[\\widehat m(x)]-m(x)=\\frac{\\sum_{i=1}^n(m(x_i)-m(x))K\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\nOn a maintenant \\(|m(x_i)-m(x)|\\leq L\\|x_i-x\\|\\). Or \\[K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] est non nul si et seulement si \\(\\|x_i-x\\|\\leq h\\). Donc pour tout \\(i=1,\\dots,n\\) \\[L\\|x_i-x\\|K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\leq Lh K\\left(\\frac{\\|x_i-x\\|}{h}\\right).\\] D’où le résultat.\nOn a \\[\\mathop{\\mathrm{V}}[\\widehat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}=\\frac{\\sigma^2}{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}.\\] Or \\[\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)\\geq C_1n\\textrm{Vol}(B_h)\\geq C_1\\gamma_dnh^d\\] où \\(\\gamma_d=\\pi^{d/2}/\\Gamma(d/2+1)\\). On a donc \\[\\mathop{\\mathrm{V}}[\\widehat m(x)]\\leq \\frac{\\sigma^2}{C_1\\gamma_dnh^d}=\\frac{C_2\\sigma^2}{nh^d}\\] avec \\(C_2=1/(C_1\\gamma_d)\\).\nOn déduit \\[\\mathbf E[(\\widehat m(x)-m(x))^2]\\leq L^2h^2+\\frac{C_2\\sigma^2}{nh^d}.\\]\nSoit \\(M(h)\\) le majorant ci-dessus. On a \\[M(h)'=2hL^2-\\frac{C_2\\sigma^2d}{n}h^{-d-1}.\\] La dérivée s’annule pour \\[h_{opt}=\\frac{2L^2}{C_2\\sigma^2d}n^{-\\frac{1}{d+2}}.\\] Lorsque \\(h=h_{opt}\\) l’erreur quadratique vérifie \\[\\mathbf E[(\\hat m(x)-m(x))^2]=\\mathrm{O}\\left(n^{-\\frac{2}{d+2}}\\right).\\] La vitesse diminue lorsque la dimension \\(d\\) augmente, c’est le fléau de la dimension.\n\n\n\nExercice 5 (Vitesse de la régression univariée en design equi-espacé)  \n\n\\(\\widehat\\beta\\) minimise \\(\\sum_{i=1}^n(Y_i-\\beta x_i)^2\\). On a donc \\[\\widehat\\beta=\\frac{\\sum_{i=1}^nx_iY_i}{\\sum_{i=1}^nx_i^2}.\\]\nOn déduit \\[\\mathbf E[\\widehat\\beta]=\\beta\\quad\\textrm{et}\\quad\\mathop{\\mathrm{V}}(\\widehat\\beta)=\\frac{\\sigma^2}{\\sum_{i=1}^nx_i^2}.\\]\nComme \\[\\sum_{i=1}^nx_i^2=\\frac{(n+1)(2n+1)}{6n},\\] on obtient le résultat demandé.\n\n\n\nExercice 6 (Critère LOO)  \n\nOn désigne par \\(\\widehat F_h\\) le vecteur \\((\\widehat f_h(x_i),i=1,\\dots,n)\\), \\(\\widehat F_k\\) le vecteur \\((\\widehat f_k(x_i),i=1,\\dots,n)\\) et \\(\\mathbb Y=(y_1,\\dots,y_n)\\). On voit facilement que \\[\\widehat F_h=S_h\\mathbb Y\\quad\\text{et}\\quad \\widehat F_k=S_k\\mathbb Y\\] où \\(S_h\\) et \\(S_k\\) sont des matrices \\(n\\times n\\) dont le terme général est défini par \\[S_{ij,h}=\\frac{K((x_i-x_j)/h)}{\\sum_l K((x_i-x_l)/h)}\n\\quad\\text{et}\\quad\nS_{ij,k}=\n\\left\\{\n\\begin{array}{ll}\n  1/k&\\text{ si $x_j$ est parmi les $k$-ppv de $x_i$} \\\\\n  0 & \\text{ sinon}.\n  \\end{array}\n\\right.\\]\nPour simplifier on note \\(K_{ij}=K((x_i-x_j)/h)\\) On a \\[\\widehat f_h^i(x_i)=\\frac{\\sum_{j\\neq i}K_{ij}y_j}{\\sum_{j\\neq i}K_{ij}}.\\] Par conséquent \\[\\widehat f_h^i(x_i)\\left[\\sum_{j=1}^nK_{ij}-K_{ii}\\right]=\\sum_{j\\neq i}K_{ij}y_j.\\] On obtient le résultat demandé en divisant tout \\(\\sum_{j=1}^nK_{ij}\\). Pour l’estimateur de plus proches voisins, on remarque que, si on enlève la \\(i\\)ème observation alors l’estimateur des \\(k\\) plus proches voisins de \\(x_i\\) s’obtient à partir ce celui des \\(k+1\\) plus proches voisins avec la \\(i\\)ème observation de la façon suivante : \\[\\widehat f_k^i(x_i)=\\frac{k+1}{k}\\sum_{j\\neq i}S_{ij,k+1}y_j.\\] On obtient le résultat demandé on observant que \\(S_{ii,k+1}=1/(k+1)\\) et donc \\[\\frac{1}{1-S_{ii,k+1}}=\\frac{k+1}{k}.\\]\nOn obtient pour l’estimateur à noyau \\[\\begin{align*}\nLOO(\\widehat f_h)= & \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\widehat f_h^i(x_i)\\right)^2 \\\\\n= & \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-S_{ii,h}y_i-\\sum_{j\\neq i}S_{ij,h}y_{j}}{1-S_{ii,h}}\\right)^2 \\\\\n= & \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\widehat f_h(x_i)}{1-S_{ii,h}}\\right)^2.\n\\end{align*}\\] Le calcul est similaire pour l’estimateur des plus proches voisins.\n\n\n\nExercice 7 (Caret et kppv) On importe les données et on ne garde que les deux variables demandées :\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\")\ndf &lt;- ozone[,c(\"O3\",\"T12\")]\n\nOn construit la grille de plus proches voisins candidats :\n\ngrille &lt;- data.frame(k=1:40)\n\nOn indique à caret qu’on veut faire de la validation croisée 10 blocs :\n\nlibrary(caret)\nctrl &lt;- trainControl(method=\"cv\")\n\nOn lance la validation croisée avec la fonction train :\n\nset.seed(1234)\nsel.k &lt;- train(O3~.,data=df,method=\"knn\",trControl=ctrl,tuneGrid=grille)\nsel.k\n\nk-Nearest Neighbors \n\n50 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 44, 46, 45, 44, 45, 45, ... \nResampling results across tuning parameters:\n\n  k   RMSE      Rsquared   MAE     \n   1  25.39941  0.2777479  21.83800\n   2  20.89184  0.3784027  17.05972\n   3  17.81455  0.5039548  14.88541\n   4  16.49216  0.5636676  14.01838\n   5  16.75867  0.5579746  14.33716\n   6  16.76845  0.5384320  13.92737\n   7  16.11270  0.5792998  13.77474\n   8  15.96855  0.5992164  13.64931\n   9  16.48290  0.5679445  14.13226\n  10  16.89146  0.5572318  14.62217\n  11  17.14669  0.5541549  14.68934\n  12  17.69184  0.5469553  15.27732\n  13  17.41791  0.5668045  15.10278\n  14  17.53775  0.5484378  15.17445\n  15  17.61846  0.5669393  15.35179\n  16  17.89693  0.5550383  15.60799\n  17  18.08559  0.5579067  15.72184\n  18  18.25932  0.5655324  15.89826\n  19  18.84244  0.5402343  16.27675\n  20  19.08747  0.5429639  16.30312\n  21  19.44070  0.5266746  16.57676\n  22  19.66410  0.5196443  16.65315\n  23  19.69539  0.5339187  16.60436\n  24  19.94851  0.5245251  16.73931\n  25  20.04650  0.5072557  16.78093\n  26  20.14871  0.5013264  16.78305\n  27  20.28257  0.5036138  16.89089\n  28  20.45060  0.4988594  16.89900\n  29  20.54389  0.5105147  16.92002\n  30  20.79567  0.5080200  17.12472\n  31  20.96099  0.5014461  17.25705\n  32  21.14524  0.4891512  17.23304\n  33  21.38173  0.4875148  17.38980\n  34  21.59260  0.4810615  17.43883\n  35  21.83266  0.4707654  17.67392\n  36  22.06004  0.4448406  17.74640\n  37  22.34462  0.4236828  17.95419\n  38  22.54304  0.3999950  18.11507\n  39  22.61034  0.4147744  18.15091\n  40  22.68049  0.4296545  18.23871\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 8.\n\n\nOn sélectionnera\n\nsel.k$bestTune\n\n  k\n8 8\n\n\nplus proches voisins.",
    "crumbs": [
      "Correction des exercices",
      "V Introduction à la régression non paramétrique",
      "17 Estimateurs à noyau et $k$ plus proches voisins"
    ]
  },
  {
    "objectID": "code/chap2.html",
    "href": "code/chap2.html",
    "title": "2 La régression linéaire multiple",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nlibrary(\"scatterplot3d\")\nscatterplot3d(ozone[,\"T12\"],ozone[,\"Vx\"],ozone[,\"O3\"],\n              type=\"h\",pch=16, box=FALSE, xlab=\"T12\", ylab=\"Vx\", zlab=\"O3\")\n\n\n\n\n\n\n\n\n\nregmulti &lt;- lm(O3~T12+Vx, data = ozone)\nsummary(regmulti)\n\n\nCall:\nlm(formula = O3 ~ T12 + Vx, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.984 -10.152  -2.407  11.710  34.494 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  35.4530    10.7446   3.300  0.00185 ** \nT12           2.5380     0.5151   4.927 1.08e-05 ***\nVx            0.8736     0.1772   4.931 1.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.82 on 47 degrees of freedom\nMultiple R-squared:  0.5249,    Adjusted R-squared:  0.5047 \nF-statistic: 25.96 on 2 and 47 DF,  p-value: 2.541e-08\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nplot(ht~circ, data = eucalypt, xlab = \"circ\", ylab = \"ht\")\n\n\n\n\n\n\n\n\n\nregmult &lt;- lm(ht ~ circ + I(sqrt(circ)), data = eucalypt)\nresume.mult &lt;- summary(regmult)\nresume.mult\n\n\nCall:\nlm(formula = ht ~ circ + I(sqrt(circ)), data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1881 -0.6881  0.0427  0.7927  3.7481 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -24.35200    2.61444  -9.314   &lt;2e-16 ***\ncirc           -0.48295    0.05793  -8.336   &lt;2e-16 ***\nI(sqrt(circ))   9.98689    0.78033  12.798   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.136 on 1426 degrees of freedom\nMultiple R-squared:  0.7922,    Adjusted R-squared:  0.7919 \nF-statistic:  2718 on 2 and 1426 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(ht ~ circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),max(eucalypt[,\"circ\"]), length = 100))\nlines(grille[,\"circ\"], predict(regmult, grille))",
    "crumbs": [
      "Codes R",
      "I Introduction au modèle linéaire",
      "2 La régression linéaire multiple"
    ]
  },
  {
    "objectID": "code/chap7.html",
    "href": "code/chap7.html",
    "title": "7 Choix de variables",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nlibrary(leaps)\nrecherche &lt;- regsubsets(O3 ~ T12 + T15 + Ne12 + N12 + S12 + E12 + W12 + Vx + O3v, int = T,nbest = 1, nvmax = 10, method = \"exhaustive\", data = ozone)\n\n\nplot(recherche, scale = \"bic\")\n\n\n\n\n\n\n\nplot(recherche, scale = \"Cp\")\n\n\n\n\n\n\n\nplot(recherche, scale = \"adjr2\")\n\n\n\n\n\n\n\nplot(recherche, scale = \"r2\")\n\n\n\n\n\n\n\n\n\nresume &lt;- summary(recherche)\nnomselec &lt;- colnames(resume$which)[resume$which[which.min(resume$bic),]][-1]\nformule &lt;- formula(paste(\"O3~\",paste(nomselec,collapse=\"+\")))\nmodeleslectionne &lt;- lm(formule, data = ozone)\nmodeleslectionne\n\n\nCall:\nlm(formula = formule, data = ozone)\n\nCoefficients:\n(Intercept)          T15         Ne12           Vx          O3v  \n    61.8252       1.0577      -3.9935       0.3146       0.2629",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "7 Choix de variables"
    ]
  },
  {
    "objectID": "correction/chap6.html",
    "href": "correction/chap6.html",
    "title": "6 Variables qualitatives : ANCOVA et ANOVA",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, A, C, B.\n\n\nExercice 2 (Analyse de la covariance)  \n\nNous avons pour le modèle complet la matrice suivante : \\[\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}&\\cdots&0\\\\\n\\vdots&\\cdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n\\] et pour les deux sous-modèles, nous avons les matrices suivantes : \\[\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}\\\\\n\\vdots &\\cdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&x_{In_I}\n\\end{bmatrix}\n\\quad\nX=\\begin{bmatrix}\n1&x_{11}&\\cdots&0\\\\\n\\vdots &\\vdots&\\vdots&\\vdots\\\\\n1&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n\\]\nDans le modèle complet, nous obtenons par le calcul \\[\nX'X = \\begin{bmatrix}\nn_1&0&\\cdots&\\sum x_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&n_I&0&\\cdots&\\sum x_{iI}\\\\\n\\sum x_{i1}&0&\\cdots&\\sum x^2_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&\\sum x_{iI}&0&\\cdots&\\sum x^2_{iI}\\\\\n\\end{bmatrix}\n  \\quad\nX'Y = \\begin{bmatrix}\n\\sum y_{i1}\\\\\n\\vdots\\\\\n\\sum y_{iI}\\\\\n\\sum x_{i1}y_{i1}\\\\\n\\vdots\\\\\n\\sum x_{iI}y_{iI}\\\\\n\\end{bmatrix}\n\\] Une inversion par bloc de \\(X'X\\) et un calcul matriciel donnent le résultat indiqué.\nUne autre façon de voir le problème est de partir du problème de minimisation \\[\\begin{eqnarray*}\n&&\\min \\sum_{i=1}^I\\sum_{j=1}^{n_i}\\left(y_{ij}-\\alpha_{j}-\\beta_{j}x_{ij}\\right)^2\\\\\n&=& \\min \\sum_{j=1}^{n_i}\\left(y_{j1}-\\alpha_1-\\beta_{1}x_{j1}\\right)^2+\\cdots\n+\\sum_{j=1}^{n_I}\\left(y_{jI}-\\alpha_I-\\beta_{I}x_{JI}\\right)^2.\n\\end{eqnarray*}\\] Cela revient donc à calculer les estimateurs des MC pour chaque modalité de la variable qualitative. Attention tout de même, des régressions pour chaque modalité donnent bien les mêmes coefficients \\(\\alpha_{i}, \\beta_{i}\\) mais les écarts-types estimés seront différents: un par modalité dans le cas des régressions pour chaque modalité, un seul écart-type estimé dans le cas de l’ANCOVA.\n\n\n\nExercice 3 (Estimateurs des MC et ANOVA à 1 facteur) La preuve de cette proposition est relativement longue et peu difficile. Nous avons toujours \\(Y\\) un vecteur de \\(\\mathbb R^n\\) à expliquer. Nous projetons \\(Y\\) sur le sous-espace engendré par les colonnes de \\(A_c\\), noté \\(\\mathcal M_{A_c}\\), de dimension I, et obtenons un unique \\(\\hat Y\\). Cependant, en fonction des contraintes utilisées, le repère de \\(\\mathcal M_{A_c}\\) va changer.\nLe cas le plus facile se retrouve lorsque \\(\\mu=0\\). Nous avons alors \\[\\begin{eqnarray*}\n(A_c'A_c) =\n\\begin{bmatrix}\nn_1&0&\\cdots&0 \\\\\n0&n_2&0&\\cdots \\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0& \\cdots & 0 & n_I\n\\end{bmatrix}\n\\quad\n(A_c'Y)=\\begin{bmatrix}\n\\sum_{j=1}^{n_1} y_{1j}\\\\\n\\sum_{j=1}^{n_2} y_{2j}\\\\\n\\vdots\\\\\n\\sum_{j=1}^{n_I} y_{Ij}\n\\end{bmatrix}\n\\end{eqnarray*}\\] d’où le résultat. La variance de \\(\\hat \\alpha\\) vaut \\(\\sigma^2 (A_c'A_c)^{-1}\\) et cette matrice est bien diagonale.\nPour les autres contraintes, nous utilisons le vecteur \\(\\vec{e}_{ij}\\) de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle repérée par le couple \\((i,j)\\) qui vaut 1 pour repérer un individu. Nous notons \\(\\vec{e}_{i}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celles repérées par les indices \\(i,j\\) pour \\(j=1,\\cdots,n_{i}\\) qui valent 1. En fait, ce vecteur repère donc les individus qui admettent la modalité \\(i\\). La somme des \\(\\vec{e}_{i}\\) vaut le vecteur \\(\\mathbf{1}\\). Les vecteurs colonnes de la matrice \\(A_c\\) valent donc \\(\\vec{e}_{1},\\cdots,\\vec{e}_{I}\\).\nConsidérons le modèle \\[\\begin{eqnarray*}\nY=\\mu \\mathbf{1} + \\alpha_1 \\vec{e_1}+ \\alpha_2 \\vec{e_2} +\n\\cdots + \\alpha_I\\vec{e_I} + \\varepsilon.\n\\end{eqnarray*}\\] Voyons comment nous pouvons récrire ce modèle lorsque les contraintes sont satisfaites.\n\n\\(\\alpha_1=0\\), le modèle devient alors \\[\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1} + 0 \\vec{e_1} + \\alpha_2 \\vec{e_2} + \\cdots +\n\\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_2 \\vec{e_2} + \\cdots + \\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=& [\\mathbf{1}, \\vec{e_2}, \\cdots, \\vec{e_I}] \\beta + \\varepsilon\\\\\n&=& X_{[\\alpha_1=0]} \\beta_{[\\alpha_1=0]} + \\varepsilon.\n\\end{eqnarray*}\\]\n\\(\\sum n_i \\alpha_i = 0\\) cela veut dire que \\(\\alpha_I= - \\sum_{j=1}^{I-1} n_j\\alpha_j/n_I\\), le modèle devient \\[\n\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1}+ \\alpha_1 \\vec{e_1} + \\cdots +\\alpha_{I-1} \\vec{e_{I-1}}   \n- \\sum_{j=1}^{I-1}  \\frac{n_j\\alpha_j}{n_I}\\vec{e_I} + \\varepsilon\\\\\n&=& \\mu \\mathbf{1} + \\alpha_1(\\vec{e}_{1}-\\frac{n_1}{n_I}\\vec{e}_{I}) + \\cdots\n+ \\alpha_{I-1} (\\vec{e}_{I-1}-\\frac{n_{I-1}}{n_I}\\vec{e}_{I})+\\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_1\\vec{v}_{1}+ \\cdots + \\alpha_{I-1} \\vec{v}_{I-1}\n+ \\varepsilon \\quad \\hbox{où} \\quad \\vec{v}_{i}= (\\vec{e}_{i}-\\frac{n_i}{n_I}\\vec{e}_{I})\\\\\n&=& X_{[\\sum  n_i \\alpha_i=0]} \\beta_{[\\sum  n_i\\alpha_i=0]} + \\varepsilon.\n\\end{eqnarray*}\n\\]\n\\(\\sum  \\alpha_i = 0\\) cela veut dire que \\(\\alpha_I= - \\sum_{j=1}^{I-1} \\alpha_j\\), le modèle devient \\[\n\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1} + \\alpha_1 \\vec{e_1} + \\cdots + \\alpha_{I-1} \\vec{e_{I-1}} -\n\\sum_{j=1}^{I-1}  \\alpha_j \\vec{e_I} + \\varepsilon\\\\\n&=& \\mu \\mathbf{1} + \\alpha_1(\\vec{e}_{1}-\\vec{e}_{I}) + \\cdots\n+ \\alpha_{I-1} (\\vec{e}_{I-1}-\\vec{e}_{I})+\\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_1\\vec{u}_{1}+ \\cdots + \\alpha_{I-1} \\vec{u}_{I-1}\n+ \\varepsilon \\quad \\hbox{où} \\quad \\vec{u}_{i}= (\\vec{e}_{i}-\\vec{e}_{I})\\\\\n&=& X_{[\\sum  \\alpha_i=0]} \\beta_{[\\sum  \\alpha_i=0]} + \\varepsilon.\n\\end{eqnarray*}\n\\]\n\nDans tous les cas, la matrice \\(X\\) est de taille \\(n \\times I\\), et de rang \\(I\\). La matrice \\(X'X\\) est donc inversible. Nous pouvons calculer l’estimateur \\(\\hat \\beta\\) des MC de \\(\\beta\\) par la formule \\(\\hat \\beta = (X'X)^{-1}X'Y\\) et obtenir les valeurs des estimateurs. Cependant ce calcul n’est pas toujours simple et il est plus facile de démontrer les résultats via les projections.\nLes différentes matrices \\(X\\) et la matrice \\(A\\) engendrent le même sous-espace, donc la projection de \\(Y\\), notée \\(\\hat Y\\) dans ce sous-espace, est toujours la même. La proposition 6.2 indique que \\[\\begin{eqnarray*}\n\\hat Y = \\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I}.\n\\end{eqnarray*}\\] Avec les différentes contraintes, nous avons les 3 cas suivants :\n\n\\(\\alpha_1=0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_2 \\vec{e_2} + \\cdots +\n\\hat \\alpha_I\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum n_i \\alpha_i = 0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1}\n\\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum  \\alpha_i = 0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=& \\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots +\n\\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\nIl suffit maintenant d’écrire que la projection est identique dans chaque cas et de remarquer que le vecteur \\(\\mathbf{1}\\) est la somme des vecteurs \\(\\vec{e_i}\\) pour \\(i\\) variant de 1à\\(I\\). Cela donne\n\n\\(\\alpha_1=0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=&\\hat \\mu\\mathbf{1}+\\hat\\alpha_2\\vec{e_2}+\\cdots+\\hat \\alpha_I\\vec{e_I}\\\\\n&=& \\hat \\mu\\vec{e_1}+(\\hat \\mu+\\hat \\alpha_2)\\vec{e_2}\n\\cdots (\\hat \\mu+\\hat \\alpha_I)\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum n_i \\alpha_i = 0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=& \\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1}\n\\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}\\\\\n&=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n(\\hat \\mu  + \\hat \\alpha_{I-1}) \\vec{e_{I-1}} +\n(\\hat \\mu  - \\sum_{i=1}^{I-1} \\frac{n_i}{n_I}\\hat \\alpha_i) \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum \\alpha_i = 0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots +\n\\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}\\\\\n&=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n(\\hat \\mu  + \\hat \\alpha_{I-1})\\vec{e_{I-1}} +\n(\\hat \\mu - \\sum_{i=1}^{I-1} \\hat \\alpha_i) \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\nEn identifiant les différents termes, nous obtenons le résultat annoncé.\n\n\nExercice 4 (Estimateurs des MC et ANOVA à deux facteurs) Nous notons \\(\\vec{e}_{ijk}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle indicée par \\(ijk\\) qui vaut 1. Sous les contraintes de type analyse par cellule, le modèle devient \\[\\begin{eqnarray*}\ny_{ijk} &=& \\gamma_{ij} + \\varepsilon_{ijk},\n\\end{eqnarray*}\\] et donc matriciellement \\[\\begin{eqnarray*}\nY= X \\beta +\\varepsilon \\quad \\quad X=(\\vec{e_{11}},\\vec{e_{12}},\\ldots,\\vec{e_{IJ}}),\n\\end{eqnarray*}\\] où le vecteur \\(\\vec{e}_{ij}= \\sum_{k} \\vec{e}_{ijk}\\). Les vecteurs colonnes de la matrice \\(X\\) sont orthogonaux entre eux. Le calcul matriciel \\((X'X)^{-1}X'Y\\) donne alors le résultat annoncé.\n\n\nExercice 5 (Estimateurs des MC et ANOVA à deux facteurs, suite) Nous notons \\(\\vec{e}_{ijk}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle indicée par \\(ijk\\) qui vaut 1. Nous définissons ensuite les vecteurs suivants~: \\[\\begin{eqnarray*}\n\\vec{e}_{ij} = \\sum_{k} \\vec{e}_{ijk} \\quad\n\\vec{e}_{i.} = \\sum_{j} \\vec{e}_{ij}  \\quad\n\\vec{e}_{.j} = \\sum_{i} \\vec{e}_{ij}  \\quad\n\\vec{e} = \\sum_{i,j,k} \\vec{e}_{ijk}.\n\\end{eqnarray*}\\] Afin d’effectuer cet exercice, nous définissons les sous-espaces suivants~: \\[\\begin{eqnarray*}\nE_1&\\!\\!:=\\!\\!&\\{m \\vec{e},\\ m \\hbox{ quelconque} \\}\\\\\nE_2&\\!\\!:=\\!\\!&\\{\\sum_i a_i \\vec{e}_{i.},\\ \\sum_i a_i=0\\}\\\\\nE_3&\\!\\!:=\\!\\!&\\{\\sum_j b_j \\vec{e}_{.j},\\ \\sum_j b_j=0\\}\\\\\nE_4&\\!\\!:=\\!\\!&\\{\\sum_{ij} c_{ij} \\vec{e}_{ij},\n\\ \\forall j \\sum_{i} c_{ij}=0 \\hbox{ et } \\forall i \\sum_{j} c_{ij}=0\\}.\n\\end{eqnarray*}\\] Ces espaces \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\) sont de dimension respective 1, \\(I-1\\), \\(J-1\\) et \\((I-1)(J-1)\\). Lorsque le plan est équilibré, tous ces sous-espaces sont orthogonaux. Nous avons la décomposition suivante~: \\[\\begin{eqnarray*}\nE = E_1 \\stackrel{\\perp}{\\oplus} E_2 \\stackrel{\\perp}{\\oplus} E_3\n\\stackrel{\\perp}{\\oplus} E_4.\n\\end{eqnarray*}\\]\nLa projection sur \\(E\\) peut se décomposer en une partie sur \\(E_1,\\cdots,E_4\\) et l’estimateur des MC est obtenu par projection de \\(Y\\) sur \\(E\\). Notons \\(P_{E^\\perp}\\), \\(P_{E},\\) \\(P_{E_1},\\) \\(P_{E_2},\\) \\(P_{E_3}\\) et \\(P_{E_4}\\) les projections orthogonales sur les sous-espaces \\(E^\\perp\\), \\(E\\), \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\), nous avons alors \\[\\begin{eqnarray*}\nP_{E_1} Y &=& \\bar{y} \\mathbf{1} ,\n\\end{eqnarray*}\\] puis, en remarquant que projeter sur le sous-espace engendré par les colonnes de \\(A=[\\vec{e}_{1.},\\cdots,\\vec{e}_{I.}]\\) est identique à la projection sur \\(E_1 \\stackrel{\\perp}{\\oplus} E_2\\), nous avons alors avec \\(\\mathbf{1} = \\sum_i \\vec{e}_{i.}\\), \\[\\begin{eqnarray*}\nP_{A} Y = \\sum_i \\bar{y}_{i.} \\vec{e}_{i.} \\quad \\hbox{donc}\n\\quad P_{E_2} Y =\\sum_i (\\bar{y}_{i.} - \\bar{y})\\ \\vec{e}_{i.}.\\\\\n\\end{eqnarray*}\\] De la même façon, nous obtenons \\[\\begin{eqnarray*}\nP_{E_3}(Y)&=&\\sum_j (\\bar{y}_{.j} - \\bar{y})\\ \\vec{e}_{.j},\\\\\nP_{E_4}(Y)&=&\\sum_{ij} (\\bar{y}_{ij}-\\bar{y}_{i.}-\\bar{y}_{.j}+\\bar{y})\\  \\vec{e}_{i.},\\\\\nP_{E^\\perp}(Y)  &=&\\sum_{ijk} (y_{ijk}-\\bar{y}_{ij})\\ \\vec{e}_{ijk},\n\\end{eqnarray*}\\] où \\(\\vec{e}_{ijk}\\) est le vecteur dont toutes les coordonnées sont nulles sauf celle indicée par \\({ijk}\\) qui vaut 1. En identifiant terme à terme, nous retrouvons le résultat énoncé.\n\n\nExercice 6 (Tableau d’ANOVA à 2 facteurs équilibrés) Lorsque le plan est équilibré, nous avons démontré, que les sous-espaces \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\) sont orthogonaux (cf. exercice précédent) deux à deux. Nous avons alors \\[\\begin{eqnarray*}\nY &=& P_{E_1}(Y) + P_{E_2}(Y) + P_{E_3}(Y) + P_{E_4}(Y) + P_{E^\\perp}(Y).\n\\end{eqnarray*}\\] Nous obtenons ensuite par le théorème de Pythagore \\[\\begin{eqnarray*}\n\\begin{array}{ccccccccccc}\n\\|Y - \\bar Y \\|^2 &=&  \\| P_{E_2}(Y)\\|^2 &+&\n\\|P_{E_3}(Y)\\|^2 &+& \\|P_{E_4}(Y)\\|^2 &+& \\|P_{E^\\perp}(Y)\\|^2\\\\\n\\mathop{\\mathrm{SCT}}&=& \\mathop{\\mathrm{SC}}_A &+& \\mathop{\\mathrm{SC}}_B &+& \\mathop{\\mathrm{SC}}_{AB} &+& \\mathop{\\mathrm{SCR}},\n\\end{array}\n\\end{eqnarray*}\\] où \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{SCT}}&=& \\sum_i \\sum_j \\sum k (y_{ijk} - \\bar y)^2\\\\\n\\mathop{\\mathrm{SC}}_A &=& Jr \\sum_i (y_{i..}-\\bar y)^2\\\\\n\\mathop{\\mathrm{SC}}_B &=& Ir \\sum_j (y_{.j.} - \\bar y)^2\\\\\n\\mathop{\\mathrm{SC}}_{AB} &=& r \\sum_i \\sum_j (y_{ij.} - y_{i..} - y_{.j.} +\\bar y)^2\\\\\n\\mathop{\\mathrm{SCR}}&=& \\sum_i \\sum_j \\sum_k (y_{ijk}- \\bar{y_{ij}})^2.\n\\end{eqnarray*}\\]\nAfin de bien visualiser les vecteurs voici un exemple avec \\(I=2\\), \\(J=3\\) et \\(r=2\\) en remplaçant les \\(0\\) par \\(.\\) :\n\\[\n\\begin{array}{*{12}c}\n      \\vec{e}&\\vec{e}_{1.}&\\vec{e}_{2.}&\n      \\vec{e}_{.1}&\\vec{e}_{.2}&\\vec{e}_{.3}&\n      \\vec{e_{11}}&\\vec{e}_{12}&\\vec{e}_{13}&\n      \\vec{e}_{21}&\\vec{e}_{22}&\\vec{e}_{23}&\\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n%%\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n\\end{array}\n\\]\n\nEn écrivant les vecteurs dans le cadre général et en faisant la somme ci-dessous \\[\n\\vec{Y}=\\mu \\vec{e}+\\sum_{i} \\alpha_i \\vec{e}_{i.}+\\sum_{j} \\beta_j \\vec{e}_{.j}\n+\\sum_{ij} (\\alpha\\beta)_{ij} \\vec{e}_{ij} + \\vec{\\varepsilon},\n\\tag{1}\\] on a bien que la ligne \\(ijk\\) vaut \\[\ny_{ijk} = \\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+ \\varepsilon_{ijk}.\n\\]\nMontrons que \\(E_1 \\perp E_2\\). Pour cela prenons deux vecteurs quelconques de \\(E_1\\) et $E_2 $, ils s’écrivent \\(m\\vec{e}\\) et \\(\\sum_{i=1}^I a_{i} \\vec{e}_{i.}\\) (avec \\(\\sum_{i=1}^I a_{i}=0\\)) et leur produit scalaire vaut \\[\n&lt;m\\vec{e};\\sum_{i=1}^I a_{i} \\vec{e}_{i.}&gt;=m\\sum_{i=1}^I a_{i}&lt;\\vec{e};\\vec{e}_{i.}&gt; = m I \\sum_{i=1}^I a_{i} =0\n\\] De même avec \\(E_1 \\perp E_3\\).\nMontrons que \\(E_1 \\perp E_4\\). Pour cela prenons deux vecteurs quelconques de \\(E_1\\) et \\(E_4\\), ils s’écrivent \\(m\\vec{e}\\) et \\(\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}\\) (avec pour tout \\(i\\) \\(\\sum_j (ab)_{ij}=0\\) et pour tout \\(j\\) \\(\\sum_i (ab)_{ij}=0\\)). Leur produit scalaire vaut \\[\\begin{align*}\n  &lt;m\\vec{e};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}&gt;\n  &=m\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}&lt;\\vec{e};\\vec{e}_{ij}&gt; = m r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\\]\nMontrons que \\(E_2 \\perp E_4\\). Pour cela prenons deux vecteurs quelconques de \\(E_2\\) et \\(E_4\\), ils s’écrivent \\(\\sum_{l=1}^I a_{l} \\vec{e}_{l}\\) (avec \\(\\sum_{l=1}^I a_{l}=0\\)) et \\(\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}\\) (avec \\(i\\) \\(\\sum_j (ab)_{ij}=0\\) et pour tout \\(j\\) \\(\\sum_i (ab)_{ij}=0\\). Leur produit scalaire vaut \\[\\begin{align*}\n  &lt;\\sum_{l=1}^I a_{l} \\vec{e}_{l.};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}&gt;\n  &=\\sum_{l=1}^I a_{l}\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}&lt;\\vec{e}_{l.};\\vec{e}_{ij}&gt; \\\\\n  &=\\sum_{l=1}^I a_{l} r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\\] De même avec \\(E_3 \\perp E_4\\).\nLa dimension de \\(E_{1}\\) vaut 1 (car \\(\\vec{e}\\) est non nul). Le sous-espace \\(E_{2}\\) est engendré par les \\(I\\) vecteurs non nuls et orthogonaux deux à deux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) donc le sous espace engendré est au moins de dimension \\(I\\). Cependant on ajoute une contrainte linéaire donc \\(dim(E_{2})=I-1\\). De même pour \\(E_{2}\\) dont la dimension est donc \\(J-1\\). Enfin sous-espace \\(E_{4}\\) est engendré par les \\(IJ\\) vecteurs non nuls et orthogonaux deux à deux \\(\\{\\vec{e}_{ij}\\}\\) (donc le sous espace engendré est de dimension \\(IJ\\)) mais auquel on ajoute plusieurs contraintes linéaires. Il faut donc compter le nombre de contrainte linéaires indépendantes.\nMontrons que les \\(I+J\\) contraintes $ i {j} (ab){ij}=0$ et \\(\\forall j \\sum_{i} (ab)_{ij}=0\\) ne sont pas indépendantes. En effet quand \\(I+J-1\\) contraintes sont vérifiées, la dernière restante l’est aussi. \\[\n  \\begin{array}{*{5}c}\n  (ab)_{11}&(ab)_{12}&\\ldots&(ab)_{1J-1}&(ab)_{1J}&=0\\\\\n  (ab)_{21}&(ab)_{22}&\\ldots&(ab)_{2J-1}&(ab)_{2J}&=0\\\\\n  \\vdots&\\vdots& &\\vdots&\\vdots&\\vdots\\\\\n  (ab)_{I1}&(ab)_{I2}&\\ldots&(ab)_{IJ-1}&(ab)_{IJ}&=0\\\\\n  =0&=0&\\ldots&=0&c=?&\n\\end{array}\n\\] Posons que \\(I+J-1\\) contraintes sont vérifiées~: \\(I\\) en ligne et \\(J-1\\) en colonnes (voir ci-dessus). En sommant toute la matrice on sait (somme en ligne) que cela vaut zéro et donc la somme en colonne vaut elle aussi 0 et donc la dernière somme \\(c\\) vaut 0 (voir ci-dessus). Nous avons donc que la dimension de \\(E_{4}\\) est \\(IJ-(I +J -1)=(I-1)(J-1)\\)\n\nCalculons \\(P_{1}Y\\) \\[\\begin{align*}\nP_{1}Y &= \\mathbf{1}(\\mathbf{1}' \\mathbf{1})^{-1} \\mathbf{1}'Y=\\mathbf{1}(IJr)^{-1} \\sum_{ijk}Y_{ijk} = \\frac{1}{IJr} Y_{...} \\mathbf{1}\\\\\n      &=\\bar Y_{...} \\vec{e}\n\\end{align*}\\]\nCalculons \\(P_{2}Y\\). On sait que \\(F_{2}=E_1 \\stackrel{\\perp}{\\bigoplus} E_2\\) et que \\(F_{2}\\) est de dimension \\(I\\). Il est donc engendré par les \\(I\\) vecteurs orthogonaux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) qui en forme une base. Du fait de la décomposition on a \\[\n\\begin{align}\nP_{F_{2}}Y &= P_{1}Y  + P_{2}Y\n\\end{align}\n\\tag{2}\\] Calculons maintenant directement la projection sur \\(F_{2}\\). Ce sous-espace est engendré par les \\(I\\) vecteurs orthogonaux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) en posant la matrice concaténant les (coordonnées des) vecteurs: \\[\\begin{align*}\nF_{2}&=(\\vec{e}_{1.}|\\vec{e}_{2}|\\cdots|\\vec{e}_{I.})\n\\end{align*}\\] on a le projecteur \\[\\begin{align*}\nP_{F_{2}}&=F_{2}(F_{2}'F_{2})^{-1}F'_{2}\n\\end{align*}\\] En effectuant le calcul matriciel on a \\[\\begin{align*}\n(F_{2}'F_{2})&=\\text{diag}(I, I, \\dotsc, I)\n\\end{align*}\\] et par calcul matriciel direct on trouve \\[\n\\begin{align}\nP_{F_{2}}Y&=F_{2}\\text{diag}(1/I, 1/I, \\dotsc, 1/I)F_{2}'Y\\nonumber\\\\\n&=F_{2}\n\\begin{pmatrix}\n\\bar Y_{1..}\\\\\n\\bar Y_{2..}\\\\\n\\vdots\\\\\n\\bar Y_{I..}\\\\\n\\end{pmatrix}\n= \\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n+ \\dotsc \\bar Y_{I..}\\vec{e}_{I.}\n\\end{align}\n\\tag{3}\\] En utilisant les équations équation 2 et équation 3 on trouve \\[\\begin{align*}\nP_{2}Y&=\\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n+ \\dotsc + \\bar Y_{I..}\\vec{e}_{I.} - \\bar Y_{...} \\vec{e}.\n\\end{align*}\\] Remarquons que \\(\\vec{e}=\\vec{e}_{1.} + \\dotsc +\\vec{e}_{I.}\\) et en remplaçant cela dans l’équation précédente nous avons \\[\n\\begin{align*}\nP_{2}Y&=(\\bar Y_{1..} -\\bar Y_{...})  \\vec{e}_{1.} + (\\bar Y_{2..} - \\bar Y_{...}) \\vec{e}_{2.}\n+ \\dotsc + (\\bar Y_{I..} -\\bar Y_{...}) \\vec{e}_{I.}.\n\\end{align*}\n\\]\nEn calquant ces calculs pour \\(E_{3}\\) on trouve \\[\\begin{align*}\nP_{3}Y&=(\\bar Y_{.1.}-\\bar Y_{...}) \\vec{e}_{.1} + (\\bar Y_{.2.}-\\bar Y_{...})\\vec{e}_{.2}\n+ \\dotsc +(\\bar Y_{.J.}-\\bar Y_{...})\\vec{e}_{.J}.\n\\end{align*}\\]\nEnfin pour \\(E_{4}\\), remarquons que \\(F_{4}=E=\\vect(\\vec{e}_{11}, \\dotsc, \\vec{e}_{IJ})\\). La projection sur \\(E\\) identifié à sa matrice \\((\\vec{e}_{11}| \\dotsc |\\vec{e}_{IJ})\\) peut être calculée de manière directe comme \\[\\begin{align}\nP_{E}Y&=E\\text{diag}(1/r, 1/r, \\cdots, 1/r)E'Y\\nonumber\\\\\n&=E\n\\begin{pmatrix}\n\\bar Y_{11.}\\\\\n\\bar Y_{21.}\\\\\n\\vdots\\\\\n\\bar Y_{IJ.}\\\\\n\\end{pmatrix}\n= \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\\label{eq:pebis}\n\\end{align}\\] En se servant de la décomposition on a \\[\\begin{align}\nP_{E}Y&=P_{1}Y + P_{2}Y  + P_{3}Y  + P_{4}Y\n\\end{align}\\] Et en identifiant les deux calculs (avec \\(\\vec{e}_{i.}=\\vec{e}_{i1} + \\dotsc +\\vec{e}_{iJ}\\) et \\(\\vec{e}_{.j}=\\vec{e}_{1j} + \\dotsc +\\vec{e}_{Ij}\\) ) \\[\\begin{align*}\nP_{4}Y&= (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ}.\n\\end{align*}\\]\nLa dernière projection s’obtient comme \\[\\begin{align*}\nQY&=Y- P_{E}Y = Y -  \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\n\\end{align*}\\]\n\nEn reprenant la décomposition en sous-espace orthogonaux suivante \\[\n\\begin{align}\n\\mathbb R^{n}&= E  \\stackrel{\\perp}{\\bigoplus} E^{\\perp} =E  \\stackrel{\\perp}{\\bigoplus} Q\\\\\n&= E_1 \\stackrel{\\perp}{\\bigoplus} E_2 \\stackrel{\\perp}{\\bigoplus} E_3 \\stackrel{\\perp}{\\bigoplus} E_4 \\stackrel{\\perp}{\\bigoplus} Q\n\\end{align}\n\\tag{4}\\] On a donc que \\[\n\\begin{align*}\nY &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y.\n\\end{align*}\n\\tag{5}\\] En utilisant toutes les définitions de la question précédente on a \\[\n\\begin{split}\nY&=\\bar Y_{...} \\vec{e}\\\\\n& \\ \\  +  (\\bar Y_{1..} - \\bar Y_{...}) \\vec{e}_{1.} + \\dotsc +\\bar (Y_{I..}- \\bar Y_{...})\\vec{e}_{I.}\\\\\n& \\ \\    + (\\bar Y_{.1.}- \\bar Y_{...}) \\vec{e}_{.1} +  \\dotsc +\\bar (Y_{.J.}- \\bar Y_{...})\\vec{e}_{.J} \\\\\n& \\ \\  + (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ} \\\\\n& \\ \\ + P_{Q}Y.\n\\end{split}\n\\tag{6}\\] En utilisant l’équation 1 on identifie terme à terme et nous obtenons les paramètres du modèle : \\[\\begin{align*}\n\\hat \\mu&=\\bar Y_{...}\\\\\n\\hat \\alpha_{i}&=(\\bar Y_{i..} - \\bar Y_{...})\\\\\n\\hat \\beta_{j}&=(\\bar Y_{.j.} - \\bar Y_{...})\\\\\n(\\widehat{\\alpha\\beta})_{ij}&=(\\bar Y_{ij.} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})\\\\\n\\end{align*}\\]\nEn utilisant l’équation 5 et en se rappelant de l’orthogonalité (équation 4) on a \\[\n\\begin{align}\n\\|Y -\\bar Y_{...} \\vec{e}\\|^{2} &=\\| P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y\\|^{2}\\nonumber\\\\\n&=\\|P_{1}Y\\|^{2} + \\|P_{2}Y\\|^{2} + \\|P_{3}Y\\|^{2} + \\|P_{4}Y\\|^{2} + \\|P_{Q}Y\\|^{2}\n\\end{align}\n\\tag{7}\\] et remplaçant les projections par leur expression (voir par exemple équation 6) et calculant les normes on a \\[\n\\begin{split}\n\\sum_{i=1}^I \\sum_{j=1}^J\\sum_{k=1}^r (Y_{ijk} - \\bar Y_{...})^{2}\n&= rJ\\sum_{i=1}^I(\\bar Y_{i..} - \\bar Y_{...})^{2}\n+ rI\\sum_{j=1}^J(\\bar Y_{.j.} - \\bar Y_{...})^{2}\\\\\n& \\ \\  + r\\sum_{i=1}^I\\sum_{j=1}^J\n(\\bar Y_{ij} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})^{2}\\\\\n& \\ \\  + \\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^r\n(Y_{ijk} -  \\bar Y_{ij})^{2}.\n\\end{split}\n\\tag{8}\\] En multipliant par \\(1/n\\) l’équation ci-dessus nous obtenons la décomposition de la variance.\nLe vecteur \\(Y\\) grâce à \\({\\mathcal{H}}_{3}\\) est un vecteur gaussien de moyenne \\(\\vec{m}=\\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\) et de variance \\(\\sigma^{2}I_{n}\\). On sait que \\[\\begin{align*}\nP_{E}Y &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y\n\\end{align*}\\] Grâce au théorème de Cochran on a que \\[\n\\frac{\\|P_{i}Y -P_{i}\\vec{m} \\|^{2}}{\\sigma^{2}} \\sim \\chi^{2} (dim(E_{i}))\n\\] ou encore que \\(\\frac{\\|P_{i}Y\\|^{2}}{\\sigma^{2}}\\) suit un \\(\\chi^{2} (dim(E_{i}))\\) décentré de paramètre de décentrage \\(\\|P_{i}\\vec{m} \\|^{2}\\). Pour \\(Q=E^{\\perp}\\) on a que \\(P_{Q}\\vec{m}=0\\) et il n’y a pas de décentrage.\nReprenons l’équation 7 qui s’écrit aussi avec des sommes (équation 8) ou encore \\[\\begin{align*}\n\\mathop{\\mathrm{SCT}}\n&= \\mathop{\\mathrm{SCE}}_{a} + \\mathop{\\mathrm{SCE}}_{b} + \\mathop{\\mathrm{SCE}}_{ab}  + \\mathop{\\mathrm{SCR}}\n\\end{align*}\\] On a donc que \\[\\begin{align*}\n\\mathop{\\mathrm{SCE}}_{a}&=\\|P_{2}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (I-1), \\|P_{2}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{b}&=\\|P_{3}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (J-1), \\|P_{3}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{ab}&=\\|P_{4}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} ((I-1)(J-1)), \\|P_{4}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{ab}&=\\|P_{Q}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (n - IJ).\n\\end{align*}\\] Nous voyons donc que chaque terme est la norme carrée d’une projection du vecteur gaussien \\(Y\\) dans un sous-espace et que ces sous-espaces sont orthogonaux 2 à 2. Les vecteurs gaussiens projetés sont donc indépendants ainsi que leur norme au carré. Les lois de ces normes carrées sont donc à \\(\\sigma^{2}\\) près des \\(\\chi^{2}\\) décentrés (sauf pour \\(Q\\)) qui sont indépendants.\nNotons \\(\\mathop{\\mathrm{CME}}_{ab} = \\mathop{\\mathrm{SCE}}_{ab}/((I-1)(J-1))\\) et \\(\\mathop{\\mathrm{CMR}}= \\mathop{\\mathrm{SCR}}/(n-IJ)\\) nous avons donc \\[\\begin{align*}\n\\frac{\\mathop{\\mathrm{CME}}_{ab}}{\\mathop{\\mathrm{CMR}}}&=\\frac{\\frac{\\mathop{\\mathrm{SCE}}_{ab}}{\\sigma^{2}(I-1)(J-1)}}{\\frac{\\mathop{\\mathrm{SCR}}}{\\sigma^{2}(n-IJ)}}\n\\end{align*}\\] qui est le rapport de deux \\(\\chi^{2}\\) indépendants ramenés à leur degrés de liberté et dont le numérateur est décentré. Nous avons donc une loi de Fisher de paramètres \\((I-1)(J-1), n -IJ, \\|P_{4}\\vec{m} \\|^{2}\\). Sous \\({\\mathrm{H_0}}:\\) « il n’y a pas d’interaction » (ou \\(P_{4}\\vec{m}=0\\)) alors la loi se simplifie et le paramètre de décentrage inconnu (qui dépend de \\(\\vec{m}\\)) disparaît et la loi est \\(F((I-1)(J-1), n -IJ)\\).",
    "crumbs": [
      "Correction des exercices",
      "II Inférence",
      "6 Variables qualitatives : ANCOVA et ANOVA"
    ]
  },
  {
    "objectID": "correction/chap4.html",
    "href": "correction/chap4.html",
    "title": "4 Extensions : non-inversibilité et (ou) erreurs corrélées",
    "section": "",
    "text": "Exercice 1 (Questions de cours) B, B, B, A\n\n\nExercice 2 (Corrélation multiple et hypothèse \\(\\mathcal{H}_1\\))  \n\nMontrons que la moyenne empirique de \\(X\\hat \\beta\\) vaut \\(\\bar Y\\). Le vecteur moyenne est obtenu en projetant sur \\(\\mathbf{1}_n\\). En effet, comme \\[\\begin{eqnarray*}\nP_{\\mathbf{1}}&=&\\mathbf{1}_n(\\mathbf{1}_n'\\mathbf{1}_n)^{-1}\\mathbf{1}_n'=\\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n',\n\\end{eqnarray*}\\] nous avons, pour une variable \\(Z=(Z_1,\\dotsc,Z_n)'\\), \\[\\begin{eqnarray*}\nP_{\\mathbf{1}}Z&=&=\\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n'Z= \\frac{1}{n}\\mathbf{1}_n\\sum_{i=1}^{n}{Z_i}=\\bar Z\\mathbf{1}_n.\n\\end{eqnarray*}\\] Comme \\(\\mathbf{1}_n\\in\\Im(X)\\), nous avons \\[\\begin{eqnarray*}\n\\bar Y&=&P_\\mathbf{1}Y=P_\\mathbf{1}P_XY=P_\\mathbf{1}X\\hat \\beta,\n\\end{eqnarray*}\\] c’est-à-dire que la moyenne empirique de \\(X\\hat \\beta\\) vaut \\(\\bar Y\\).\nLe coefficient de corrélation entre \\(\\hat Y\\) et \\(Y\\) élevé au carré s’écrit donc \\[\\begin{eqnarray*}\n\\rho^2(\\hat Y,Y)&=&\\frac{\\langle \\hat Y-\\bar Y,Y-\\bar Y\\rangle^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}\\\\\n&=&\\frac{\\langle \\hat Y-\\bar Y,Y-\\hat Y+\\hat Y-\\bar Y\\rangle^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}\\\\\n&=&\\Bigl\\{\\frac{\\langle \\hat Y-\\bar Y,Y-\\hat Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|}+\\frac{\\langle \\hat Y-\\bar Y,\\hat Y-\\bar Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|}\\Bigl\\}^2.\n\\end{eqnarray*}\\] Comme \\((Y-\\hat Y)\\in\\Im(X)^\\perp\\) et que \\((\\hat Y-\\bar Y)\\in\\Im(X)\\), nous avons \\(\\langle \\hat Y-\\bar Y,Y-\\hat Y\\rangle=0\\) et donc \\[\\begin{eqnarray*}\n\\rho^2(\\hat Y,Y)&=&\\frac{\\|\\hat Y-\\bar Y\\|^2 \\|\\hat Y-\\bar Y\\|^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}=\\mathop{\\mathrm{R^2}}2.\n\\end{eqnarray*}\\]\n\nEn effectuant le calcul nous trouvons que \\(Y-2X_1+2X_2=3\\eta\\).\nEn calculant les normes carrées, nous avons \\[\\begin{eqnarray*}\n\\|X_1\\|^2&=&1^2+1^2+1^2=3,\\\\\n\\|X_2\\|^2&=&1/2+1/2+2=3,\\\\\n\\|X_3\\|^2&=&3/2+3/2=3.\n\\end{eqnarray*}\\] En calculant les produits scalaires, nous avons \\[\\begin{eqnarray*}\n\\langle X_1,X_2\\rangle&=&1\\times 1/\\sqrt{2}+ 1\\times 1/\\sqrt{2} +1\\times (-\\sqrt{2})\n=\\sqrt{2}-\\sqrt{2}=0,\\\\\n\\langle X_1,\\eta\\rangle&=&\\sqrt{3}/\\sqrt{2}-\\sqrt{3}/\\sqrt{2}=0,\\\\\n\\langle X_2,\\eta\\rangle&=&1/\\sqrt{2}\\times\\sqrt{3}/\\sqrt{2}-1/\\sqrt{2}\\times\\sqrt{3}/\\sqrt{2}=0.\n\\end{eqnarray*}\\]\nLa représentation graphique est :\n\n\n\n\n\nNous avons ici \\(X_1\\in\\Im(X)\\), \\(X_2\\in\\Im(X)\\) et \\(\\eta\\in\\Im(X)^\\perp\\), ce qui permet de trouver \\(\\hat Y\\) : \\[\\begin{eqnarray*}\nP_XY&=&P_X(2X_1-2X_2+3\\eta)=2P_XX_1 -2P_XX_2+3P_X\\eta=\\\\\n&=&2X_1-2X_2=(2-\\sqrt{2},2-\\sqrt{2},2-2\\sqrt{2})'.\n\\end{eqnarray*}\\]\nPuisque \\(\\mathbf{1}\\) fait partie des variables explicatives, nous avons \\[\\begin{eqnarray*}\n\\rho(Y,\\hat Y)&=&\\frac{\\langle Y-\\bar Y,\\hat Y-\\bar Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|},\n\\end{eqnarray*}\\] ce qui est la définition du cosinus de l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y\\hat Y}\\).\nNotons par \\(Y_\\alpha\\) le vecteur \\(X\\alpha\\). Sa moyenne vaut \\(\\bar Y_\\alpha\\). Nous avons maintenant le cosinus de l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\). Graphiquement, la moyenne de \\(Y_\\alpha\\) est la projection sur \\(X_1=\\mathbf{1}_3\\).\nLa représentation graphique nous permet de voir que l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\) est le même que celui entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y\\hat Y}\\). L’angle est minimum (et le cosinus maximum) quand \\(\\alpha=\\hat\\beta\\) ou pour tout \\(\\alpha\\) tel que \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}=k\\overrightarrow{\\bar Y\\hat Y}\\) avec \\(k&gt;0\\).\nDu fait de l’orthogonalité entre \\(X_1\\) et \\(X_2\\), \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\) est toujours colinéaire à \\(\\overrightarrow{\\bar Y\\hat Y}\\), seul le signe change en fonction de l’orientation des vecteurs (même sens ou sens opposé).\n\nComme \\(\\rho(X_j;X_k)=1\\) alors \\(R(X_j;(\\mathbf{1},X_j))=1\\) et donc puisque la constante fait partie du modèle \\(R(X_j;X_{(j)})=1\\). L’hypothèse \\({\\mathcal{H}}_1\\) n’est donc pas vérifiée.\n\n\n\nExercice 3 (EQM de la régression Ridge)  \n\nLes démonstrations figurent en page 77 : \\[\\begin{eqnarray*}\nB(\\hat \\beta_{ridge}) &=& -\\kappa (X'X + \\kappa I)^{-1} \\beta,\\\\\nV(\\hat \\beta_{\\mathrm{ridge}})&=&\\sigma^2(X'X + \\kappa I)^{-1}X'X(X'X + \\kappa I)^{-1}\\\\\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})&=&(X'X + \\kappa I)^{-1}\\left[\\kappa^2\\beta \\beta'+\\sigma^2(X'X) \\right](X'X + \\kappa I)^{-1}.\n\\end{eqnarray*}\\]\nPuisque \\(X'X=P\\text{diag}(\\lambda_i) P'\\), nous avons \\[\\begin{eqnarray*}\n(X'X + \\kappa I)&=&P\\text{diag}(\\lambda_i) P'+ \\kappa PP'=P\\text{diag}(\\lambda_i+\\kappa)P'.\n\\end{eqnarray*}\\] En se rappelant que \\(P^{-1}=P'\\), son inverse vaut \\[\\begin{eqnarray*}\n(X'X + \\kappa I)^{-1}&=&P\\text{diag}(1/(\\lambda_i+\\kappa))P'.\n\\end{eqnarray*}\\] Nous avons donc \\[\\begin{equation*}\n\\begin{split}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})&=P\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'\\left[\\kappa^2\\beta \\beta'+\\sigma^2(X'X) \\right]P\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'\\\\\n&=P\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'.\n\\end{split}\n\\end{equation*}\\] Nous en déduisons que sa trace vaut \\[\\begin{equation*}\n\\begin{split}\n\\text{tr}\\left\\{EQM(\\hat \\beta_{\\mathrm{ridge}})\\right\\}&=\\text{tr}\\left\\{\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\right.\\\\\n&\\quad \\left.\\text{diag}(\\frac{1}{\\lambda_i+\\kappa})P'P\\right\\},\n\\end{split}\n\\end{equation*}\\] et, comme \\(P'P=I_p\\), nous avons alors \\[\\begin{eqnarray*}\n\\text{tr}\\left\\{EQM(\\hat \\beta_{\\mathrm{ridge}})\\right\\}\n&=&\\text{tr}\\left\\{\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\text{diag}(\\frac{1}{(\\lambda_i+\\kappa)^2})\\right\\}.\n\\end{eqnarray*}\\] Le \\(i^e\\) élément de la diagonale de la matrice \\(P'\\beta\\beta'P\\) vaut \\([P'\\beta]_i^2\\). Celui de \\(\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2\n  I_p\\right]\\) vaut \\(\\kappa^2[P'\\beta]_i^2+\\sigma^2\\) et celui de \\[\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2\n  I_p\\right]\\text{diag}(\\frac{1}{(\\lambda_i+\\kappa)^2})\\] vaut donc \\[\\kappa^2[P'\\beta]_i^2+\\sigma^2/(\\lambda_i+\\kappa)^2.\\] On en déduit le résultat annoncé car la trace est la somme des éléments diagonaux d’une matrice.\nL’estimateur des MC est non biaisé et son \\(\\mathop{\\mathrm{EQM}}\\) vaut sa variance : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})&=&\\sigma^2(X'X)^{-1}.\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X + \\kappa I)(X'X)^{-1}\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X(X'X)^{-1} + \\kappa I(X'X)^{-1})\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(I\\!+\\!\\kappa (X'X)^{-1})(X'X \\!+\\! \\kappa I)(X'X \\!+\\! \\kappa I)^{-1}\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X\\!+\\!2 \\kappa I \\!+\\! \\kappa^2 (X'X)^{-1})(X'X \\!+\\! \\kappa I)^{-1}.\n\\end{eqnarray*}\\]\nLe calcul de \\(\\Delta=\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})-\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\\) est immédiat en utilisant l’expression précédente de \\(\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\\) et celle rappelée en question 1.\nEn utilisant le théorème proposé avec \\(A=(X'X + \\kappa I)^{-1}\\) et \\(B=(\\sigma^2(2I_p+\\kappa^2(X'X)^{-1})-\\kappa\\beta\\beta')\\) nous obtenons le résultat demandé. Cette condition dépend de \\(\\beta\\) qui est inconnu, mais aussi de \\(X\\), c’est-à-dire des mesures obtenues.\nIntéressons-nous à la matrice \\(\\gamma\\gamma'\\). Cette matrice est symétrique donc diagonalisable, de valeurs propres positives ou nulles. La somme de ses valeurs propres est égale à la trace de cette matrice \\[\\begin{eqnarray*}\n\\text{tr}(\\gamma\\gamma')=\\text{tr}(\\gamma'\\gamma)=\\gamma'\\gamma.\n\\end{eqnarray*}\\] Montrons que cette matrice n’a qu’une seule valeur propre non nulle \\(\\gamma'\\gamma\\). Pour cela, considérons le vecteur \\(\\gamma\\in\\mathbb R^p\\) et montrons qu’il est vecteur propre de \\(\\gamma\\gamma'\\) associé à la valeur propre \\(\\gamma'\\gamma\\)~: \\[\\begin{eqnarray*}\n(\\gamma\\gamma')\\gamma&=&\\gamma(\\gamma'\\gamma)=(\\gamma'\\gamma)\\gamma.\n\\end{eqnarray*}\\] Nous avons donc un vecteur propre de \\(\\gamma\\gamma'\\) qui est \\(\\gamma\\) associé à la valeur propre \\(\\gamma'\\gamma\\). De plus, nous savons que la somme des valeurs propres positives ou nulles de \\(\\gamma\\gamma'\\) vaut \\(\\gamma'\\gamma\\). Nous en déduisons que les \\(p-1\\) valeurs propres restantes sont toutes nulles. Nous pouvons donc dire que la matrice \\(\\gamma\\gamma'\\) se décompose comme \\[\\begin{eqnarray*}\n\\gamma\\gamma'&=&UDU',\n\\end{eqnarray*}\\] où \\(U\\) est la matrice orthogonale des vecteurs propres normés à l’unité de \\(\\gamma\\gamma'\\) et \\(D=\\text{diag}(\\gamma'\\gamma,0,\\dotsc,0)\\). Nous avons donc \\[\\begin{eqnarray*}\nI_p-\\gamma\\gamma'&=&UU' - UDU'=U(\\text{diag}(1-\\gamma'\\gamma,1,\\dotsc,1)U'.\n\\end{eqnarray*}\\] Les valeurs propres de \\(I_p-\\gamma\\gamma'\\) sont donc \\(1-\\gamma'\\gamma,1,\\dotsc,1\\), qui sont toutes positives ou nulles dès que \\(\\gamma'\\gamma\\le 1\\).\nUne condition pour que \\(\\sigma^2(2I_p-\\kappa\\beta\\beta')\\) soit semi-définie positive est que \\((\\kappa\\beta\\beta')\\le \\sigma^2\\) (cf. question précédente) et donc \\((\\sigma^2(2I_p+\\kappa^2(X'X)^{-1})-\\kappa\\beta\\beta')\\) est alors la somme de 2 matrices semi-définies positives donc semi-définie positive. Cela implique qu’il s’agit d’une condition suffisante pour que \\(\\Delta\\) soit semi-définie positive.\nNous venons de montrer 2 conditions, l’une nécessaire et suffisante, l’autre suffisante, afin que \\(\\Delta\\) soit semi-définie positive. Cette assertion signifie que, quelle que soit la combinaison linéaire du vecteur de paramètre (par exemple une coordonnée), l’estimateur ridge est meilleur que celui des MC au sens de l’EQM. Cela signifie aussi que, si une de ces conditions est vérifiée, globalement au sens de la trace de l’EQM, l’estimateur ridge est meilleur que celui des MC. Au niveau des conditions, cela permet de trouver la valeur optimale de \\(\\kappa\\). Malheureusement chacune des 2 conditions dépend de la valeur \\(\\beta\\) inconnue et donc n’est pas réellement utilisable en pratique. La condition suffisante procure une amélioration, dans le sens où elle ne dépend pas de \\(X\\) donc de l’expérience. Le prix à payer est bien sûr qu’il s’agit seulement d’une condition suffisante et donc plus restrictive.\n\n\n\nExercice 4 (Régression pondérée)  \n\nNous souhaitons minimiser \\[\\begin{eqnarray*}\n\\sum_{i=1}^n \\left(y_i-\\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 p_i,\n\\end{eqnarray*}\\] où \\(p_i\\) est un réel positif. Nous pouvons écrire ce critère sous la forme suivante : \\[\\begin{eqnarray*}\n\\sum_{i=1}^n \\left(\\sqrt{p_i}y_i-\\sum_{j=1}^p \\beta_j\\sqrt{p_i} x_{ij}\\right)^2\n= \\sum_{i=1}^n \\left(y^\\star_i-\\sum_{j=1}^p \\beta_jx_{ij}^\\star\\right)^2,\n\\end{eqnarray*}\\] où \\(y^\\star_i=\\sqrt{p_i}y_i\\) et \\(x_{ij}^\\star = \\sqrt{p_i} x_{ij}\\).\nNotons \\(P^{1/2}\\) la matrice des poids qui vaut \\(P^{1/2}=\\text{diag}(\\sqrt{p_i})\\). Ce dernier critère est un critère des MC avec comme observations \\(Y^\\star\\) et \\(X^\\star\\) où \\(Y^\\star = P^{1/2} Y\\) et \\(X^\\star = P^{1/2} X\\). L’estimateur vaut alors \\[\\begin{eqnarray*}\n\\hat \\beta_{pond} &=& (X^{\\star\\prime}X^\\star)^{-1}X^{\\star\\prime}Y^\\star\\\\\n&=& (X'PX)^{-1}X'PY.\n\\end{eqnarray*}\\]\nLorsque nous avons la constante comme seule variable explicative, \\(X=\\mathbf{1}_n\\), et nous avons alors \\[\\begin{eqnarray*}\n\\hat \\beta_{pond} &=&\\frac{\\sum p_i y_i}{\\sum p_i}.\n\\end{eqnarray*}\\]\nLorsque les poids sont constants, nous retrouvons, non plus une moyenne pondérée, mais la moyenne usuelle.\n\n\n\nExercice 5 (Gauss-Markov) L’estimateur des MC s’écrit \\(\\hat \\beta_2 = \\sum_{i=1}^n p_i y_i,\\) avec \\(p_i=(x_i-\\bar x)/\\sum(x_i -\\bar x)^2\\). Considérons un autre estimateur \\(\\tilde{\\beta_2}\\) linéaire en \\(y_i\\) et sans biais, c’est-à-dire \\[\\tilde{\\beta_2} =\\sum_{i=1}^n \\lambda_i y_i.\\] Montrons que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\). L’égalité \\(\\mathbf E(\\tilde{\\beta_2}) = \\beta_1 \\sum \\lambda_i +\n\\beta_2 \\sum \\lambda_i x_i +  \\sum \\lambda_i \\mathbf E(\\varepsilon_i)\\) est vraie pour tout \\(\\beta_2\\) et \\(\\tilde \\beta_2\\) est sans biais donc \\(\\mathbf E(\\tilde \\beta_2)=\\beta_2\\) pour tout \\(\\beta_2\\), c’est-à-dire que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\).\nMontrons que \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat\n\\beta_2)\\). \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) = \\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2 + \\hat \\beta_2)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2)+\n2\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2).\n\\end{eqnarray*}\\] \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2)\n\\!=\\!\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2},\\hat \\beta_2) -\\mathop{\\mathrm{V}}(-\\hat \\beta_2)\n\\!=\\!\\frac{\\sigma^2\\sum \\lambda_i(x_i-\\bar x)}{\\sum (x_i-\\bar x)^2} -\n\\frac{\\sigma^2}{\\sum (x_i-\\bar x)^2}\n\\!=\\!0,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) =\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Une variance est toujours positive et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Le résultat est démontré. On obtiendrait la même chose pour \\(\\hat \\beta_1\\).\n\n\nExercice 6 (Corrélation spatiale)  \n\nÉcrivons pour la ligne/site \\(i\\) : \\[\\begin{align*}\nY_{i}&=X_{i.}'\\beta + \\varepsilon_{i}\n\\end{align*}\\] et nous savons que \\[\\begin{align*}\n\\varepsilon_{i}= \\rho \\sum_{i=1}^n M_{ij} \\varepsilon_{j} + \\eta_{i}\n\\end{align*}\\] Comme quand \\(i=j\\) on a \\(M_{ij}=0\\) on en déduit \\[\\begin{eqnarray*}\n\\varepsilon_i=\\rho\\sum_{j\\ne i,j=1}^{n}{M_{ij} \\varepsilon_j} + \\eta_i,\n\\end{eqnarray*}\\]\nLe site \\(i\\) est expliqué par un modèle de type auto-régression par les autres sites. Le site est très dépendant des sites proches (fort \\(M_{ij}\\)) et peu ou pas dépendant des autres sites (faible \\(M_{ij}\\) ou valeur nulle).\nOn repart de \\[\\begin{align*}\n\\varepsilon&=\\rho M \\varepsilon +\\eta\\\\\n(I - \\rho M)\\varepsilon &=\\eta\\\\\n\\varepsilon&=(I - \\rho M)^{-1}\\eta=A^{-1}\\eta\n\\end{align*}\\]\nComme \\(\\eta\\) est gaussien de moyenne nulle et de variance \\(\\sigma^{2}I\\) on a que \\(\\varepsilon\\) est gaussien de moyenne nulle et de variance \\[\\begin{align*}\n\\mathop{\\mathrm{V}}(\\varepsilon) &= \\mathop{\\mathrm{V}}(A^{-1}\\eta) = A^{-1} \\mathop{\\mathrm{V}}(\\eta) {A'}^{-1} =\\sigma^{2}  A^{-1} {A'}^{-1}\\\\\n&=\\sigma^{2}\\Omega\n\\end{align*}\\]\n\nPour la vraisemblance du modèle trouvons la loi de \\(Y=X\\beta + \\varepsilon\\). Comme \\(\\varepsilon\\) est gaussien de moyenne nulle et de variance \\(\\sigma^{2}\\Omega\\) on a que \\(Y\\) gaussien de moyenne \\(X\\beta\\) et de variance \\(\\sigma^{2}\\Omega\\). On a donc \\[\\begin{align*}\nL(Y,\\beta,\\sigma^2,\\rho)&=  \\frac{1}{(2\\pi)^{n/2}}\\frac{1}{|\\sigma^{2}\\Omega|^{1/2}}\n\\exp\\Bigl\\{-\\frac{1}{2\\sigma^2}(Y-X\\beta)'\\Omega^{-1}(Y-X\\beta)\\Bigr\\}\n\\end{align*}\\]\nLa log-vraisemblance est \\[\n\\begin{align*}\n\\mathcal{L}&=-\\frac{n}{2}\\log (2\\pi\\sigma^{2}) - \\frac{1}{2}\\log|\\Omega| - \\frac{1}{2\\sigma^2}(Y-X\\beta)'\\Omega^{-1}(Y-X\\beta)\n\\end{align*}\n\\tag{1}\\] La dérivée par rapport à \\(\\beta\\) est \\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial\\beta}&=-\\frac{1}{2\\sigma^2}X'\\Omega^{-1}(Y-X\\beta)\\\\\n\\end{align*}\\] et en l’annulant on a \\[\\begin{align*}\nX'\\Omega^{-1}Y=X'\\Omega^{-1}X\\hat\\beta\n\\end{align*}\\] d’où \\[\n\\begin{eqnarray}\n\\hat \\beta&=&(X'\\hat \\Omega^{-1}X)^{-1}X'\\hat \\Omega^{-1}Y\\nonumber\\\\\n&=&(X'\\hat A'\\hat AX)^{-1}X'\\hat A'\\hat AY.\n\\end{eqnarray}\n\\tag{2}\\]\nLa dérivée par rapport à \\(\\sigma^2\\) est \\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial\\sigma^2}&=-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}(Y-X\\beta)'\\Omega^{-1}(Y-X\\beta)\n\\end{align*}\\] et en l’annulant on a \\[\n\\begin{align}\n\\hat \\sigma^2&=\\frac{1}{n}(Y-X\\hat \\beta)'\\hat \\Omega^{-1} (Y-X\\hat \\beta)\\nonumber\\\\\n&=\\frac{1}{n}(Y-X\\hat \\beta)\\hat A'\\hat A (Y-X\\hat \\beta).\n\\end{align}\n\\tag{3}\\]\nEn utilisant l’équation 1 avec \\(\\Omega^{-1}=\\hat A'\\hat A\\) et en remplaçant \\(\\hat \\sigma^2\\) par sa valeur (équation 3) on a \\[\n   \\begin{align*}\n\\mathcal{L}(Y,\\hat\\beta,\\hat\\sigma^2,\\hat\\rho)&=  -\\frac{n}{2}\\log (2\\pi\\hat\\sigma^{2}) + \\frac{1}{2}\\log|\\hat A'\\hat A| -\n    \\frac{1}{2\\hat\\sigma^2}(Y-X\\hat\\beta)'\\hat A'\\hat A (Y-X\\hat\\beta) \\\\\n\\mathcal{L}(Y,\\hat\\beta,\\hat\\sigma^2,\\hat\\rho)&= -\\frac{n}{2}\\log (\\hat\\sigma^{2})  -\\frac{n}{2}\\log (2\\pi) + \\frac{1}{2}\\log|\\hat A'\\hat A|\n- \\frac{n(Y-X\\hat\\beta)'\\hat A'\\hat A (Y-X\\hat\\beta)}{2(Y-X\\hat \\beta)\\hat A'\\hat A (Y-X\\hat \\beta)}\\\\\n\\mathcal{L}(Y,\\hat\\beta,\\hat\\sigma^2,\\hat\\rho)&= -\\frac{n}{2}\\log (\\hat\\sigma^{2})+ \\frac{1}{2}\\log|\\hat A'\\hat A| + cte\n\\end{align*}\n\\]\nComme nous savons les valeurs de \\(\\hat \\beta\\) comme fonction de \\(\\rho\\) (équation 2) et de \\(\\hat \\sigma^2\\) comme fonction de \\(\\rho\\) (équation 3) il suffit de les remplacer dans \\(-\\mathcal{L}\\) et on obtient \\[\\begin{eqnarray*}\n    h(\\hat\\rho)\\!&=\\!&\\!\\frac{n}{2}\\log Y'(I\\!-\\!X(X'\\hat A'\\hat AX)^{-1}X'\\hat A'\\hat A)'\n\\hat A'\\hat A(I\\!-\\!X(X'\\hat A'\\hat AX)^{-1}X'\\hat A'\\hat A)Y \\\\\n    && \\quad - \\frac{1}{2}\\log|\\hat A'\\hat A|^2\n    \\end{eqnarray*}\\]",
    "crumbs": [
      "Correction des exercices",
      "I Introduction au modèle linéaire",
      "4 Extensions : non-inversibilité et (ou) erreurs corrélées"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Présentation",
    "section": "",
    "text": "3ème édition - Mai 2023\n\n\nLes auteurs\n\nPierre-André Cornillon\nNicolas Hengartner\nÉric Matzner-Løber\nLaurent Rouvière\n\n\n\nDescriptif\n\n4e de couverture\nAvant-propos\nSommaire détaillé\n\n\n\nBoutique en ligne\nPar ici\n\n\n\n\nRésumé\nCet ouvrage expose de manière détaillée, exemples à l’appui, différentes façons de répondre à un des problèmes statistiques les plus courants : la régression. Cette nouvelle édition se décompose en cinq parties.\n\nLa première donne les grands principes des régressions simple et multiple par moindres carrés. Les fondamentaux de la méthode, tant au niveau des choix opérés que des hypothèses et leur utilité, sont expliqués.\nLa deuxième partie est consacrée à l’inférence et présente les outils permettant de vérifier les hypothèses mises en œuvre. Les techniques d’analyse de la variance et de la covariance sont également présentées dans cette partie.\nLe cas de la grande dimension est ensuite abordé dans la troisième partie. Différentes méthodes de réduction de la dimension telles que la sélection de variables, les régressions sous contraintes (lasso, elasticnet ou ridge) et sur composantes (PLS ou PCR) sont notamment proposées. Un dernier chapitre propose des algorithmes, basés sur des méthodes de rééchantillonnage comme l’apprentissage/validation ou la validation croisée, qui permettent d’établir une comparaison entre toutes ces méthodes.\nLa quatrième partie se concentre sur les modèles linéaires généralisés et plus particulièrement sur les régressions logistique et de Poisson avec ou sans technique de régularisation. Une section particulière est consacrée aux comparaisons de méthodes en classification supervisée. Elle introduit notamment des critères de performance pour scorer des individus comme les courbes ROC et lift et propose des stratégies de choix seuil (Younden, macro F1…) pour les classer. Ces notions sont ensuite mises en œuvre sur des données réelles afin de sélectionner une méthode de prévision parmi plusieurs algorithmes basés sur des modèles logistiques (régularisés ou non). Une dernière section aborde le problème des données déséquilibrées qui est souvent rencontré en régression binaire.\nEnfin, la dernière partie présente l’approche non paramétrique à travers les splines, les estimateurs à noyau et des plus proches voisins. La présentation témoigne d’un réel souci pédagogique des auteurs qui bénéficient d’une expérience d’enseignement auprès de publics très variés. Les résultats exposés sont replacés dans la perspective de leur utilité pratique grâce à l’analyse d’exemples concrets. Les commandes permettant le traitement des exemples sous R figurent dans le corps du texte.\n\nEnfin, chaque chapitre est complété par une suite d’exercices corrigés.",
    "crumbs": [
      "Présentation"
    ]
  },
  {
    "objectID": "donnees.html",
    "href": "donnees.html",
    "title": "Jeux de données",
    "section": "",
    "text": "ad_data.txt\n\nartere.txt\ncourbe_lasso.csv\ndd_exo3_1.csv\ndd_exo3_2.csv\ndd_exo3_3.csv\nechan_lasso.csv\neucalyptus.txt\n\nlogit_ex6.csv\nlogit_ridge_lasso.csv\nozone.txt\nozone_complet.txt\nozone_long.txt\nozone_simple.txt\nozone_transf.txt\npanne.txt\npoissonData.csv\npoissonData3.csv",
    "crumbs": [
      "Jeux de données"
    ]
  },
  {
    "objectID": "correction/chap1.html",
    "href": "correction/chap1.html",
    "title": "1 La régression linéaire simple",
    "section": "",
    "text": "Exercice 1 (Questions de cours) B, A, B, A.\n\n\nExercice 2 (Biais des estimateurs) Les estimateurs \\(\\hat \\beta_j\\) sont fonctions de \\(Y\\) (aléatoire), ce sont donc des variables aléatoires. Une autre façon d’écrire \\(\\hat \\beta_2\\) en fonction de \\(\\beta_2\\) consiste à remplacer \\(y_i\\) par sa valeur soit \\[%\n\\begin{eqnarray*}\n\\hat \\beta_2 &=&\\frac{\\sum (x_i - \\bar x) y_i}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\beta_1\\sum (x_i - \\bar x)+\\beta_2\\sum x_i(x_i - \\bar x)+\n\\sum (x_i - \\bar x) \\varepsilon_i }{\\sum(x_i-\\bar x)^2} \\\\\n&=& \\beta_2 + \\frac{\\sum (x_i - \\bar x) \\varepsilon_i}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\n\\] Par hypothèse \\(\\mathbf E(\\varepsilon_i)=0\\), les autres termes ne sont pas aléatoires, le résultat est démontré.\nLe résultat est identique pour \\(\\hat \\beta_1\\) car \\(\\mathbf E(\\hat \\beta_1) =\n\\mathbf E(\\bar y) -\\bar x \\mathbf E(\\hat \\beta_2)= \\beta_1 +\\bar x \\beta_2 - \\bar\nx \\beta_2=\\beta_1\\), le résultat est démontré.\n\n\nExercice 3 (Variance des estimateurs) Nous avons \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\n\\mathop{\\mathrm{V}}\\left(\\beta_2+\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}\n{\\sum(x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\n\\] Or \\(\\beta_2\\) est inconnu mais pas aléatoire et les \\(x_i\\) ne sont pas aléatoires donc \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\\mathop{\\mathrm{V}}\\left(\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}{\\sum(x_i-\\bar x)^2}\\right)\n=\\frac{\\mathop{\\mathrm{V}}\\left(\\sum(x_i-\\bar x)\\varepsilon_i\\right)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}\\\\\n&=&\\frac{\\sum_{i,j}(x_i-\\bar x)(x_j-\\bar x)\\mathop{\\mathrm{Cov}}(\\varepsilon_i,\\varepsilon_j)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}.\n\\end{eqnarray*}\n\\] Or \\(\\mathop{\\mathrm{Cov}}(\\varepsilon_i,\\varepsilon_j)=\\delta_{ij}\\sigma^2\\) donc \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\\frac{\\sum_i(x_i-\\bar x)^2\\sigma^2}{\\left[\\sum_i(x_i-\\bar x)^2\\right]^2}\n=\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\n\\] Plus les mesures \\(x_i\\) sont dispersées autour de leur moyenne, plus \\(\\mathop{\\mathrm{V}}(\\hat \\beta_2)\\) est faible et plus l’estimation est précise. Bien sûr, plus \\(\\sigma^2\\) est faible, c’est-à-dire plus les \\(y_i\\) sont proches de la droite inconnue, plus l’estimation est précise.\\ Puisque \\(\\hat \\beta_1=\\bar y - \\hat \\beta_2 \\bar x\\), nous avons \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_1)\n&=&\n\\mathop{\\mathrm{V}}\\left(\\bar y-\\hat \\beta_2 \\bar x \\right)=\\mathop{\\mathrm{V}}\\left(\\bar y\\right)+V(\\bar x \\hat \\beta_2)-2\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2 \\bar x)\\\\\n&=&\\mathop{\\mathrm{V}}\\left(\\frac{\\sum y_i}{n}\\right)+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2)\\\\\n&=&\\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\sum_i\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2).\n\\end{eqnarray*}\n\\]\nCalculons \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\bar y, \\hat \\beta_2)\n&=& \\frac{1}{n}\n\\mathop{\\mathrm{Cov}}\\left(\\sum_{i}\\left(\\beta_1+\\beta_2 x_i+\\varepsilon_i\\right),\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{n}\\sum_{i}\\mathop{\\mathrm{Cov}}\\left(\\varepsilon_i,\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{\\sum_j(x_j-\\bar x)^2}\n\\sum_{i}\\frac{1}{n}\\mathop{\\mathrm{Cov}}\\left(\\varepsilon_i,\\sum_j(x_j-\\bar x)\\varepsilon_j\\right)\\\\\n&=&\\frac{\\sigma^2 \\frac{1}{n}\\sum_{i}(x_i-\\bar x)}{\\sum_j(x_j-\\bar x)^2}=0.\n\\end{eqnarray*}\n\\] Nous avons donc \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_1)\n&=& \\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\sigma^2 \\sum x_i^2}{n\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\n\\] Là encore, plus \\(\\sigma^2\\) est faible, c’est-à-dire plus les \\(y_i\\) sont proches de la droite inconnue, plus l’estimation est précise. Plus les valeurs \\(x_i\\) sont dispersées autour de leur moyenne, plus la variance de l’estimateur sera faible. De même, une faible moyenne \\(\\bar x\\) en valeur absolue contribue à bien estimer \\(\\beta_1\\).\n\n\nExercice 4 (Covariance de \\(\\hat\\beta_1\\) et \\(\\hat\\beta_2\\)) Nous avons \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\hat \\beta_1,\\hat \\beta_2) &=&\\mathop{\\mathrm{Cov}}(\\bar y-\\hat \\beta_2 \\bar x,\\hat \\beta_2)\n= \\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2)- \\bar x \\mathop{\\mathrm{V}}(\\hat \\beta_2)=\n-\\frac{\\sigma^2 \\bar x}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\n\\] La covariance entre \\(\\beta_1\\) et \\(\\beta_2\\) est négative. L’équation \\(\\bar y=\\hat \\beta_1+\\hat \\beta_2\\bar x\\) indique que la droite des MC passe par le centre de gravité du nuage \\((\\bar x, \\bar y)\\). Supposons \\(\\bar x\\) positif, nous voyons bien que, si nous augmentons la pente, l’ordonnée à l’origine va diminuer et vice versa. Nous retrouvons donc le signe négatif pour la covariance entre \\(\\hat \\beta_1\\) et \\(\\hat\n\\beta_2\\).\n\n\nExercice 5 (Théorème de Gauss-Markov) L’estimateur des MC s’écrit \\(\\hat \\beta_2 = \\sum_{i=1}^n p_i y_i,\\) avec \\(p_i=(x_i-\\bar x)/\\sum(x_i -\\bar x)^2\\).\nConsidérons un autre estimateur \\(\\tilde{\\beta_2}\\) linéaire en \\(y_i\\) et sans biais, c’est-à-dire\n\\[\\tilde{\\beta_2} =\\sum_{i=1}^n \\lambda_i y_i.\\]\nMontrons que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\). L’égalité \\(\\mathbf E(\\tilde{\\beta_2}) = \\beta_1 \\sum \\lambda_i +\n\\beta_2 \\sum \\lambda_i x_i +  \\sum \\lambda_i \\mathbf E(\\varepsilon_i)\\) est vraie pour tout \\(\\beta_2\\) et \\(\\tilde \\beta_2\\) est sans biais donc \\(\\mathbf E(\\tilde \\beta_2)=\\beta_2\\) pour tout \\(\\beta_2\\), c’est-à-dire que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\).\nMontrons que \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat\n\\beta_2)\\). \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) = \\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2 + \\hat \\beta_2)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2)+\n2\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2).\n\\end{eqnarray*}\n\\] \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2)\n\\!=\\!\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2},\\hat \\beta_2) -\\mathop{\\mathrm{V}}(-\\hat \\beta_2)\n\\!=\\!\\frac{\\sigma^2\\sum \\lambda_i(x_i-\\bar x)}{\\sum (x_i-\\bar x)^2} -\n\\frac{\\sigma^2}{\\sum (x_i-\\bar x)^2}\n\\!=\\!0,\n\\end{eqnarray*}\n\\] et donc \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) =\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\n\\] Une variance est toujours positive et donc \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\n\\] Le résultat est démontré. On obtiendrait la même chose pour \\(\\hat \\beta_1\\).\n\n\nExercice 6 (Somme des résidus) Il suffit de remplacer les résidus par leur définition et de remplacer \\(\\hat \\beta_1\\) par son expression \\[%\n\\begin{eqnarray*}\n\\sum_i \\hat \\varepsilon_i\n= \\sum_i (y_i - \\bar y + \\hat \\beta_2 \\bar x - \\hat \\beta_2 x_i)\n= \\sum_i (y_i-\\bar y) - \\hat \\beta_2 \\sum_i (x_i - \\bar x)= 0.\n\\end{eqnarray*}\n\\]\n\n\nExercice 7 (Estimateur de la variance du bruit) Récrivons les résidus en constatant que \\(\\hat \\beta_1= \\bar y- \\hat \\beta_2 \\bar x\\) et \\(\\beta_1=\\bar y - \\beta_2 \\bar x - \\bar \\varepsilon\\), \\[%\n\\begin{eqnarray*}\n\\hat \\varepsilon_i&=& \\beta_1 + \\beta_2 x_i +\\varepsilon_i - \\hat \\beta_1 - \\hat \\beta_2x_i\\\\\n&=& \\bar{y} - \\beta_2 \\bar{x} - \\bar{\\varepsilon} + \\beta_2 x_i +\\varepsilon_i -\\bar{y} + \\hat \\beta_2 \\bar{x} - \\hat \\beta_2x_i\\\\\n&=& (\\beta_2-\\hat \\beta_2)(x_i -\\bar{x}) + (\\varepsilon_i - \\bar{\\varepsilon}).\n\\end{eqnarray*}\n\\] En développant et en nous servant de l’écriture de \\(\\hat \\beta_2\\) donnée dans la solution de l’exercice 2, nous avons \\[%\n\\begin{eqnarray*}\n\\sum \\hat \\varepsilon_i^2& \\!=\\!& (\\beta_2-\\hat \\beta_2)^2 \\!\\sum  \\!(x_i \\!-\\!\\bar{x})^2\n\\!+\\!\\sum \\!(\\varepsilon_i \\!-\\! \\bar{\\varepsilon})^2\\!+\\!2 \\!(\\beta_2\\!-\\!\\hat \\beta_2)\n\\!\\sum \\!(x_i \\!-\\!\\bar{x})(\\varepsilon_i \\!- \\!\\bar{\\varepsilon})\\\\\n&=& (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2\n+\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2 - 2 (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2.\n\\end{eqnarray*}\n\\] Prenons en l’espérance \\[%\n\\begin{eqnarray*}\n\\mathbf E\\left( \\sum \\hat{\\varepsilon_i}^2\\right)= \\mathbf E\\left(\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2\\right) -\n\\sum  (x_i -\\bar{x})^2 \\mathop{\\mathrm{V}}(\\hat \\beta_2)\n= (n-2) \\sigma^2.\n\\end{eqnarray*}\n\\]\n\n\nExercice 8 (Prévision) Calculons la variance \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}\\left(\\hat y^p_{n+1}\\right)\n&=&\\mathop{\\mathrm{V}}\\left(\\hat \\beta_1 + \\hat \\beta_2x_{n+1}\\right)\n=\\mathop{\\mathrm{V}}(\\hat \\beta_1)+x_{n+1}^2 \\mathop{\\mathrm{V}}(\\hat \\beta_2)\n+2 x_{n+1} \\mathop{\\mathrm{Cov}}\\left(\\hat \\beta_1,\\hat \\beta_2\\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum x_i^2}{n}+x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum (x_i-\\bar x)^2}{n}+\\bar x^2 + x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\sigma^2\\left(\\frac{1}{n}+\n\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\n\\] Plus la valeur à prévoir s’éloigne du centre de gravité, plus la valeur prévue sera variable (i.e. de variance élevée).\nVariance de l’erreur de prévision\\ Nous obtenons la variance de l’erreur de prévision en nous servant du fait que \\(y_{n+1}\\) est fonction de \\(\\varepsilon_{n+1}\\) seulement, alors que \\(\\hat y^p_{n+1}\\) est fonction des autres \\(\\varepsilon_i\\), \\(i=1,\\cdots,n\\). Les deux quantités ne sont pas corrélées. Nous avons alors \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\varepsilon_{n+1}^p) \\!=\\! \\mathop{\\mathrm{V}}\\left(y_{n+1} \\!-\\! \\hat y_{n+1}^p\\right)\n\\!=\\! \\mathop{\\mathrm{V}}(y_{n+1})\\!+\\!\\mathop{\\mathrm{V}}(\\hat y_{n+1}^p)\\!=\\! \\sigma^2\\left(1+\\frac{1}{n}\n\\!+\\!\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\n\\]\n\n\nExercice 9 (\\(R^2\\) et coefficient de corrélation) Le coefficient \\(\\mathop{\\mathrm{R^2}}\\) s’écrit \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}&=&%\\frac{\\|\\hat{Y} -\\bar{y}\\1\\|^2}{\\|Y-\\bar{y}\\1\\|^2}=\n\\frac{\\sum_{i=1}^{n}{\\left(\\hat \\beta_1 + \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n\\sum_{i=1}^{n}{\\left(y_i-\\bar{y}\\right)^2}}=\n\\frac{\\sum_{i=1}^{n}{\\left( \\bar{y}-\\hat \\beta_2 \\bar{x}+ \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\\\\n&=&\n\\frac{\\hat \\beta_2^2\\sum_{i=1}^{n}{\\left( x_i - \\bar{x}\\right)^2}}{\n\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2}\n=\\frac{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}}{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\right]^2\n\\sum_{i=1}^{n}(y_i-\\bar{y})^2}\\\\\n&=&\\frac{\\left[\n\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2}{\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\sum_{i=1}^{n}{(y_i-\\bar{y})^2}}=\\rho^2(X,Y).\n\\end{eqnarray*}\n\\]\n\n\nExercice 10 (Les arbres) Le calcul donne \\[%\n\\begin{eqnarray*}\n\\hat \\beta_1 =\\frac{6.26}{28.29}=0.22 \\quad \\quad\n\\hat \\beta_0 = 18.34-0.22 \\times 34.9=10.662.\n\\end{eqnarray*}\n\\] Nous nous servons de la propriété \\(\\sum_{i=1}^n \\hat \\varepsilon_i=0\\) pour obtenir \\[%\n\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}2 &=& \\frac{\\sum_{i=1}^{20} (\\hat y_i - \\bar y)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=\\frac{\\sum_{i=1}^{20}\n(\\hat \\beta_1 x_i - \\hat \\beta_1 \\bar x)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=0.22^2 \\times \\frac{28.29}{2.85}=0.48.\n\\end{eqnarray*}\n\\] Les statistiques de test valent 5.59 pour \\(\\beta_0\\) et 4.11 pour \\(\\beta_1\\). Elles sont à comparer à un fractile de la loi de Student admettant 18 ddl, soit 2.1. Nous rejetons dans les deux cas l’hypothèse de nullité du coefficient. Nous avons modélisé la hauteur par une fonction affine de la circonférence, il semblerait évident que la droite passe par l’origine (un arbre admettant un diamètre proche de zéro doit être petit), or nous rejetons l’hypothèse \\(\\beta_0=0\\). Les données mesurées indiquent des arbres dont la circonférence varie de 26 à 43 cm, les estimations des paramètres du modèle sont valides pour des données proches de \\([26;43]\\).\n\n\nExercice 11 (Modèle quadratique) Les modèles sont \\[%\n\\begin{eqnarray*}\n\\mathtt{O3} &=& \\beta_1 + \\beta_2 \\mathtt{T12} +\\varepsilon \\quad\n\\hbox{modèle classique,}\\\\\n\\mathtt{O3} &=& \\gamma_1 + \\gamma_2 \\mathtt{T12}^2 +\\varepsilon \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\n\\] L’estimation des paramètres donne \\[%\n\\begin{eqnarray*}\n\\widehat{\\mathtt{O3}} &=& 31.41 + 2.7 \\ \\mathtt{T12} \\quad\\quad \\mathop{\\mathrm{R^2}}2=0.28 \\quad\n\\hbox{modèle classique,}\\\\\n\\widehat{\\mathtt{O3}} &=& 53.74 + 0.075 \\ \\mathtt{T12}^2 \\quad \\mathop{\\mathrm{R^2}}2=0.35 \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\n\\] Les deux modèles ont le même nombre de paramètres, nous préférons le modèle quadratique car le \\(\\mathop{\\mathrm{R^2}}\\) est plus élevé.",
    "crumbs": [
      "Correction des exercices",
      "I Introduction au modèle linéaire",
      "1 La régression linéaire simple"
    ]
  },
  {
    "objectID": "correction/chap5.html",
    "href": "correction/chap5.html",
    "title": "5 Inférence dans le modèle gaussien",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, C, A, B, B.\n\n\nExercice 2 (Théorème 5.1) L’IC (i) découle de la propriété (i) de la proposition 5.3. La propriété (ii) donnant un IC pour \\(\\sigma^2\\) découle de la loi de \\(\\hat \\sigma^2\\). Enfin, la propriété (iii) est une conséquence de la loi obtenue propriété (ii) de la proposition 5.3.\n\n\nExercice 3 (Test et \\(R^2\\)) En utilisant l’orthogonalité des sous-espaces (figure 5.3 page 99) et le théorème de Pythagore, nous avons \\[\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2.\n\\end{eqnarray*}\\] Nous pouvons le démontrer de la manière suivante : \\[\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat Y_0-Y+Y-\\hat Y\\|^2\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n+2\\langle \\hat Y_0-Y,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle Y-\\hat Y_0,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle.\n\\end{eqnarray*}\\] Or \\(\\Im(X_0) \\subset \\Im(X)\\), nous avons donc \\(P_{X^\\perp}P_{X_0^\\perp}=P_{X^\\perp}\\). De plus, \\(\\hat \\varepsilon=P_{X^\\perp}Y\\), cela donne \\[\\begin{eqnarray*}\n\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle &=&\n\\langle P_{X^\\perp}Y,P_{X^\\perp}Y\\rangle\n+\\langle P_{X}P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle \\\\\n&=& \\|\\hat \\varepsilon\\|^2 + 0.\n\\end{eqnarray*}\\] Le résultat est démontré, revenons à la statistique de test. Introduisons les différentes écritures du \\(\\mathop{\\mathrm{R^2}}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}2 = \\frac{\\|\\hat Y - \\bar Y\\|^2}{\\|Y - \\bar Y\\|^2}=1 -\n\\frac{\\|\\hat \\varepsilon\\|^2}{\\|Y - \\bar Y\\|^2}.\n\\end{eqnarray*}\\] La statistique de test vaut \\[\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2}\n{\\| Y-\\hat Y\\|^2}\\frac{n-p}{p-p_0}\\\\\n&=&\\frac{\\|\\hat \\varepsilon_0\\|^2/\\|Y-\\bar Y\\|^2-\n\\| \\hat \\varepsilon\\|^2/\\|Y-\\bar Y\\|^2}\n{\\| Y-\\hat Y\\|^2/\\|Y-\\bar Y\\|^2}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\\] nous obtenons \\[\\begin{eqnarray*}\nF&=&\\frac{\\mathop{\\mathrm{R^2}}-\\mathop{\\mathrm{R^2_0}}}{1-\\mathop{\\mathrm{R^2}}}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\\] soit le résultat annoncé. Cette dernière quantité est toujours positive car \\(\\mathop{\\mathrm{R^2_0}}\\leq \\mathop{\\mathrm{R^2}}\\) et nous avons là un moyen de tester des modèles emboîtés via le coefficient de détermination.\n\n\nExercice 4 (Test et \\(R^2\\) et constante dans le modèle) à corriger ;).\n\n\nExercice 5 (Ozone)  \n\nLes résultats sont dans l’ordre \\[\\begin{eqnarray*}\n6.2, 0.8, 6.66, -1.5, -1, 50, 5, 124.\n\\end{eqnarray*}\\]\nLa statistique de test de nullité du paramètre se trouve dans la troisième colonne, nous conservons \\({\\mathrm{H_0}}\\) pour les paramètres associés à Ne9 et Ne12, et la rejetons pour les autres.\nLa statistique de test de nullité simultanée des paramètres autres que la constante vaut 50. Nous rejetons \\({\\mathrm{H_0}}\\).\nNous connaissons \\[\\begin{align*}\n\\hat y^{p}_{n+1} &= x'_{n+1}\\hat \\beta,\\\\\nx'_{n+1}&=(1, 10, 20, 0, 0, 1)\\\\\n\\hat \\beta&=(62, -4, 5, -1.5, -0.5, 0.8)'\n\\end{align*}\\] et donc la prévision est \\(\\hat y^{p}_{n+1} =122.8\\). Pour l’intervalle de confiance il nous faut \\(\\hat\\sigma=16\\) mais aussi la matrice \\(X'X\\) (donc toutes les données) ce que nous n’avons pas ici. On ne peut donc faire d’intervalle de confiance.\nNous sommes en présence de modèles emboîtés, nous pouvons appliquer la formule adaptée (voir l’exercice précédent) : \\[\\begin{eqnarray*}\nF&=& \\frac{\\mathop{\\mathrm{R^2}}2-\\mathop{\\mathrm{R^2_0}}2}{1-\\mathop{\\mathrm{R^2}}2}\\frac{n-p}{p-p_0}\\\\\n&=& \\frac{0.66-0.5}{1-0.66}\\frac{124}{2}= 29.\n\\end{eqnarray*}\\] Nous conservons \\({\\mathrm{H_0}}\\), c’est-à-dire le modèle le plus simple.\n\n\n\nExercice 6 (Équivalence du test T et du test F) Récrivons la statistique de test \\(F\\), en se rappelant que \\(X_0\\) est la matrice \\(X\\) privée de sa \\(j^e\\) colonne, celle correspondant au coefficient que l’on teste : \\[\\begin{eqnarray*}\nF&=&\\frac{\\|X\\hat \\beta-P_{X_0}X\\hat \\beta\\|^2}{\\hat \\sigma^2}\n  =\\frac{\\|X_j\\hat \\beta_j-\\hat \\beta_jP_{X_0}X_j\\|^2}{\\hat \\sigma^2}\n=\\frac{\\hat\\beta_j^2}{\\hat \\sigma^2}X_j'(I-P_{X_0})X_j.\n\\end{eqnarray*}\\] Récrivons maintenant le carré de la statistique \\(T\\) en explicitant \\(\\hat\n\\sigma^2_{\\hat \\beta_j}\\) : \\[\\begin{eqnarray*}\nT^2&=&\\frac{\\hat \\beta_j^2}{\\hat \\sigma^2 [(X'X)^{-1}]_{jj}},\n\\end{eqnarray*}\\] où \\([(X'X)^{-1}]_{jj}\\) est le \\(j^e\\) élément diagonal de la matrice \\((X'X)^{-1}\\). Afin de calculer ce terme, nous utilisons la formule permettant d’obtenir l’inverse d’une matrice bloc, formule donnée en annexe A.2 page 416. Pour appliquer facilement cette formule, en changeant l’ordre des variables, la matrice \\(X\\) devient \\((X_0|X_j)\\) et \\(X'X\\) s’écrit alors \\[\\begin{eqnarray*}\nX'X&=&\\left(\n\\begin{array}{c|c}\nX'_0X_0&X'_0X_j\\\\\\hline\nX'_jX_0&X'_jX_j\n\\end{array}\\right).\n\\end{eqnarray*}\\] Son inverse, en utilisant la formule d’inverse de matrice bloc, est \\[\\begin{eqnarray*}\n[(X'X)^{-1}]_{jj}&=&\\left(X'_jX_j-X'_jX_0(X'_0X_0)^{-1}X'_0X_j\\right)^{-1}\n=\\left(X_j'(I-P_{X_0})X_j\\right)^{-1}.\n\\end{eqnarray*}\\] Nous avons donc \\(T^2=F\\). Au niveau des lois, l’égalité est aussi valable et nous avons que le carré d’un Student à \\((n-p)\\) ddl est une loi de Fisher à \\((1,n-p)\\) ddl. Bien entendu, le quantile \\((1-\\alpha)\\) d’une loi de Fisher correspond au quantile \\(1-\\alpha/2\\) d’une loi de Student. La loi \\(\\mathcal{T}\\) est symétrique autour de 0 et donc, lorsqu’elle est élevée au carré, les valeurs plus faibles que \\(t_{n-p}(\\alpha/2)\\), qui ont une probabilité sous \\({\\mathrm{H_0}}\\) de \\(\\alpha/2\\) d’apparaître, et celles plus fortes que \\(t_{n-p}(1-\\alpha/2)\\), qui ont une probabilité sous \\({\\mathrm{H_0}}\\) de \\(\\alpha/2\\) d’apparaître, deviennent toutes plus grandes que \\(t^2_{n-p}(1-\\alpha/2)\\). La probabilité que ces valeurs dépassent ce seuil sous \\({\\mathrm{H_0}}\\) est de \\(\\alpha\\) et correspond donc bien par définition à \\(f_{1,n-p}(1-\\alpha)\\).\n\n\nExercice 7 (Équivalence du test F et du test de VM) Nous avons noté la vraisemblance en début du chapitre par \\[\\begin{eqnarray*}\n\\mathcal{L}(Y,\\beta,\\sigma^2) &=& \\prod_{i=1}^n f_{Y}(y_i)\n= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\sum_{i=1}^n \\left(y_i- \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right]}\\\\\n&=& \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\|Y-X\\beta\\|^2\\right]}.\n\\end{eqnarray*}\\] Cette vraisemblance est maximale lorsque \\(\\hat \\beta\\) est l’estimateur des MC et que \\(\\hat \\sigma^2 = \\|Y-X\\hat \\beta\\|^2/n\\). Nous avons alors \\[\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}(Y,\\beta,\\sigma^2)&=&\n\\left(\\frac{n}{2\\pi\\|Y-X\\hat \\beta\\|^2 }\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\\\\\n&=&\\left(\\frac{n}{2\\pi \\mathop{\\mathrm{SCR}}}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2),\n\\end{eqnarray*}\\] où \\(\\mathop{\\mathrm{SCR}}=\\|Y-X\\hat \\beta\\|^2\\).\nSous l’hypothèse \\({\\mathrm{H_0}}\\) nous obtenons de façon évidente le résultat suivant : \\[\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}_0(Y,\\beta_0,\\sigma^2)\n=\\left(\\frac{n}{2\\pi \\mathop{\\mathrm{SCR}}_0}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2_0),\n\\end{eqnarray*}\\] où \\(\\mathop{\\mathrm{SCR}}_0\\) correspond à la somme des carrés résiduels sous \\({\\mathrm{H_0}}\\), c’est-à-dire \\(\\mathop{\\mathrm{SCR}}_0=\\|Y-X_0\\hat \\beta_0\\|^2\\). On définit le test du rapport de vraisemblance maximale (VM) par la région critique suivante : \\[\\begin{eqnarray*}\n\\mathcal{D}_\\alpha = \\left\\{\nY \\in \\mathbb R^n : \\lambda=\\frac{\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2)}\n{\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2)} &lt; \\lambda_0\n\\right\\}.\n\\end{eqnarray*}\\] La statistique du rapport de vraisemblance maximale vaut ici \\[\\begin{eqnarray*}\n\\lambda = \\left(\\frac{\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}_0}\\right)^{n/2} =\n\\left(\\frac{\\mathop{\\mathrm{SCR}}_0}{\\mathop{\\mathrm{SCR}}}\\right)^{-n/2}.\n\\end{eqnarray*}\\] Le test du rapport de VM rejette \\({\\mathrm{H_0}}\\) lorsque la statistique \\(\\lambda\\) est inférieure à une valeur \\(\\lambda_0\\) définie de façon à avoir le niveau du test égal à \\(\\alpha\\). Le problème qui reste à étudier est de connaître la distribution (au moins sous \\({\\mathrm{H_0}}\\)) de \\(\\lambda\\). Définissons, pour \\(\\lambda\\) positif, la fonction bijective \\(g\\) suivante : \\[\\begin{eqnarray*}\ng(\\lambda) = \\lambda^{-2/n}-1.\n\\end{eqnarray*}\\] La fonction \\(g\\) est décroissante (sa dérivée est toujours négative), donc \\(\\lambda&lt;\\lambda_0\\) si et seulement si \\(g(\\lambda)&gt;g(\\lambda_0)\\). Cette fonction \\(g\\) va nous permettre de nous ramener à des statistiques dont la loi est connue. Nous avons alors \\[\\begin{eqnarray*}\ng(\\lambda)&&gt;&g(\\lambda_0)\\\\\n\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&&gt;&g(\\lambda_0)\\\\\n\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&&gt;&f_0\n\\end{eqnarray*}\\] où \\(f_0\\) est déterminée par \\[\\begin{eqnarray*}\nP_{{\\mathrm{H_0}}}\\left(\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&gt;f_0\n\\right)=\\alpha,\n\\end{eqnarray*}\\] avec la loi de cette statistique qui est une loi \\(\\mathcal{F}_{p-p_0,n-p}\\) (cf.~section précédente). Le test du rapport de VM est donc équivalent au test qui rejette \\({\\mathrm{H_0}}\\) lorsque la statistique \\[\\begin{eqnarray*}\nF=\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}\n\\end{eqnarray*}\\] est supérieure à \\(f_0\\), où \\(f_0\\) est la valeur du fractile \\(\\alpha\\) de la loi de Fisher à \\((p-p_0,n-p)\\) degrés de liberté.\n\n\nExercice 8 (Test de Fisher pour une hypothèse linéaire quelconque) Nous pouvons toujours traduire l’hypothèse \\({\\mathrm{H_0}}\\) : \\(R\\beta=r\\) en terme de sous-espace de \\(\\mathcal M_X\\). Lorsque \\(r=0\\), nous avons un sous-espace vectoriel de \\(\\mathcal M_X\\) et lorsque \\(r\\neq 0\\) nous avons un sous-espace affine de \\(\\mathcal M_X\\). Dans les deux cas, nous noterons ce sous-espace \\(\\mathcal M_0\\) et \\(\\mathcal M_0 \\subset \\mathcal M_X\\). Cependant nous ne pourrons plus le visualiser facilement comme nous l’avons fait précédemment avec \\(\\mathcal M_{X_0}\\) où nous avions enlevé des colonnes à la matrice \\(X\\). Nous allons décomposer l’espace \\(\\mathcal M_X\\) en deux sous-espaces orthogonaux \\[\\begin{eqnarray*}\n\\mathcal M_X = \\mathcal M_0 \\stackrel{\\perp}{\\oplus} ( \\mathcal M_0^\\perp \\cap \\mathcal M_X ).\n\\end{eqnarray*}\\] Sous \\({\\mathrm{H_0}}\\), l’estimation des moindres carrés donne \\(\\hat Y_0\\) projection orthogonale de \\(Y\\) sur \\(\\mathcal M_0\\) et nous appliquons la même démarche pour construire la statistique de test. La démonstration est donc la même que celle du théorème 5.2. C’est-à-dire que nous regardons si \\(\\hat Y_0\\) est proche de \\(\\hat Y\\) et nous avons donc \\[\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat Y -\\hat Y_0\\|^2/\\dim(\\mathcal M_0^{\\perp}  \\cap \\mathcal M_X)}{\\|Y - \\hat Y\\|^2/\n\\dim(\\mathcal M_{X^{\\perp}})}\\\\\n&=&\\frac{n-p}{q} \\frac{\\|Y-\\hat Y_0\\|^2-\\|Y-\\hat Y\\|^2}{ \\|Y-\\hat Y\\|^2}\\\\\n&=& \\frac{n-p}{q}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}\\sim \\mathcal{F}_{q,n-p}.\n\\end{eqnarray*}\\] Le problème du test réside dans le calcul de \\(\\hat Y_0\\). Dans la partie précédente, il était facile de calculer \\(\\hat Y_0\\) car nous avions la forme explicite du projecteur sur \\(\\mathcal M_0\\). Une première façon de procéder revient à trouver la forme du projecteur sur \\(\\mathcal M_0\\). Une autre façon de faire est de récrire le problème de minimisation sous la contrainte \\(R\\beta=r\\). Ces deux manières d’opérer sont présentées en détail dans la correction de l’exercice 2.13. Dans tous les cas l’estimateur des MC contraints par \\(R\\beta=r\\) est défini par \\[\\begin{eqnarray*}\n\\hat \\beta_0&=&\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\\]\n\n\nExercice 9 (Généralisation de la régression ridge) Soit la fonction à minimiser \\[\\begin{align*}\n    R(\\beta)&=\\|Y-X\\beta\\|^2 -\\sum_{j=1}^{p}\\delta_j(\\beta_j^2) \\\\\n    &= (Y-X\\beta)'(Y-X\\beta) - \\beta' \\Delta \\beta\n  \\end{align*}\\] avec \\(\\delta_{1}, \\dotsc, \\delta_{p}\\) des réels positifs ou nuls.\nSachant que \\(\\frac{\\partial \\beta' A\\beta}{\\partial \\beta}=2A\\beta\\) (avec \\(A\\) symétrique) et que \\(\\frac{\\partial X\\beta}{\\partial \\beta}=X'\\) nous avons la dérivée partielle suivante \\[\\begin{align*}\n    \\frac{\\partial R}{\\partial \\beta}&=-2X'(Y-X\\beta)  + 2\\Delta \\beta\n  \\end{align*}\\] En annulant cette dérivée nous avons \\[\\begin{align*}\n    -2X'(Y-X\\hat\\beta_{\\mathrm{RG}}) + 2\\Delta \\hat\\beta_{\\mathrm{RG}}&=0\\\\\n         (X'X + \\Delta) \\hat\\beta_{\\mathrm{RG}} &=  X'Y\n  \\end{align*}\\] donc en prémultipliant par \\((X'X-\\Delta)^{-1}\\) nous obtenons \\[\\begin{align*}\n    \\hat\\beta_{\\mathrm{RG}}=(X'X-\\Delta)^{-1}X'Y.\n  \\end{align*}\\] En régression multiple le nombre de paramètres est \\(p=\\text{tr}(P_{X})\\) avec \\(P_{X}\\) la matrice de l’endomorphisme qui permet d’obtenir \\(\\hat Y\\) à partir de \\(Y\\). Dans cette régression ridge, nous avons que \\[\\begin{align*}\n    \\hat Y_{\\mathrm{RG}}&=X\\hat\\beta_{\\mathrm{RG}}=X(X'X-\\Delta)^{-1}X'Y\n  \\end{align*}\\] donc la matrice de l’endomorphisme est ici \\(X(X'X-\\Delta)^{-1}X'\\) et le nombre équivalent de paramètres est \\(\\text{tr}(X(X'X-\\Delta)^{-1}X')\\).\n\n\nExercice 10 (IC pour la régression ridge)  \n\nLoi de \\(\\hat \\beta\\) : \\(\\mathcal{N}(\\beta, \\sigma^{2}(X'X)^{-1})\\) grâce au modèle et à \\({\\mathcal{H}}_3\\).\nNous avons \\[\n\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)= (X'X-\\tilde\\kappa I)^{-1}X'Y\n\\] avec \\(A=(X'X-\\tilde\\kappa I)^{-1}X'\\) qui est une matrice fixe. Avec \\({\\mathcal{H}}_{3}\\) et le modèle de régression multiple on a que \\(Y\\sim\\mathcal{N}(X\\beta, \\sigma^{2}I)\\).\nPuisque \\(Y\\) est un vecteur gaussien, il en est de même de \\(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)=AY\\). Calculons son espérance \\[\\begin{align*}\n\\mathbf E(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\mathbf E(AY)=A\\mathbf E(Y)=AX\\beta\\\\\n&=(X'X-\\tilde\\kappa I)^{-1}X'X\\beta\n\\end{align*}\\] et sa variance \\[\\begin{align*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\mathop{\\mathrm{V}}(AY)=A\\mathop{\\mathrm{V}}(Y)A'=A\\sigma^{2}I A' = \\sigma^{2} A A'\\\\\n&=\\sigma^{2}(X'X-\\tilde\\kappa I)^{-1}X'X(X'X-\\tilde\\kappa I)^{-1}.\n\\end{align*}\\]\nCalculons le produit scalaire de \\(Y-\\hat Y_{\\mathrm{ridge}}\\) et \\(\\hat Y_{MC}:\\) \\[\\begin{align*}\n&lt;Y-\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}&gt;&=&lt;Y-\\hat Y_{MC} + \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}&gt;  \\\\\n& =  &lt;Y-\\hat Y_{MC}; \\hat Y_{MC}&gt; + &lt;   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}&gt;\\\\\n&= 0 + &lt;   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}&gt;\n\\end{align*}\\] Or \\(\\hat Y_{\\mathrm{ridge}} = X\\beta_{\\mathrm{ridge}}(\\tilde\\kappa)\\) donc il appartient au sous espace vectoriel \\(\\Im(X)\\), de même que \\(\\hat Y_{MC}=P_{X}Y\\). Sauf si \\(\\tilde\\kappa=0\\) on a que \\(\\hat Y_{\\mathrm{ridge}}\\neq \\hat Y_{MC}\\) donc \\(\\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}}\\) est un vecteur non nul de \\(\\Im(X)\\) et donc son produit scalaire avec \\(\\hat Y_{MC}\\in \\Im(X)\\) est non nul.\nIl faut pouvoir démontrer l’indépendance de \\(\\hat\\sigma_{\\mathrm{ridge}}\\) et \\(\\hat \\beta_{\\mathrm{ridge}}\\). Pour le théorème 5.1, on montre l’indépendance entre\\(\\hat \\beta\\) et \\(\\hat \\sigma\\) en considérant les 2 vecteurs \\(\\hat\\beta\\) et \\(\\hat \\varepsilon=(Y-\\hat Y)\\). Comme nous pouvons écrire \\(\\hat \\beta=(X'X)^{-1}X'P_XY\\), \\(\\hat \\beta\\) est donc une fonction fixe (dépendante uniquement des \\(X\\)) de \\(P_XY\\). De plus, \\(\\hat \\varepsilon=P_{X^\\perp}Y\\) est orthogonal à \\(P_XY\\). Ces 2 vecteurs suivent des lois normales et sont donc indépendants. Il en résulte que \\(\\hat \\beta\\) et \\(Y-\\hat Y\\) sont indépendants et de même pour \\(\\hat \\beta\\) et \\(\\hat \\sigma\\).\nIci, \\(\\hat\\sigma_{\\mathrm{ridge}}\\) est une fonction de \\(Y-\\hat Y_{\\mathrm{ridge}}\\). Le vecteur \\(\\hat\\beta_{\\mathrm{ridge}}=(X'X+\\tilde\\kappa I_p)^{-1}X'Y=(X'X+\\tilde\\kappa I_p)^{-1}X'P_XY\\) est une fonction fixe (\\(\\tilde \\kappa\\) est considéré comme fixé) de \\(P_XY\\). Par contre, \\(P_XY\\) n’est pas orthogonal à \\((Y-\\hat Y_{\\mathrm{ridge}})\\), comme nous l’avons montré, nous ne pouvons donc montrer l’indépendance de \\(\\hat\\beta_{\\mathrm{ridge}}\\) et \\(\\hat\\sigma_{\\mathrm{ridge}}\\).\nUne autre idée serait d’utiliser \\(\\hat\\sigma\\) mais en général si l’on utilise la régression ridge c’est que l’on se doute que \\(\\hat Y\\) n’est pas un bon estimateur de \\(X\\beta\\) et donc \\(\\hat\\sigma\\) qui est une fonction de \\(Y-\\hat Y\\) risque de ne pas être un bon estimateur de \\(\\sigma\\). L’estimateur \\(\\hat\\sigma\\) peut même être nul, ce qui pratiquement peut arriver quand \\(p&gt;n\\).\nPour la régression on considère (et c’est le cas ici) que \\(X\\) est fixe. Dans ce cas, pour effectuer un bootstrap on estime \\(\\hat \\beta\\) puis on déduit les \\(\\{\\hat \\epsilon_{i}\\}\\). De cet ensemble sont tirés de manière équiprobable avec remise \\(n\\) résidus \\(\\{\\hat \\epsilon_{i}^{*}\\}\\). Ces nouveaux résidus sont additionnés à \\(X\\beta\\) pour faire un nouveau vecteur \\(Y^{*}\\) et avoir un échantillon bootstap \\(Y^{*}, X\\). Le seul changement est que l’on utilise \\(\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)\\).\nEntrées : \\(\\tilde \\kappa\\) fixé, \\(\\alpha\\) fixé, \\(B\\) choisi.  Sorties : IC, au niveau \\(\\alpha\\), coordonnée par coordonnée de \\(\\beta\\).\n\nEstimer \\(\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)\\) .\nEn déduire \\(\\hat \\varepsilon_{\\mathrm{ridge}}=Y-X\\hat \\beta_{\\mathrm{ridge}}\\).\nPour \\(k=1\\) à \\(B\\)\n\ntirer avec remise \\(n\\) résidus estimés parmi les \\(n\\) coordonnées de \\(\\hat \\varepsilon_{\\mathrm{ridge}}\\) ;\non note ces résidus (réunis dans 1 vecteur) \\(\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}\\) ;\nconstruire 1 échantillon \\(Y^{(k)}=X\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)+\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}\\) ;\n\\(\\tilde \\kappa^{(k)} \\leftarrow \\tilde \\kappa\\) ;\nestimer le vecteur de paramètre \\(\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})=(X'X+\\tilde\\kappa^{(k)} I_p)^{-1}X'Y^{(k)}\\) ;\n\nPour \\(j=1\\) à \\(p\\)\n\ncalculer les quantiles empiriques de niveau \\(\\alpha/2\\) et \\(1-\\alpha/2\\) pour la coordonnée \\(j\\), sur tous les vecteurs \\(\\{\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa)\\}\\) ;\n\n\nIci nous souhaitons intégrer l’incertitude sur l’estimation de \\(\\tilde \\kappa\\) qui n’est pas fixé. Le plus simple est alors d’utiliser la procédure adaptée au cas \\(X\\) aléatoire: pour avoir un échantillon bootstap \\(Y^{*}, X^{*}\\) on tire au hasard avec remise les individus de cet échantillon \\(*\\) parmi les individus de départ \\(1\\leq i\\leq n\\): fs\nEntrées : \\(\\alpha\\) fixé, \\(B\\) choisi.  Sorties : IC, au niveau \\(\\alpha\\), coordonnée par coordonnée de \\(\\beta\\).\n\nPour \\(k=1\\) à \\(B\\)\n\nPour \\(i=1\\) à \\(n\\)\n\ntirer de manière équiprobable avec remise parmi \\(\\{1, 2, \\cdots, n\\}\\) (notons \\(l\\) le nombre obtenu)\nsélectionner la ligne \\(l\\) de \\(X\\) et la coordonnée \\(l\\) de \\(Y\\). Ils constitueront les individus \\(i\\) de l’échantillon bootstrap \\(k\\). On note ces individus \\(X_i^{(k)}\\) et \\(Y_i^{(k)}\\) ;\n\nOn estime \\(\\tilde \\kappa^{(k)}\\) selon la méthode choisie avec les données\\(X^{(k)}\\) et \\(Y^{(k)}\\) ;\nOn calcule le vecteur de paramètre \\(\\hat \\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})=({X^{(k)}}'X^{(k)}+\\tilde\\kappa^{(k)} I_p)^{-1}{X^{(k)}}'Y^{(k)}\\) ;\n\nPour \\(j=1\\) à \\(p\\)\n\ncalculer les quantiles empiriques de niveau \\(\\alpha/2\\) et \\(1-\\alpha/2\\) pour la coordonnée \\(j\\), sur tous les vecteurs \\(\\{\\hat \\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})\\}\\).",
    "crumbs": [
      "Correction des exercices",
      "II Inférence",
      "5 Inférence dans le modèle gaussien"
    ]
  },
  {
    "objectID": "correction/chap7.html",
    "href": "correction/chap7.html",
    "title": "7 Choix de variables",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, C, B en général. Un cas particulier de la dernière question est le suivant : si les variables sélectionnées \\(\\xi\\) engendrent un sous-espace orthogonal au sous-espace engendré par les variables non sélectionnées \\(\\bar\\xi\\), alors C est la bonne réponse.\n\n\nExercice 2 (Analyse du biais) La preuve des deux premiers points s’effectue comme l’exemple de la section 7.2.1. Nous ne détaillerons que le premier point. Supposons que \\(|\\xi|\\) soit plus petit que \\(p\\), le “vrai” nombre de variables entrant dans le modèle. Nous avons pour estimateur de \\(\\beta\\) \\[\n\\hat \\beta_{\\xi} = (X_{\\xi}'X_{\\xi})^{-1}X_{\\xi}'Y = P_{X_{\\xi}}Y.\n\\] Le vrai modèle étant obtenu avec \\(p\\) variables, \\(\\mathbf E(Y)=X_p \\beta\\). Nous avons alors \\[\n\\begin{eqnarray*}\n\\mathbf E(\\hat \\beta_{\\xi})&=&P_{X_{\\xi}}X_p \\beta\\\\\n&=& P_{X_{\\xi}}X_{\\xi} \\beta_{\\xi} +P_{X_{\\xi}}X_{\\bar \\xi} \\beta_{\\bar \\xi}.\n\\end{eqnarray*}\n\\] Cette dernière quantité n’est pas nulle sauf si \\(\\Im(X_{\\xi}) \\perp \\Im(X_{\\bar \\xi})\\). Comme \\(\\hat \\beta_{\\xi}\\) est en général biaisé, il en est de même pour la valeur prévue \\(\\hat y_{\\xi}\\) dont l’espérance ne vaudra pas \\(X\\beta\\).\n\n\nExercice 3 (Variance des estimateurs) L’estimateur obtenu avec les \\(|\\xi|\\) variables est noté \\(\\hat \\beta_{\\xi}\\) et l’estimateur obtenu dans le modèle complet \\(\\hat \\beta\\). Ces vecteurs ne sont pas de même taille, le premier est de longueur \\(|\\xi|\\), le second de longueur \\(p\\). Nous comparons les \\(|\\xi|\\) composantes communes, c’est-à-dire que nous comparons \\(\\hat \\beta_{\\xi}\\) et \\([\\hat \\beta]_{\\xi}\\). Partitionnons la matrice \\(X\\) en \\(X_{\\xi}\\) et \\(X_{\\bar \\xi}\\). Nous avons alors \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta) &=& \\sigma^2 \\left(\n\\begin{array}{cc}\nX'_{\\xi}X_{\\xi} &X'_{\\xi}X_{\\bar \\xi}\\\\\nX'_{\\bar \\xi}X_{\\xi} &X'_{\\bar \\xi}X_{\\bar \\xi}\n\\end{array}\n\\right)^{-1}.\n\\end{eqnarray*}\n\\] En utilisant la formule d’inverse par bloc, donnée en annexe A, nous obtenons \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}\\right]^{-1},\n\\end{eqnarray*}\n\\] alors que la variance de \\(\\hat \\beta_{\\xi}\\) vaut \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}\\right]^{-1}.\n\\end{eqnarray*}\n\\] Nous devons comparer \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})\\) et \\(\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi})\\). Nous avons \\[\n\\begin{eqnarray*}\nX_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}=X_{\\xi}'(I-P_{X_{\\bar \\xi}})X_{\\xi}\n=X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}.\n\\end{eqnarray*}\n\\] La matrice \\(P_{X_{\\bar \\xi}^\\perp}\\) est la matrice d’un projecteur, alors elle est semi-définie positive (SDP) (cf. annexe A), donc \\(X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}\\) est également SDP. La matrice \\(X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}\\) est définie positive (DP) puisque c’est \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})/ \\sigma^2\\). Utilisons le changement de notation suivant : \\[\n\\begin{eqnarray*}\nA=X_{\\xi}'X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi} \\quad \\hbox{et} \\quad\nB=X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi}.\n\\end{eqnarray*}\n\\] La matrice \\(A\\) est DP et la matrice \\(B\\) SDP. La propriété donnée en annexe A indique que \\(A^{-1}-(A+B)^{-1}\\) est SDP, or \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})-\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi}) = \\sigma^2 (A^{-1}-(A+B)^{-1}).\n\\end{eqnarray*}\n\\] Donc la quantité \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})-\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi})\\) est SDP. Le résultat est démontré. L’estimation, en terme de variance, de \\(\\xi\\) composantes est plus précise que les mêmes \\(\\xi\\) composantes extraites d’une estimation obtenue avec \\(p\\) composantes.\nLa variance des valeurs ajustées dépend de la variance de \\(\\hat \\beta\\), le point 2 de la proposition se démontre de façon similaire.\nRemarque : nous venons de comparer deux estimateurs de même taille via leur matrice de variance. Pour cela, nous montrons que la différence de ces deux matrices est une matrice SDP. Que pouvons-nous dire alors sur la variance de chacune des coordonnées ? Plus précisément, pour simplifier les notations, notons le premier estimateur (de taille \\(p\\)) \\(\\tilde \\beta\\) de variance \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)\\) et le second estimateur \\(\\hat \\beta\\) de variance \\(\\mathop{\\mathrm{V}}(\\hat \\beta)\\). Si \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta)\\) est SDP, pouvons-nous dire que \\(\\mathop{\\mathrm{V}}(\\tilde \\beta_i)-\\mathop{\\mathrm{V}}(\\hat \\beta_i)\\) est un nombre positif pour \\(i\\) variant de \\(1\\) à \\(p\\) ? Considérons par exemple le vecteur \\(u_1'=(1,0,\\dotsc,0)\\) de \\(\\mathbb R^p\\). Nous avons alors \\[\nu_1' \\hat \\beta = \\hat \\beta_1 \\quad \\hbox{et}\n\\quad u_1' \\tilde \\beta = \\tilde \\beta_1.\n\\] Comme \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta)\\) est SDP, nous avons pour tout vecteur \\(u\\) de \\(\\mathbb R^p\\) que \\(u'(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta))u\\geq 0\\), c’est donc vrai en particulier pour \\(u_1\\). Nous avons donc \\[\n\\begin{eqnarray*}\nu_1'(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta))u_1&\\geq& 0\\\\\nu_1'\\mathop{\\mathrm{V}}(\\tilde \\beta)u_1-u_1'\\mathop{\\mathrm{V}}(\\hat \\beta)u_1&\\geq& 0\\\\\n\\mathop{\\mathrm{V}}(u_1'\\tilde \\beta)-\\mathop{\\mathrm{V}}(u_1'\\hat \\beta)&\\geq& 0\\\\\n\\mathop{\\mathrm{V}}(\\tilde \\beta_1) &\\geq & \\mathop{\\mathrm{V}}(\\hat \\beta_1).\n\\end{eqnarray*}\n\\] Nous pouvons retrouver ce résultat pour les autres coordonnées des vecteurs estimés ou encore pour des combinaisons linéaires quelconques de ces coordonnées.\n\n\nExercice 4 (Choix de variables) Tous les modèles possibles ont été étudiés, la recherche est donc exhaustive. En prenant comme critère l’AIC ou le BIC, le modèle retenu est le modèle M134. Comme prévu, le \\(\\mathop{\\mathrm{R^2}}\\) indique le modèle conservant toutes les variables. Cependant le \\(\\mathop{\\mathrm{R^2}}\\) peut être utilisé pour tester des modèles emboîtés. Dans ce cas, le modèle retenu est également le M134.\nPour une procédure avec test nous devons démarrer d’un modèle. Démarrons par exemple du modèle avec uniquement la constante. Nous ajoutons une variable après l’autre. Nous ajoutons celle avec la statistique de test la plus élevée et cette valeur doit être plus grande que 2.3 (sinon aucune variable n’est ajoutée et le modèle courant est conservé). Dans les modèles à une variable, c’est le modèle M1 qui est choisi (statistique la plus élevée de 41.9 et supérieure à 2.3). Ensuite nous ajoutons à M1 une variable (2 ou 3 ou 4) et la meilleure est la 4 mais la statistique est de 0.9 (&lt; 2.3) on n’ajoute pas de variable et on choisit M1.\nDémarrons du modèle complet 1234, on enlève la moins significative (la valeur de la statistique de test la plus faible en dehors de la constante et qui doit être plus faible que 2.3). Ici nous enlevons la variable 2 et nous avons donc le modèle M134. Dans ce modèle la variable la moins significative est la 3 mais sa statistique est plus grande que 2.3, on conserve donc le modèle M134.\n\n\nExercice 5 (Utilisation du \\(R^2\\)) Plaçons nous dans le cas pratique où la constante fait partie des modèles, elle est donc dans une des colonnes de \\(Z\\) (par exemple la première). Le \\(\\mathop{\\mathrm{R^2}}\\) est par définition \\[\n\\begin{align*}\n\\mathop{\\mathrm{R^2}}(Z)&=\\frac{\\|P_{Z}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\\\\\n\\mathop{\\mathrm{R^2}}(X)&=\\frac{\\|P_{X}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\n\\end{align*}\n\\] Comme \\(\\Im(Z)\\subset \\Im(X)\\) on peut décomposer en deux \\(\\Im(X)\\): la partie \\(\\Im(Z)\\) puis le reste ce qui se note \\[\n\\Im(X)=\\Im(Z) \\stackrel{\\perp}{\\oplus} (\\Im(X)\\cap \\Im(Z)^{\\perp})\n\\] Au niveau des projecteurs on a donc \\[\nP_{X}=P_{Z} + P_{X\\cap Z^{\\perp}}\n\\] ce qui permet d’écrire le \\(\\mathop{\\mathrm{R^2}}\\) du modèle avec \\(X\\) comme suit puis par pythagore (\\((P_{Z}Y - P_{\\mathbf{1}}Y)\\in \\Im(Z)\\) et \\(P_{X\\cap Z^{\\perp}}Y\\in \\Im(Z)^{\\perp}\\)) \\[\n\\begin{align*}\n\\mathop{\\mathrm{R^2}}(X)&=\\frac{\\|P_{Z}Y + P_{X\\cap Z^{\\perp}}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}} \\\\\n&= \\frac{\\|(P_{Z}Y - P_{\\mathbf{1}}Y) + P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\\\\\n&=\\frac{\\|(P_{Z}Y - P_{\\mathbf{1}}Y) \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}} + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}= \\mathop{\\mathrm{R^2}}2(Z) + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\n\\end{align*}\n\\] Comme la seconde partie est positive ou nulle on a que \\(\\mathop{\\mathrm{R^2}}(X)\\) est au moins aussi grand que \\(\\mathop{\\mathrm{R^2}}(Z)\\). Il y a égalité dans le cas rare où \\(Y\\perp (\\Im(X)\\cap \\Im(Z)^{\\perp})\\) c’est à dire que l’on a ajouté des variables qui ne sont pas entièrement reliées aux variables de \\(Z\\) mais dont la partie non redondante avec celle de \\(Z\\) (qui existe car le rang de \\(X\\) est \\(p\\)) a une corrélation empirique avec \\(Y\\) de 0 exactement. Comme le \\(\\mathop{\\mathrm{R^2}}\\) augmente en ajoutant des variables le modèle sélectionné sera celui avec toutes les variables.\nDonc si nous cherchons à expliquer la concentration d’ozone à Rennes et si nous avons en même temps la consommation de nouille au Viêt Nam les mêmes jours, le \\(\\mathop{\\mathrm{R^2}}\\) nous conduira à sélectionner aussi la consommation de nouille au Viet-Nam comme variable explicative, variable dont on sent le peu d’influence sur la concentration d’ozone.\n\n\nExercice 6 (Cas orthogonal)  \n\nLes variables sont orthogonales donc on a \\(X'X=I_{p}\\) et l’estimateur des MCO s’écrit \\[\n\\hat \\beta=(X'X)^{-1}X'Y=X'Y\n\\] En remplaçant \\(Y\\) par le modèle (\\(Y=X\\beta + \\varepsilon\\)) on a \\[\n\\hat \\beta=X'X\\beta + X'\\varepsilon = \\beta + X'\\varepsilon.\n\\]\nLa somme des résidus vaut ici \\[\n\\begin{align*}\n\\mathop{\\mathrm{SCR}}&=\\|Y-X\\hat\\beta\\|^{2}=(Y-X\\hat\\beta)'(Y-X\\hat\\beta)= Y'Y - 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta\\\\\n&= \\sum_{i=1}^n y_{i}^2- 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta.\n\\end{align*}\n\\] Évaluons le second terme du membre de droite: comme \\(X\\hat\\beta=P_{X}Y\\) et en utilisant le fait que \\(Y=P_{X}Y + P_{X^{\\perp}}Y\\) on obtient \\[\nY'X\\hat\\beta= (P_{X}Y + P_{X^{\\perp}}Y)'P_{X}Y= Y' P_{X}' P_{X}Y = \\hat\\beta'X'X\\hat \\beta\n\\] car \\(P_{X}Y\\) et \\(P_{X^{\\perp}}Y\\) sont orthogonaux et leur produit scalaire \\(Y'P_{X^{\\perp}}'P_{X}Y\\) vaut 0. On a donc \\[\n\\mathop{\\mathrm{SCR}}=\\sum_{i=1}^n y_{i}^2 - \\hat\\beta'X'X\\hat \\beta\n\\] Comme ici la matrice \\(X\\) est orthogonale on a \\(X'X=I_{p}\\) et l’expression devient \\[\n\\mathop{\\mathrm{SCR}}=\\sum_{i=1}^n y_{i}^2 - \\sum_{j=1}^p \\hat \\beta_{j}\n\\tag{1}\\] La somme du carré des résidus est d’autant plus faible que les coefficients estimés sont grands en valeur absolue.\nPrenons un modèle \\(\\xi\\) qui regroupe les \\(k\\) premières variables (ce qui est pratique pour les notations). Comme la matrice \\(X\\) est orthogonale on a au niveau des sous espaces que toutes les variables engendrent des sous-espaces vectoriels orthogonaux que l’on peut regrouper: \\[\n\\begin{align*}\n\\Im(X) &= \\Im(X_{1})\\stackrel{\\perp}{\\oplus} \\Im(X_{2})\\stackrel{\\perp}{\\oplus} \\cdots \\stackrel{\\perp}{\\oplus}\\Im(X_{p})\\\\\n&= \\Im(X_{\\xi})\\stackrel{\\perp}{\\oplus}\\Im(X_{\\bar\\xi})\n\\end{align*}\n\\] Ce qui donne au niveau des projecteurs \\[\nP_{X}= P_{X_{\\xi}} + P_{X_{\\bar\\xi}}\n\\] Écrivons par ailleurs l’ajustement: \\[\n\\begin{align*}\nP_{X}Y&= X\\hat \\beta= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\\\\\nP_{X_{\\xi}}Y + P_{X_{\\bar\\xi}}Y&= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\n\\end{align*}\n\\] Si je projette sur \\(\\Im(X_{\\xi})\\) l’équation ci-dessus cela nous donne (\\(\\Im(X_{\\bar\\xi})\\subset \\Im(X_{\\xi})^{\\perp}\\)) \\[\nP_{X_{\\xi}}Y= X_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}\n\\] Or nous savons que les MCO dans le modèle \\(\\xi\\) donne l’ajustement \\(P_{X_{\\xi}}Y=X_{\\xi}\\hat\\beta_{\\xi}\\) qui est unique, ce qui donne en identifiant les deux écritures que \\[\nX_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}= X_{\\xi}\n\\] Comme une base de \\(\\Im(X_{\\bar\\xi})\\) est donnée par les colonnes orthonormées de \\(X_{\\xi}\\) on en déduit que les coefficients dans la base sont uniques d’où \\[\n[\\hat \\beta]_{\\xi}= \\hat\\beta_{\\xi}.\n\\]\nLes critères AIC et BIC valent pour le modèle \\(\\xi\\) \\[\n-2\\mathcal{L} + 2 |\\xi + 1| f(n)\n\\] et quand on compare le modèle \\(\\xi\\) et le modèle \\(\\{\\xi, l\\}\\) on compare donc \\[\n-2\\mathcal{L}(\\xi) + 2 |\\xi + 1| f(n) \\ \\ \\mathrm{et} \\ \\ -2\\mathcal{L}(\\{\\xi, l\\}) + 2 |\\xi + 2| f(n)\n\\] Comme \\(\\mathcal{L}(\\xi)\\) vaut \\(-{n}/{2}\\log\\sigma^{2} -{n}/{2}-{1}/{2\\sigma^{2}}.\\mathop{\\mathrm{SCR}}(\\xi)\\) lorsque l’on compare on peut éliminer les termes identiques et il reste donc \\[\n\\frac{1}{\\sigma^{2}}\\mathop{\\mathrm{SCR}}(\\xi) \\ \\ \\mathrm{et} \\ \\  \\frac{1}{\\sigma^{2}}\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\}) + 2  f(n)\n\\] Effectuons la différence: \\[\n\\Delta =  \\frac{1}{\\sigma^{2}}(\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\}) - \\mathop{\\mathrm{SCR}}(\\xi)) + 2  f(n)\n\\] et en utilisant équation 1 et la question 3 on obtient \\[\n\\begin{align*}\n\\Delta &= \\frac{1}{\\sigma^{2}}(\\sum_{i=1}^n y_{i}^2 - \\sum_{\\xi} \\hat \\beta_{j}^2 - \\hat \\beta_{l}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2)+ 2  f(n)\\\\\n&= -\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n)\n\\end{align*}\n\\] Quand \\(\\Delta\\) est négatif l’AIC (ou le BIC) du modèle \\(\\{\\xi, l\\}\\) est plus faible que l’AIC (ou le BIC) que celui du modèle \\(\\xi\\), c’est à dire que l’AIC (ou le BIC) du modèle \\(\\{\\xi, l\\}\\) est meilleur: on ajoute la variable \\(l\\). Cela donne donc \\[\n\\begin{align*}\n\\Delta & &lt; 0\\\\\n-\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n) & &lt; 0\\\\\n\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}}&&gt; 2f(n)\n\\end{align*}\n\\]\nEn écrivant que \\[\nN=\\frac{\\mathop{\\mathrm{SCR}}(\\xi) - \\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{\\sigma^{2}}\n\\] nous voyons qu’il faut évaluer la différence des SCR. En utilisant équation 1 et la question 3 on a \\[\n\\begin{align*}\n\\mathop{\\mathrm{SCR}}(\\xi) - \\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})&=\\sum_{i=1}^n y_{i}^2  -  \\sum_{\\xi} \\hat \\beta_{j}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2 + \\hat \\beta_{l}^2  \\\\\n&= \\hat \\beta_{l}^2\n\\end{align*}\n\\] Nous en déduisons la valeur de \\(N\\) (la dernière égalité découle du calcul de \\(\\hat \\beta\\) en question 1) \\[\nN=\\frac{\\hat\\beta_{l}^{2}}{\\sigma^{2}}= \\frac{(\\beta_{l} + [X' \\varepsilon]_{l})^{2}}{\\sigma^{2}}\n\\] Le dernier membre nous indique que la partie aléatoire est \\([X' \\varepsilon]_{l}\\). D’après \\({\\mathcal{H}}_{3}\\) on a que \\(X' \\varepsilon\\) est gaussien d’espérance \\(\\mathbf E(X'\\varepsilon)=X'\\mathbf E(\\varepsilon)=0\\) et de variance \\(\\mathop{\\mathrm{V}}(X'\\varepsilon)=X'\\mathop{\\mathrm{V}}(\\varepsilon)X=\\sigma^{2}I_{p}\\). Sa coordonnée \\(l\\) notée \\([X' \\varepsilon]_{l}\\) est donc une loi normale \\(N(0, \\sigma^{2})\\) et on en déduit que \\[\n\\frac{\\hat \\beta_{l}}{\\sigma} \\sim N(\\frac{\\beta_{l}}{\\sigma}, 1).\n\\] Le carré de cette loi normale (noté \\(N\\)) est donc un \\(\\chi^{2}(1)\\) décentré de paramètre de décentrage \\({\\beta_{l}^{2}}/{\\sigma^{2}}\\). On connait donc la loi de la statistique de test \\(N\\) (un \\(\\chi^{2}(1)\\) décentré de paramètre de décentrage \\(\\beta_{l}^{2}\\)). Quand \\({\\mathrm{H_0}}\\) est vrai, c’est-à-dire le modèle \\(\\xi\\) est valide, il n’y a pas la variable \\(l\\) dans le modèle, donc \\(\\beta_{l}=0\\) et le paramètre de décentrage vaut \\(0\\). Le seuil de rejet basé sur la statistique \\(N\\) est son quantile de niveau 0.95 sous \\({\\mathrm{H_0}}\\)’ c’est à dire celui d’un \\(\\chi^{2}(1)\\) à 0.95 :\n\nqchisq(0.95, df=1)\n\n[1] 3.841459\n\n\nPar définition le \\(\\mathop{\\mathrm{R^2_a}}\\) vaut \\[\n\\mathop{\\mathrm{R^2_a}}(\\xi) =1-\\frac{n-1}{\\mathop{\\mathrm{SCT}}}\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}\n\\] Calculons la différence des \\(\\mathop{\\mathrm{R^2_a}}\\) \\[\n\\Delta\\mathop{\\mathrm{R^2_a}}= \\frac{n-1}{\\mathop{\\mathrm{SCT}}}(\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{n-|\\xi| -1})\n\\] Cette différence est positive (on ajoute la variable \\(l\\)) si \\[\n\\begin{align*}\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{n-|\\xi| -1}&&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}- \\frac{\\mathop{\\mathrm{SCR}}(\\xi) -\\hat\\beta^{2}_{l} }{n-|\\xi| -1}&&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}.\\frac{n-|\\xi|}{n-|\\xi|-1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1} &&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}\\frac{-1}{n-|\\xi| -1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1}&&gt;0\n\\end{align*}\n\\] En utilisant l’approximation on a donc approximativement \\[\n\\begin{align*}\n   \\frac{1}{n-|\\xi| -1}(\\hat\\beta^{2}_{l}- \\sigma^{2})  &&gt;0\\\\\n  \\frac{\\sigma^{2}}{n-|\\xi| -1} (\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1)&&gt;0\\\\\n\\end{align*}\n\\] Le premier terme étant positif on retrouve que \\[\n\\begin{align*}\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1&&gt;0\\\\\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}&&gt;1.\n\\end{align*}\n\\]\nComme \\[\n\\mathop{\\mathrm{C_p}}(\\xi)=\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2}-n+2|\\xi|.\n\\] on fait là encore la différence des \\(\\mathop{\\mathrm{C_p}}\\) \\[\n\\begin{align*}\n\\Delta\\mathop{\\mathrm{C_p}}&= \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})-\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2}+2 =\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi) -\\hat\\beta^{2}_{l} -\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2} +2\\\\\n&=-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2\n\\end{align*}\n\\] Cette différence est négative (on ajoute la variable \\(l\\)) si \\[\n-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2&lt;0 \\quad\\Longleftrightarrow\\quad\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2}&gt;2.\n\\]\nD’après la question 3 nous n’avons besoin d’estimer qu’une fois le modèle complet et les coordonnées donnent les estimateurs dans les modèles restreints.\nAlgorithme :\n\nEstimer \\(\\beta\\) dans le modèle complet \\(\\hat \\beta = X'Y\\) .\nDéduire \\(\\hat\\sigma^{2}\\) dans le modèle complet \\(\\hat\\sigma^{2}=\\|Y - X\\hat \\beta\\|^{2}/(n-p)\\) .\nOrdonner les coordonnées dans l’ordre décroissant: \\[\n\\hat \\beta_{{(1)}} \\geq \\hat \\beta_{{(2)}}\\geq \\cdots \\geq \\hat \\beta_{{(p)}}.\n\\] Les colonnes correspondantes seront notées \\((k)\\).\nPour \\(k=1\\) à \\(p\\)\n\nSi \\(\\frac{\\hat\\beta^{2}_{(k)}}{\\hat\\sigma^2}\\) &gt; Seuil alors ajout la variable \\((k)\\)\nSinon Sortie de la boucle, plus de variable à ajouter\n\nLe seuil vaut \\(2f(n)\\) pour l’AIC et le BIC, il vaut 3.84 pour un test, 1 pour le \\(\\mathop{\\mathrm{R^2_a}}\\) et 2 pour le \\(\\mathop{\\mathrm{C_p}}\\).",
    "crumbs": [
      "Correction des exercices",
      "III Réduction de dimension",
      "7 Choix de variables"
    ]
  },
  {
    "objectID": "correction/chap10.html",
    "href": "correction/chap10.html",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "",
    "text": "Exercice 1 (Fonctions R)  \n\nfonction sse_regbic :\n\nsse_regbic &lt;- function(don, bloc, b, nvmax, method) {\n    recherche &lt;- regsubsets(maxO3~., int=T, nbest=1,data=don[bloc!=b, ],\n                       nvmax=nvmax, method=method)\n    resume &lt;- summary(recherche)\n    nomselec &lt;- colnames(resume$which)[resume$which[which.min(resume$bic), ]][-1]\n    formule &lt;- formula(paste(\"maxO3 ~\", paste(nomselec, collapse = \"+\")))\n    m_reg &lt;- lm(formule, data=don[bloc!=b, ])\n    previsions &lt;- predict(m_reg, don[bloc==b, ])\n    return(sum((don[bloc==b,\"maxO3\"] - previsions)^2))\n}\n\nfonction sse_glmnet :\n\nlibrary(glmnet)\nsse_glmnet &lt;- function(X, Y, bloc, b, a) {\n    rech &lt;- cv.glmnet(X[bloc!=b, ], Y[bloc!=b, drop=FALSE], alpha=a)\n    prev &lt;- predict(rech, newx=X[bloc==b, ], s=rech$lambda.min)\n    return(sum((Y[bloc==b, \"maxO3\"] - as.vector(prev))^2))\n}\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_2\\) et \\(\\beta_3\\).\nfonction sse_pls :\n\nsse_pls &lt;- function(don, bloc, b, compmax) {\n    rech &lt;- plsr(maxO3~., data=don[bloc!=b, ], ncomp=compmax, validation=\"CV\", scale=TRUE)\n    ncomp &lt;- which.min(MSEP(rech)$val[\"CV\", , ])-1\n    prev &lt;- predict(rech, newdata=don[bloc==b, ], ncomp=ncomp)\n    return(sum((don[bloc==b, \"maxO3\"] - as.vector(prev))^2))\n}\n\nfonction sse_pls :\n\nsse_pcr &lt;- function(don, bloc, b, compmax) {\n    rech &lt;- pcr(maxO3~., data=don[bloc!=b, ],ncomp=compmax, validation=\"CV\", scale=TRUE)\n    ncomp &lt;- which.min(MSEP(rech)$val[\"CV\", , ])-1\n    prev &lt;- predict(rech, newdata=don[bloc==b, ], ncomp=ncomp)\n    return(sum((don[bloc==b, \"maxO3\"] - as.vector(prev))^2))\n}",
    "crumbs": [
      "Correction des exercices",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap3.html",
    "href": "code/chap3.html",
    "title": "3 Validation du modèle",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone_long.txt\", header = T, sep = \";\")\nmod.lin6v &lt;- lm(O3~T6 + T12 + Ne12 + Ne15 + Vx + O3v,data=ozone)\n\n\nplot(rstudent(mod.lin6v), pch = \".\",ylab = \"Résidus studentisés par VC\")\nabline(h = c(-2,2))\nlines(lowess(rstudent(mod.lin6v)))\n\n\n\n\n\n\n\n\n\nplot(mod.lin6v, which = 2, sub = \"\", main = \"\")\nabline(0,1)\n\n\n\n\n\n\n\n\n\nplot(cooks.distance(mod.lin6v),type=\"h\",ylab=\"Distance de Cook\")\np &lt;- ncol(ozone) ; n &lt;- nrow(ozone)\nseuil1 &lt;- qf(0.1,p,n-p) ; abline(h=seuil1)\n\n\n\n\n\n\n\ninfl.ozone &lt;- influence.measures(mod.lin6v)\nplot(infl.ozone$infmat[,\"hat\"],type=\"h\",ylab=\"hii\")\nseuil1 &lt;- 3*p/n ; abline(h=seuil1,col=1,lty=2)\nseuil2 &lt;- 2*p/n ; abline(h=seuil2,col=1,lty=3)\n\n\n\n\n\n\n\n\n\nresidpartiels &lt;- resid(mod.lin6v, type = \"partial\")\nprov &lt;- loess(residpartiels[,\"O3v\"] ~ ozone$O3v)\nordre &lt;- order(ozone$O3v)\nplot(ozone$O3v, residpartiels[,\"O3v\"], pch=\".\",ylab=\"\",xlab=\"\")\nmatlines(ozone$O3v[ordre], predict(prov)[ordre])\nabline(lsfit(ozone$O3v, residpartiels[,\"O3v\"]), lty = 2)",
    "crumbs": [
      "Codes R",
      "I Introduction au modèle linéaire",
      "3 Validation du modèle"
    ]
  },
  {
    "objectID": "correction/chap9.html",
    "href": "correction/chap9.html",
    "title": "9 Régression sur composantes : PCR et PLS",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, B, C, A, B (les composantes sont orthogonales donc faire des régressions univariées \\(Y\\) contre chaque composante \\(X^{*}_{j}\\) ou faire la régression multiple \\(Y\\) contre toutes les composantes \\(X^{*}_{j}\\) revient au même pour des données centrées), A, C.\n\n\nExercice 2 (Régression sur composantes) Créons une fonction pour calculer \\(X^*\\) et la matrice \\(\\Lambda^{-1}\\) et \\(P\\)\n\northonormalisation &lt;- function(X) {\n    Diago &lt;- eigen(t(X)%*%X, symmetric=TRUE)\n    valpos &lt;- Diago$values&gt;1e-10\n    Lambda &lt;- Diago$values[valpos]\n    P &lt;- Diago$vector[, valpos]\n    ## X^*\n    Xet &lt;- X %*% P\n    return(list(Xet=Xet, P=P, Lambda=Lambda))\n}\n\nCréons une fonction pour\n\ncentrer réduire\ncalculer \\(X^*\\) et la matrice \\(\\Lambda^{-1}\\) et \\(P\\) (la fonction est proposée dans le dernier exercice)\nChoisir l’ordre de présentation des coefficients\nCalculer le BIC ou l’AIC\nRetourne les coefficients sur l’échelle de départ\n\n\nreg_CP_BIC &lt;- function(X, Y, pen=log(nrow(X))) {\n    n &lt;- nrow(X)\n    Xbar  &lt;- apply(X, 2, mean)\n    stdX &lt;- sqrt(apply(X, 2, var)*(n-1)/n)\n    Xcr &lt;- scale(X, center=Xbar, scale=stdX)\n    ortho &lt;- orthonormalisation(Xcr)\n    ## \\hat \\beta^*\n    betaet &lt;- (1/ortho$Lambda * t(ortho$Xet)) %*% Y\n    ## choix des coef non nuls\n    ## ordre\n    ordre &lt;- order(abs(betaet*sqrt(ortho$Lambda)), decreasing = TRUE)\n    betaetO &lt;- betaet[ordre]\n    LambdaO &lt;- ortho$Lambda[ordre]\n    ####################\n    ## BIC\n    ####################\n    BIC &lt;- rep(0, length(ordre))\n    SCR &lt;- sum((Y - mean(Y))^2)\n    for (k in 1:length(ordre)){\n        SCR &lt;- SCR - betaetO[k]^2 * LambdaO[k]\n        BIC[k] &lt;- n*log(SCR/n) + k*pen\n    }\n    nb &lt;- which.min(BIC)\n    choix &lt;- ordre[1:nb]\n    betaetfin &lt;- betaet*0\n    betaetfin[choix,1] &lt;-  betaet[choix,1]\n    ####################\n    ## RETOUR aux données\n    ####################\n    betafin &lt;- (ortho$P%*%betaetfin)/stdX\n    mu &lt;- mean(Y)-Xbar%*%betafin\n    return(list(mu=mu,beta=betafin,nbr=nb))\n}\n\n\n\nExercice 3 (Régression sur composantes) identique\n\n\nExercice 4 (Régression sur composantes) Nous prenons ici l’ordre des composantes de l’ACP Créons une fonction pour\n\ncentrer réduire\ncalculer \\(X^*\\) et la matrice \\(\\Lambda^{-1}\\) et \\(P\\) (la fonction est proposée dans le dernier exercice)\nRetourne les coefficients sur l’echelle de départ et sur l’échelle centrée réduite pour comparer avec la library pls.\n\n\nreg_CP &lt;- function(X, Y, K) {\n    n &lt;- nrow(X)\n    Xbar  &lt;- apply(X, 2, mean)\n    # standardisation comme dans pcr\n    stdX &lt;- sqrt(apply(X, 2, var))\n    Xcr &lt;- scale(X, center=Xbar, scale=stdX)\n    ortho &lt;- orthonormalisation(Xcr)\n    ## \\hat \\beta^*\n    betaet &lt;- (1/ortho$Lambda * t(ortho$Xet)) %*% Y\n    betaetfin &lt;- betaet*0\n    betaetfin[1:K,1] &lt;-  betaet[1:K,1]\n    betafinpcr &lt;- ortho$P%*%betaetfin\n    ####################\n    ## RETOUR aux données\n    ####################\n    betafin &lt;- (ortho$P%*%betaetfin)/stdX\n    mu &lt;- mean(Y)-Xbar%*%betafin\n    return(list(mu=mu,beta=betafin, betaCR=betafinpcr))\n}\nozone &lt;- read.table(\"../donnees/ozone.txt\", header=TRUE,sep=\";\", row.names=1)\nlibrary(pls)\nmodele.pcr &lt;- pcr(O3~.,ncomp=2,data = ozone[,1:10],scale=T)\ncoefpcr1 &lt;- coefficients(modele.pcr)[,1,1]\ncoefpcr2 &lt;- reg_CP(ozone[,2:10], ozone[,1], K=2)$betaCR\nall( abs(coefpcr1-coefpcr2)&lt;1e-14)\n\n[1] TRUE\n\n\n\n\nExercice 5 (Théorème 9.2) Elle s’effectue par récurrence. Nous allons ajouter à cette propriété un résultat intermédiaire qui constituera la première partie de la propriété: \\[\nX^{(j)}=X\\prod_{i=1}^{j-1}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X).\n\\] La seconde partie sera bien sûr de vérifier que \\(\\tilde{w}^{(j)}\\) s’écrit bien sous la forme annoncée.\nLa propriété pour \\(j=1\\) : la première partie n’a pas de sens et, concernant \\(\\tilde{w}^{(j)}\\), par construction \\(X=X^{(1)}\\) et donc \\(\\tilde{w}^{(1)}=w^{(1)}\\).\nLa propriété pour \\(j=2\\) est-elle vraie ?  Nous savons que par définition \\(X^{(2)}=P_{{t^{(1)}}^\\perp}X^{(1)}\\) et \\(X^{(1)}=X\\) donc \\[\n\\begin{eqnarray*}\nX^{(2)}&=&P_{{t^{(1)}}^\\perp}X^{(1)}=X-P_{t^{(1)}}X\n=X-t^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X\\\\\n&=&X(I-w^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X),\n\\end{eqnarray*}\n\\] car \\(t^{(1)}=Xw^{(1)}\\). Ceci démontre la première partie de la propriété. Ensuite, puisque \\(t^{(2)}=X^{(2)}w^{(2)}=X\\tilde{w}^{(2)}\\), en remplaçant \\(X^{(2)}\\) par \\(X(I-w^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X)\\) nous avons démontré la propriété pour le rang \\(j=2\\).\nSupposons maintenant la propriété vraie au rang \\((j-1)\\). Nous avons par définition : \\(X^{(j)}=P_{{t^{(j-1)}}^\\perp}X^{(j-1)}\\) donc \\(X^{(j)}=X^{(j-1)}-P_{t^{(j-1)}}X^{(j-1)}\\). Or par construction les \\(\\{t^{(k)}\\}_{k=1}^j\\) sont toutes orthogonales donc \\(P_{t^{(j-1)}}X^{(j-1)}=P_{t^{(j-1)}}X\\). Nous avons, grâce à la propriété vraie pour le rang \\((j-1)\\), que\n\\[\n\\begin{split}\nX^{(j)}&=X^{(j-1)}-t^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X\\\\\n&=X^{(j-1)}-X^{(j-1)}w^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X\\\\\n&=X\\prod_{i=1}^{j-2}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X)\n(I-w^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X)\n\\end{split}\n\\] démontrant la première partie de la proposition. Ensuite, puisque \\(t^{(j)}=X^{(j)}w^{(j)}=X\\tilde{w}^{(j)}\\), en remplaçant \\(X^{(j)}\\) par \\(X\\prod_{i=1}^{j-1}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X)\\), nous avons démontré la propriété pour le rang \\(j\\).\n\n\nExercice 6 (Géométrie des estimateurs)  \n\nLes quatre premières réponses sont évidentes, les coordonnées de \\(\\hat Y\\) valent \\(1.5,0.5\\) et \\(0\\). Ici \\(p\\) vaut 2 et \\(B_1\\) est un cercle de centre \\(O\\) de rayon 1, alors que \\(B_2\\) est un losange.\nIntuitivement, l’image d’un cercle par une application linéaire est une ellipse et l’image d’un losange est un parallélogramme.\nLe dessin suivant représente les ensembles \\(C_1\\) et \\(C_2\\) et \\(\\hat Y\\) grâce aux ordres R suivants :\n\nX &lt;- matrix(c(1,0,0,1/sqrt(3),2/sqrt(3),0),3,2)\nsss &lt;- 1\niter &lt;- 1\ncoord &lt;- matrix(0,500,2)\nfor (tt in  seq(-pi,pi,length=500)) {\n   coord[iter,] &lt;- (X%*%as.matrix(sqrt(sss)\n                         *c(cos(tt),sin(tt))))[1:2,]\n   iter &lt;- iter+1\n }\niter &lt;- 1\ncoord2 &lt;- matrix(0,500,2)\nfor (tt in  seq(-1,1,length=250)) {\n   coord2[iter,] &lt;- (X%*%as.matrix(c(tt,1-abs(tt))))[1:2,]\n   coord2[iter+250,] &lt;- (X%*%as.matrix(c(tt,\n                                abs(tt)-1)))[1:2,]\n   iter &lt;- iter+1\n }\nplot(coord,type=\"l\",xlab=\"\",ylab=\"\")\nlines(coord2)\n\n\n\n\n\n\nPar définition, \\(X\\hat \\beta_{\\mathrm{ridge}}\\) est l’élément de \\(C_1\\) le plus proche de \\(\\hat Y\\). De même, \\(X\\hat \\beta_{\\mathrm{lasso}}\\) est l’élément de \\(C_2\\) le plus proche de \\(\\hat Y\\). Cela donne graphiquement \nL’ensemble \\(C_1\\), composé de vecteurs de la forme \\(u=X_1\\alpha_1+X_2\\alpha_2\\) avec la norme du vecteur \\(\\alpha\\) valant 1, peut être vu comme l’ensemble des composantes dans lequel on va choisir la composante PLS. La première composante PLS est le vecteur de \\(C_1\\) dont le produit scalaire avec \\(Y\\) (et donc \\(\\hat Y\\)) est le plus grand. Graphiquement, c’est le vecteur de \\(C_1\\) dont l’extrémité sur l’ellipse est le pied de la tangente à l’ellipse perpendiculaire à \\(O\\hat Y\\). La prévision de \\(Y\\) par la régression PLS est la projection de \\(Y\\) et donc de \\(\\hat Y\\) sur la composante PLS.\nLa calcul donne simplement \\[\n\\begin{eqnarray*}\nX'X = \\begin{pmatrix}\n1&\\sqrt{3}/3\\\\\n\\sqrt{3}/3&5/3\n\\end{pmatrix}.\n\\end{eqnarray*}\n\\] Les valeurs propres sont 2 et 2/3. Le premier axe principal correspond au vecteur propre associé à la valeur 2. Pour trouver la première composante principale, il faut pré-multiplier ce vecteur par \\(X\\). Cela donne le vecteur de coordonnées \\((1,1,0)'\\). Les commandes GNU-R sont\n\nX &lt;- matrix(c(1,0,0,1/sqrt(3),2/sqrt(3),0),3,2)\nsvd &lt;- eigen(t(X)%*%X)\nX%*%svd$vect[,1]\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    0\n\n\nLa prévision de \\(Y\\) par la régression PCR est la projection de \\(Y\\) (et donc de \\(\\hat Y\\)) sur la composante PCR. Dans cet exemple, la projection de \\(\\hat Y\\) sur la composante PCR est un point de l’ellipse, mais cela est uniquement dû aux données de cet exercice. Le graphique suivant représente les 4 projections :\n\n\n\n\n\n\n\n\nExercice 7 (Orthonormalisation) Créons une fonction pour calculer \\(X^*\\) et la matrice \\(\\Lambda^{-1}\\) et \\(P\\)\n\northonormalisation &lt;- function(X) {\n    Diago &lt;- eigen(t(X)%*%X, symmetric=TRUE)\n    valpos &lt;- Diago$values&gt;1e-10\n    Lambda &lt;- Diago$values[valpos]\n    P &lt;- Diago$vector[, valpos]\n    ## X^*\n    Xet &lt;- X %*% P\n    return(list(Xet=Xet, P=P, Lambda=Lambda))\n}",
    "crumbs": [
      "Correction des exercices",
      "III Réduction de dimension",
      "9 Régression sur composantes : PCR et PLS"
    ]
  },
  {
    "objectID": "correction/chap16.html",
    "href": "correction/chap16.html",
    "title": "16 Introduction à la régression spline",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nC\nA\nA\nB\n\n\n\nExercice 2 (Fonction polyreg)  \n\nOn importe les données\n\nozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=T,sep=\";\")\nsdT12 &lt;- sd(ozone$T12)\n\nOn crée la grille\n\ngrillex &lt;- seq(min(ozone$T12)-sdT12,max(ozone$T12)+sdT12, length =100)\n\nOn transforme en data frame\n\ndf &lt;- data.frame(T12=grillex)\n\nOn effectue une régression polynomiale de degré 3\n\nbasepoly &lt;- poly(ozone$T12,degree=3,raw=T)\nnewval &lt;- predict(basepoly,df$T12)\ndfpoly &lt;- data.frame(O3=ozone$O3,basepoly)\nregpoly &lt;- lm(O3~.,data=dfpoly)\n\nOn prévoit sur une grille\n\ndfnewval &lt;- data.frame(newval)\nprev &lt;- predict(regpoly,dfnewval) \nplot(O3~T12,data=ozone)\nlines(grillex,prev,col=2)\n\n\n\n\n\n\n\n\nCréation de la fonction\n\npolyreg &lt;- function(ozone,degre=3){\n  sdT12 &lt;- sd(ozone$T12)\n  grillex &lt;- seq(min(ozone$T12)-sdT12,max(ozone$T12)+sdT12, length =100)\n  df &lt;- data.frame(T12=grillex)\n  basepoly &lt;- poly(ozone$T12,degree=3,raw=T)\n  newval &lt;- predict(basepoly,df$T12)\n  dfpoly &lt;- data.frame(O3=ozone$O3,basepoly)\n  regpoly &lt;- lm(O3~.,data=dfpoly)\n  dfnewval &lt;- data.frame(newval)\n  prev &lt;- predict(regpoly,dfnewval)\n  return(list(grillex,prev))\n  }\n\n\n\n\nExercice 3 On applique la fonction précédente\n\nozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=T,sep=\";\")\nplot(O3 ~ T12, xlim=c(0,35), ylim=c(0,150), data=ozone)\niter &lt;- 1\nfor(ii in c(1,2,3,9)){\n    tmp &lt;- polyreg(ozone,d=ii)\n    lines(tmp$grillex,tmp$grilley,col=iter,lty=iter)\n    iter &lt;- iter+1\n    }\nlegend(15,150,c(\"d=1\",\"d=2\",\"d=3\",\"d=9\"),col=1:4,lty=1:4)\n\n\n\n\n\n\n\n\n\n\nExercice 4 Considérons la matrice \\(X_B\\) du plan d’expérience obtenue à partir d’une variable réelle \\(X\\) transformée dans \\(\\mathcal{S}_{\\xi}^{d+1}\\) . Cette matrice est composée des \\(d+K+1\\) fonction de base notée \\(b_j\\) et où \\(K\\) est le nombre de noeuds intérieurs et \\(d\\) le degré.\nDans le cours, il est indiqué que les fonctions de base \\(b_j\\) et \\(b_{j+d+1}\\) en conservant l’ordre des fonctions. Donc \\(b_1\\) est orthogonale à toutes les fonctions \\(b_j\\) avec \\(j&gt;d+1\\), idem pour \\(b_2\\) avec \\(j&gt;d+2\\).\nEn faisant donc le calcul \\(X_B'X_B\\) on obtient une matrice bande et donc les termes \\(a_{ij}\\) sont nuls quand \\(j&gt;i+d+1\\).\nOn en déduit que les paramètres estimées \\(\\hat \\beta_k\\) ne sont pas corrélés avec les \\(\\hat \\beta_j\\) dès que \\(j&gt;k+d+1\\). XB est une matrice bande.",
    "crumbs": [
      "Correction des exercices",
      "V Introduction à la régression non paramétrique",
      "16 Introduction à la régression spline"
    ]
  },
  {
    "objectID": "code/chap16.html",
    "href": "code/chap16.html",
    "title": "16 Introduction à la régression spline",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=T,sep=\";\")\n\n\npolyreg &lt;- function(donnee,d=3){\n  sigmax &lt;- sd(donnee[,\"T12\"])\n  grillex &lt;- seq(min(donnee[,\"T12\"])-sigmax,max(donnee[,\"T12\"])+sigmax,length=100)\n  aprevoir &lt;- data.frame(T12=grillex)\n  regpol &lt;- lm(O3~poly(T12,degree=d,raw=TRUE),data=donnee)\n  prev &lt;- predict(regpol,aprevoir)\n  return(list(grillex=grillex,grilley=prev))\n}\n\n\nplot(O3~T12,data=ozone,xlab=\"T12\",ylab=\"O3\",xlim=c(0,35), ylim=c(0,150))\niter &lt;- 1\nfor(ii in c(1,2,3,9)){\n tmp &lt;- polyreg(ozone,d=ii)\n lines(tmp$grillex,tmp$grilley,col=iter,lty=iter)\n iter &lt;- iter+1\n}\nlegend(\"topleft\",c(\"d=1\",\"d=2\",\"d=3\",\"d=9\"),col=1:4,lty=1:4)\n\n\n\n\n\n\n\n\n\nind &lt;- which(ozone[,2]&lt;23)\nregd &lt;- lm(O3~T12,data=ozone[ind,])\nregf &lt;- lm(O3~T12,data=ozone[-ind,])\ngxd &lt;- seq(3,23,length=50)\ngyd &lt;- regd$coef[1]+gxd*regd$coef[2]\ngxf &lt;- seq(23,35,length=50)\ngyf &lt;- regf$coef[1]+gxf*regf$coef[2]\nplot(O3~T12,data=ozone)\nlines(gxd,gyd,col=2,lty=1,lwd=2)\nlines(gxf,gyf,col=2,lty=1,lwd=2)\nabline(v=23)\n\n\n\n\n\n\n\n\n\nlibrary(splines)\nXB &lt;- bs(ozone[,2], knots=c(15,23), degree=2,Boundary.knots=c(5,32))\nregs &lt;- lm(ozone[,\"O3\"] ~ XB)\nregs$coef\n\n(Intercept)         XB1         XB2         XB3         XB4 \n  51.101947   61.543761    5.562286   70.459103  106.711539 \n\n\n\ngrillex &lt;- seq(5,32,length=100)\nbgrillex &lt;- bs(grillex, knots=c(15,23), degree=2,Boundary.knots=c(5,32))\nprev &lt;- bgrillex%*%as.matrix(regs$coeff[-1])+regs$coeff[1]\nplot(O3~T12,data=ozone)\nlines(grillex,prev,col=2)\nabline(v=c(15,23))\n\n\n\n\n\n\n\n\n\nregssplinel1 &lt;- smooth.spline(ozone[,2],ozone[,1],lambda =100)\nprevl1 &lt;- predict(regssplinel1,grillex)\nplot(O3~T12,data=ozone)\nlines(prevl1$x,prevl1$y,col=2)\n\n\n\n\n\n\n\n\n\nregsspline &lt;- smooth.spline(ozone[,2],ozone[,1])\nprev &lt;- predict(regsspline,grillex)\nplot(O3~T12,data=ozone)\nlines(prev$x,prev$y,col=2)\n\n\n\n\n\n\n\n\n\nregsspline\n\nCall:\nsmooth.spline(x = ozone[, 2], y = ozone[, 1])\n\nSmoothing Parameter  spar= 0.9410342  lambda= 0.006833357 (15 iterations)\nEquivalent Degrees of Freedom (Df): 4.156771\nPenalized Criterion (RSS): 11036.88\nGCV: 289.8012",
    "crumbs": [
      "Codes R",
      "V Introduction à la régression non paramétrique",
      "16 Introduction à la régression spline"
    ]
  },
  {
    "objectID": "code/chap1.html",
    "href": "code/chap1.html",
    "title": "1 Régression simple",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=TRUE,sep=\";\")\nplot(O3~T12, data=ozone, xlab=\"T12\", ylab=\"O3\")\n\n\n\n\n\n\n\n\n\nreg &lt;- lm(O3~T12, data=ozone)\nsummary(reg)\n\n\nCall:\nlm(formula = O3 ~ T12, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-45.256 -15.326  -3.461  17.634  40.072 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4150    13.0584   2.406     0.02 *  \nT12           2.7010     0.6266   4.311 8.04e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.5 on 48 degrees of freedom\nMultiple R-squared:  0.2791,    Adjusted R-squared:  0.2641 \nF-statistic: 18.58 on 1 and 48 DF,  p-value: 8.041e-05\n\n\n\nplot(O3~T12, data=ozone)\nT12 &lt;- seq(min(ozone[,\"T12\"]), max(ozone[,\"T12\"]), length = 100)\ngrille &lt;- data.frame(T12)\nICdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nmatlines(grille$T12, cbind(ICdte), lty = c(1,2,2), col = 1)\n\n\n\n\n\n\n\n\n\nplot(O3~T12, data = ozone, ylim = c(0,150))\nT12 &lt;- seq(min(ozone[,\"T12\"]), max(ozone[,\"T12\"]), length = 100)\ngrille &lt;- data.frame(T12)\nICdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nICprev &lt;- predict(reg, new=grille, interval=\"pred\", level=0.95)\nmatlines(T12, cbind(ICdte,ICprev[,-1]), lty=c(1,2,2,3,3), col=1)\nlegend(\"topleft\", lty=2:3, c(\"Y\",\"E(Y)\"))\n\n\n\n\n\n\n\n\n\nIC &lt;- confint(reg, level = 0.95)\nIC\n\n               2.5 %   97.5 %\n(Intercept) 5.159232 57.67071\nT12         1.441180  3.96089\n\n\n\nlibrary(ellipse)\nplot(ellipse(reg, level=0.95), type = \"l\", xlab = \"\", ylab = \"\")\npoints(coef(reg)[1], coef(reg)[2], pch = 3)\nlines(IC[1,c(1,1,2,2,1)], IC[2,c(1,2,2,1,1)], lty = 2)\n\n\n\n\n\n\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nplot(ht~circ, data = eucalypt, xlab = \"circ\", ylab = \"ht\")\n\n\n\n\n\n\n\n\n\nreg &lt;- lm(ht~circ, data = eucalypt)\nsummary(reg)\n\n\nCall:\nlm(formula = ht ~ circ, data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7659 -0.7802  0.0557  0.8271  3.6913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.037476   0.179802   50.26   &lt;2e-16 ***\ncirc        0.257138   0.003738   68.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.199 on 1427 degrees of freedom\nMultiple R-squared:  0.7683,    Adjusted R-squared:  0.7682 \nF-statistic:  4732 on 1 and 1427 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(ht~circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),\n                                max(eucalypt[,\"circ\"]), length = 100))\nICdte &lt;- predict(reg, new=grille, interval=\"confi\", level=0.95)\nmatlines(grille$circ, ICdte, lty = c(1,2,2), col = 1)\n\n\n\n\n\n\n\n\n\nplot(ht~circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ncirc &lt;- seq(min(eucalypt[,\"circ\"]),max(eucalypt[,\"circ\"]), len = 100)\ngrille &lt;- data.frame(circ)\nCdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nICprev &lt;- predict(reg, new=grille, interval=\"pred\", level=0.95)\nmatlines(circ, cbind(ICdte,ICprev[,-1]),lty=c(1,2,2,3,3), col=1)",
    "crumbs": [
      "Codes R",
      "I Introduction au modèle linéaire",
      "1 Régression simple"
    ]
  },
  {
    "objectID": "code/chap17.html",
    "href": "code/chap17.html",
    "title": "17 Estimateurs à noyau et \\(k\\) plus proches voisins",
    "section": "",
    "text": "Estimateurs à noyau\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\")\n\n\nind &lt;- order(ozone[,\"T12\"])\nT12o &lt;- ozone[ind,\"T12\"]\nO3o &lt;- ozone[ind,\"O3\"]\n\n\nreg1 &lt;- lm(O3o~1,weight=c(rep(1,10),rep(0,40)))\nreg2 &lt;- lm(O3o~1,weight=c(rep(0,10),rep(1,10),rep(0,30)))\nreg3 &lt;- lm(O3o~1,weight=c(rep(0,20),rep(1,10),rep(0,20)))\nreg4 &lt;- lm(O3o~1,weight=c(rep(0,30),rep(1,10),rep(0,10)))\nreg5 &lt;- lm(O3o~1,weight=c(rep(0,40),rep(1,10)))\n\n\nplot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\nabline(v=c(14,18),col=\"red\",lwd=2)\nabline(v=c(16),col=\"blue\",lty=2)\npoints(16,mean(O3o[T12o&gt;=14 & T12o&lt;=18]),col=\"blue\",pch=17,cex=1.5)\n\n\n\n\n\n\n\n\n\nlibrary(ibr)\nx &lt;- seq(7,30,by=0.01)\npar(mfrow=c(1,3))\nh &lt;- c(20,3,0.05)\nfor (i in h){\n  plot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\n  tmp &lt;- npregress(T12o,O3o,bandwidth = i)\n  prev &lt;- predict(tmp,newdata=x)\n  lines(x,prev,col=\"blue\",lwd=2)\n}\n\n\n\n\n\n\n\n\n\n\nLes \\(k\\) plus proches voisins\n\npar(mfrow=c(1,3))\nlibrary(FNN)\nk &lt;- c(50,10,1)\nfor (i in k){\n  mod &lt;- knn.reg(train=T12o,test=as.matrix(x),y=O3o,k=i)\n  plot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\n  lines(x,mod$pred,col=\"blue\",lwd=2)\n}\n\n\n\n\n\n\n\n\n\n\nSélection des paramètres\n\nhcv &lt;- npregress(T12o,O3o)$bandwidth\nhcv\n\n[1] 1.688373\n\n\n\nknn.reg(train=T12o,y=O3o,k=10)$PRESS/length(T12o)\n\n[1] 287.6629\n\n\n\nK_cand &lt;- 1:49\nloo &lt;- rep(0,length(K_cand))\nfor (i in 1:length(K_cand)){\n  loo[i] &lt;- knn.reg(train=T12o,y=O3o,k=K_cand[i])$PRESS/length(T12o)\n}\nK_cand[which.min(loo)]\n\n[1] 8\n\n\n\nmod.kppv &lt;- knn.reg(train=T12o,test=as.matrix(x),y=O3o,k=8)\nmod.noyau &lt;- npregress(T12o,O3o,bandwidth = hcv)\nprev.noyau &lt;- predict(mod.noyau,newdata=x)\nplot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\nlines(x,mod.kppv$pred,col=\"blue\",lwd=2)\nlines(x,prev.noyau,col=\"red\",lty=2,lwd=2)\n\n\n\n\n\n\n\n\n\nmod.noyau$df\n\n[1] 5.339428",
    "crumbs": [
      "Codes R",
      "V Introduction à la régression non paramétrique",
      "17 Estimateurs à noyau et $k$ plus proches voisins"
    ]
  },
  {
    "objectID": "code/chap5.html",
    "href": "code/chap5.html",
    "title": "5 Inférence dans le modèle Gaussien",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\nresume3 &lt;- summary(modele3)\ncoef3 &lt;- coef(resume3)\nIC3 &lt;- t(confint(modele3, level = 0.95))\nIC3\n\n       (Intercept)       T12        Vx      Ne12\n2.5 %     57.15842 0.3138112 0.1491857 -6.960609\n97.5 %   111.93625 2.3162807 0.8237055 -2.826137\n\n\n\nlibrary(ellipse)\npar(mfrow=c(3,2))\nfor(i in 1:3){\n  for(j in (i+1):4){\n    plot(ellipse(modele3,c(i,j),level=0.95),type=\"l\",\n         xlab=paste(\"beta\",i,sep=\"\"),ylab=paste(\"beta\",j,sep=\"\"))\n    points(coef(modele3)[i], coef(modele3)[j],pch=3)\n    lines(c(IC3[1,i],IC3[1,i],IC3[2,i],IC3[2,i],IC3[1,i]),\n          c(IC3[1,j],IC3[2,j],IC3[2,j],IC3[1,j],IC3[1,j]),lty=2)\n  }\n}\n\n\n\n\n\n\n\n\n\nc(resume3$sigma^2*modele3$df.res/qchisq(0.975,modele3$df.res),\n  resume3$sigma^2*modele3$df.res/qchisq(0.025,modele3$df.res))\n\n[1] 133.6699 305.3706\n\n\n\nExemple 1 : la concentration en ozone\n\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\nresume3 &lt;- summary(modele3)\nresume3\n\n\nCall:\nlm(formula = O3 ~ T12 + Vx + Ne12, data = ozone)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.0461  -8.4824   0.7861   7.7024  28.2916 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  84.5473    13.6067   6.214 1.38e-07 ***\nT12           1.3150     0.4974   2.644  0.01117 *  \nVx            0.4864     0.1675   2.903  0.00565 ** \nNe12         -4.8934     1.0270  -4.765 1.93e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 46 degrees of freedom\nMultiple R-squared:  0.6819,    Adjusted R-squared:  0.6611 \nF-statistic: 32.87 on 3 and 46 DF,  p-value: 1.663e-11\n\n\n\nmodele2 &lt;- lm(O3 ~ T12 + Vx, data = ozone)\nanova(modele2, modele3)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ T12 + Vx\nModel 2: O3 ~ T12 + Vx + Ne12\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     47 13299.4                                  \n2     46  8904.6  1    4394.8 22.703 1.927e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nExemple 2 : la hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nregsimple &lt;- lm(ht ~ circ, data = eucalypt)\nregM &lt;- lm(ht ~ circ + I(sqrt(circ)), data = eucalypt)\nanova(regsimple, regM)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ circ\nModel 2: ht ~ circ + I(sqrt(circ))\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1   1427 2052.1                                 \n2   1426 1840.7  1    211.43 163.8 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(regM)\n\n\nCall:\nlm(formula = ht ~ circ + I(sqrt(circ)), data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1881 -0.6881  0.0427  0.7927  3.7481 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -24.35200    2.61444  -9.314   &lt;2e-16 ***\ncirc           -0.48295    0.05793  -8.336   &lt;2e-16 ***\nI(sqrt(circ))   9.98689    0.78033  12.798   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.136 on 1426 degrees of freedom\nMultiple R-squared:  0.7922,    Adjusted R-squared:  0.7919 \nF-statistic:  2718 on 2 and 1426 DF,  p-value: &lt; 2.2e-16\n\n\n\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),\n                                max(eucalypt[,\"circ\"]), len = 100))\nICdte &lt;- predict(regM,new=grille,interval=\"conf\",level=0.95)\nICpre &lt;- predict(regM,new=grille,interval=\"pred\",level=0.95)\nplot(ht ~ circ, data = eucalypt, pch=\"+\", col=\"grey60\")\nmatlines(grille,cbind(ICdte,ICpre[,-1]),lty=c(1,2,2,3,3),col=1)\nlegend(\"topleft\", lty=2:3, c(\"E(Y)\",\"Y\"))\n\n\n\n\n\n\n\n\n\n\nIntervalle de confiance bootstrap\n\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\n\n\nresume3 &lt;- summary(modele3)\nresume3$coef[,1:2]\n\n              Estimate Std. Error\n(Intercept) 84.5473326 13.6067253\nT12          1.3150459  0.4974102\nVx           0.4864456  0.1675496\nNe12        -4.8933729  1.0269960\n\n\n\nres &lt;- residuals(modele3)\nychap &lt;- predict(modele3)\nCOEFF &lt;- matrix(0, ncol = 4, nrow = 1000)\ncolnames(COEFF) &lt;- names(coef(modele3))\nozone.boot &lt;- ozone\n\n\nfor(i in 1:nrow(COEFF)){\n  resetoile &lt;- sample(res, length(res), replace=T)\n  O3etoile &lt;- ychap + resetoile\n  ozone.boot[,\"O3\"] &lt;- O3etoile\n  regboot &lt;- lm(formula(modele3), data=ozone.boot)\n  COEFF[i,] &lt;- coef(regboot)\n }\n\n\napply(COEFF, 2, quantile, probs = c(0.025,0.975))\n\n      (Intercept)       T12        Vx      Ne12\n2.5%      59.0565 0.3754807 0.1793584 -6.924389\n97.5%    109.3364 2.2212252 0.7865275 -2.968154\n\n\n\nhist(COEFF[,\"T12\"], main = \"\", xlab = \"Coefficient de T12\")",
    "crumbs": [
      "Codes R",
      "II Inférence",
      "5 Inférence dans le modèle Gaussien"
    ]
  },
  {
    "objectID": "correction/chap14.html",
    "href": "correction/chap14.html",
    "title": "14 Comparaison en classification supervisée",
    "section": "",
    "text": "Exercice 1 (Questions de cours) C, D, A, B.\n\n\nExercice 2 (Règle de Bayes)  \n\n\nLa règle de Bayes est définie par \\[%\ng^\\star(x)=\\left\\{\n\\begin{array}{ll}\n  1& \\text{si }\\mathbf P(Y=1|X=x)\\geq 0.5 \\\\\n  0& \\text{sinon.}\n\\end{array}\\right.\n\\] L’erreur de Bayes est définie par \\(L^\\star=\\mathbf P(g^\\star(X)\\neq Y)\\).\nOn a \\[%\n\\begin{aligned}\n  \\mathbf P(g(X)\\neq Y|X=x) & = 1-\\Big(\\mathbf P(g(X)=Y,g(X)=1|X=x) \\\\\n  & \\hspace{2cm}+\\mathbf P(g(X)=Y,g(X)=0|X=x)\\Big) \\\\\n  & = 1-\\Big(\\mathbf 1_{g(x)=1}\\mathbf P(Y=1|X=x) \\\\\n  & \\hspace{2cm}+\\mathbf 1_{g(x)=0}\\mathbf P(Y=0|X=x)\\Big) \\\\\n  & = 1-(\\mathbf 1_{g(x)=1}\\eta(x)+\\mathbf 1_{g(x)=0}(1-\\eta(x))).\n\\end{aligned}\n\\]\nOn déduit \\[%\n\\begin{aligned}\n  \\mathbf P( g(X)\\neq Y|X=x)-\\mathbf P(g^\\star(X)\\neq Y|X=x)\n  &  = \\eta(x)\\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right)+(1-\\eta(x))\\left(\\mathbf 1_{g^\\star(x)=0}-\\mathbf 1_{g(x)=0}\\right) \\\\\n  &  = \\eta(x)\\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right)+(1-\\eta(x))\\left(1-\\mathbf 1_{g^\\star(x)=1}-(1-\\mathbf 1_{g(x)=1})\\right) \\\\\n  &  = (2\\eta(x)-1) \\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right) \\\\\n\\end{aligned}\n\\]\n\n\nQuand le premier terme \\((2\\eta(x)-1)\\) est positif cela signifie que \\(\\eta(x)=\\mathbf P(Y|=1X=x)\\geq 0.5\\). Par définition de \\(g^\\star\\) nous en déduisons donc que \\(g^\\star(x)=1\\) et donc que le second terme est aussi positif ou nul.\nQuand le premier terme \\((2\\eta(x)-1)\\) est négatif cela signifie que \\(\\eta(x)=\\mathbf P(Y|=1X=x)&lt; 0.5\\). Par définition de \\(g^\\star\\) nous en déduisons donc que \\(g^\\star(x)=0\\) et donc que le second terme est aussi négatif ou nul.\nEn conclusion nous avons bien que \\[%\n\\mathbf P(g(X)\\neq Y|X=x)-\\mathbf P(g^\\star(X)\\neq Y|X=x) = (2\\eta(x)-1) \\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right) \\geq 0\n\\]\n\n\nOn conclut en intégrant par la loi de \\(X\\) que \\[%\n\\mathbf P(g(X)\\neq Y)\\geq \\mathbf P(g^\\star(X)\\neq Y).\n\\]\n\nPour la règle de Bayes, on a\n\nsi \\(x\\leq 0\\), on a \\(\\mathbf P(Y=1|X=x)=\\mathbf P(U\\leq 2)=\\frac{1}{5}\\) \\(\\Longrightarrow\\) \\(g^\\star(x)=0\\).\nsi \\(x&gt; 0\\), on a \\(\\mathbf P(Y=1|X=x)=\\mathbf P(U&gt; 2)=\\frac{9}{10}\\) \\(\\Longrightarrow\\) \\(g^\\star(x)=1\\).\n\nL’erreur de Bayes vaut \\[%\n\\begin{aligned}\n  L^\\star=\\mathbf P(g^\\star(X)\\neq Y&  |X\\leq 0)\\mathbf P(X\\leq 0) \\\\\n  &  +\\mathbf P(g^\\star(X)\\neq Y|X&gt; 0)\\mathbf P(X&gt;0).\n\\end{aligned}\n\\] Or \\[%\n\\mathbf P(g^\\star(X)\\neq Y|X\\leq 0)=\\mathbf P(Y\\neq 0|X\\leq 0)=\\frac{1}{5}\n\\] et \\[%\n\\mathbf P(g^\\star(X)\\neq Y|X&gt; 0)=\\mathbf P(Y\\neq 1|X&gt; 0)=\\frac{1}{10}.\n\\] On obtient \\[%L^\\star=\\frac{1}{5}\\,\\frac{1}{2}+\\frac{1}{10}\\,\\frac{1}{2}=\\frac{3}{20}.\\]\n\n\n\nExercice 3 (Fonctions R)  \n\n\nExercice 4 (Score aléatoire) Il suffit d’utiliser l’indépendance entre \\(S(X)\\) et \\(Y\\) : \\[\n\\begin{aligned}\nx(s)= & \\mathbf P(S(X)&gt; s|Y=0)=\\mathbf P(S(X)&gt; s)=\\mathbf P(S(X)\\geq s) \\\\\n=& \\mathbf P(S(X)\\geq s|Y=1)=y(s),\n\\end{aligned}\n\\]\n\n\nExercice 5 (Score parfait et courbe ROC) Nous prendrons un score variant entre \\(-\\infty\\) et \\(+\\infty\\). Si le score est issu d’une probabilité il variera entre \\(0\\) et \\(1\\) et il suffira de remplacer \\(-\\infty\\) par 0 et \\(+\\infty\\) par 1 dans le calculs qui suivent. Le score est parfait et nous avons donc un seuil \\(s^\\star\\) tel que - si \\(S(x)\\leq s^\\star\\) nous avons forcément \\(Y=0\\) et réciproquement. Autrement dit \\(\\mathbf P(Y=0|S(x)\\leq s^\\star)=1\\) et \\(\\mathbf P(S(X)\\leq s^\\star|Y=0)=1\\) - et aussi dans l’autre sens si \\(S(x)&gt; s^\\star\\) nous avons forcément \\(Y=1\\) et réciproquement. Autrement dit \\(\\mathbf P(Y=1|S(x)&gt; s^\\star)=1\\) et \\(\\mathbf P(S(X)&gt; s^\\star|Y=1)=1\\).\n\nAnalysons la courbe paramétrée en ce point: \\(s^\\star\\). On a ici pour l’abscisse \\[%\n\\begin{align*}\nx(s^\\star)= & \\mathbf P(S(X)&gt; s^\\star|Y=0) \\\\\n=&1 - \\mathbf P(S(X)\\geq s^\\star | Y=0)\\\\\n=& 1-1 =0.\n\\end{align*}\n\\]\nCalculons l’ordonnée: \\[%\n\\begin{aligned}\ny(s^\\star)=&\\mathbf P(S(X)\\geq s^\\star|Y=1)= 1-\\mathbf P(S(X)\\leq s^\\star|Y=1) \\\\\n=&1- \\frac{\\mathbf P(S(X)\\leq s^\\star\\cap Y=1)}{\\mathbf P(Y=1)}\\\\\n=&1- \\frac{\\mathbf P(Y=1| S(X)\\leq s^\\star)\\mathbf P(S(X)\\leq s^\\star) }{\\mathbf P(Y=1)} \\\\\n=&1- \\frac{(1-\\mathbf P(Y=0| S(X)\\leq s^\\star))\\mathbf P(S(X)\\leq s^\\star) }{\\mathbf P(Y=1)} \\\\\n=&1 -0=1\n\\end{aligned}\n\\] puisque \\(\\mathbf P(Y=0| S(X)\\leq s^\\star)=1.\\)\nConlusion: en \\(s^\\star\\) la courbe est au point \\(M=(1,0)\\).\nPlaçons nous en une valeur \\(s&gt;s^\\star.\\) Puisque \\(s&gt;s^\\star\\) nous avons forcément que que quand \\(S(X)&gt;s\\) on a \\(Y=1\\) et donc \\(\\mathbf P(Y=1| S(X)&gt; s)=0.\\) \\[%\n\\begin{aligned}\nx(s)= & \\mathbf P(S(X)&gt; s|Y=0) \\\\\n=&\\frac{\\mathbf P(S(X)&gt; s\\cap Y=0)}{\\mathbf P(Y=0)}\\\\\n=& \\frac{\\mathbf P(Y=0| S(X)&gt; s)\\mathbf P(S(X)&gt; s) }{\\mathbf P(Y=0)} \\\\\n=& \\frac{(1-\\mathbf P(Y=1| S(X)&gt; s))\\mathbf P(S(X)&gt; s) }{\\mathbf P(Y=0)}=0\n\\end{aligned}\n\\] Pour l’ordonnée nous avons que \\[%\n\\begin{aligned}\ny(s)=&\\mathbf P(S(X)\\geq s|Y=1)= 1-\\mathbf P(S(X)\\leq s|Y=1)\n\\end{aligned}\n\\] Comme \\(prob(S(X)\\leq s|Y=1)\\) est une fonction de répartition nous avens donc que \\(\\lim_{s\\rightarrow+\\infty}prob(S(X)\\leq s|Y=1)=1\\) et donc que \\[%\n\\begin{aligned}\n\\lim_{s\\rightarrow+\\infty}y(s)=&0\n\\end{aligned}\n\\] Conclusion: pour \\(s\\geq s^\\star\\) l’abscisse vaut toujours 0 et l’ordonnée tend vers 0. Donc quand \\(s\\in[s^\\star, +\\infty[\\) le point \\((x(s),y(s))\\) parcourt le segment \\([O,M]\\).\nPlaçons nous dans le cas où \\(s&lt;s^\\star\\). Dans ce cas nous avons que l’évènement \\(\\{S(X)\\geq s^\\star|Y=1\\} \\subset\\{S(X)\\geq s|Y=1\\}\\) et donc \\[%\n\\begin{aligned}\ny(s)=&\\mathbf P(S(X)\\geq s|Y=1)\\geq  \\mathbf P(S(X)\\geq s^\\star|Y=1)=1\n\\end{aligned}\n\\] De plus nous avons que \\[%\n\\begin{aligned}\nx(s)=&\\mathbf P(S(X)&gt; s|Y=0)= 1-\\mathbf P(S(X)\\leq s|Y=0)\n\\end{aligned}\n\\]\nComme \\(prob(S(X)\\leq s|Y=0)\\) est une fonction de répartition nous avens donc que \\(\\lim_{s\\rightarrow-\\infty}prob(S(X)\\leq s|Y=0)=0\\) et donc que \\[%\n\\begin{aligned}\n\\lim_{s\\rightarrow-\\infty}x(s)=&1\n\\end{aligned}\n\\] Conclusion: pour \\(s\\leq s^\\star\\) l’ordonnée vaut toujours 1 et l’absicsse tend vers 0. Donc quand \\(s\\in]- \\infty , s^\\star]\\) le point \\((x(s),y(s))\\) parcourt le segment \\([M, (1,1)]\\).\n\n\n\nExercice 6 (Score parfait et courbe lift) Nous prendrons un score variant entre \\(-\\infty\\) et \\(+\\infty\\). Si le score est issu d’une probabilité il variera entre \\(0\\) et \\(1\\) et il suffira de remplacer \\(-\\infty\\) par 0 et \\(+\\infty\\) par 1 dans le calculs qui suivent. Le score est parfait et nous avons donc un seuil \\(s^\\star\\) tel que - si \\(S(x)\\leq s^\\star\\) nous avons forcément \\(Y=0\\) ou autrement dit \\(\\mathbf P(Y=0|S(x)\\leq s^\\star)=1\\); - et aussi dans l’autre sens si \\(S(x)&gt; s^\\star\\) nous avons forcément \\(Y=1\\) ou autrement dit \\(\\mathbf P(Y=1|S(x)&gt; s^\\star)=1\\).\n\nAnalysons la courbe paramétrée en ce point: \\(s^\\star\\). On a ici pour l’abscisse \\[%\n\\begin{align*}\nx(s^\\star)= & \\mathbf P(S(X)&gt; s^\\star) = \\mathbf P(S(X)&gt; s^\\star \\cap Y=0) +  \\mathbf P(S(X)&gt; s^\\star \\cap    Y=1)\\\\\n= & \\mathbf P(S(X)&gt; s^\\star | Y=0)\\mathbf P(Y=0) + \\mathbf P(S(X)&gt; s^\\star | Y=1)\\mathbf P(Y=1)\n\\end{align*}\n\\] Comme le score est parfait nous avons que \\(\\mathbf P(S(X)&gt; s^\\star | Y=0)=0\\) et \\(\\mathbf P(S(X)&gt; s^\\star | Y=1)=1\\) d’où la conclusion \\[%\n\\begin{aligned}\nx(s^\\star)= \\mathbf P(Y=1)\n\\end{aligned}\n\\]\nCalculons maintenant l’ordonnée: \\[%\n\\begin{aligned}\ny(s^\\star)=&\\mathbf P(S(X)\\geq s^\\star |Y=1)=1\n\\end{aligned}\n\\] puisque le score est parfait.$\nConlusion: en \\(s^\\star\\) la courbe est au point \\(M=(\\mathbf P(Y=1),1)\\).\nPlaçons nous en un point \\(s\\geq s^\\star\\). L’abscisse vaut \\[%\n\\begin{aligned}\nx(s)= & \\mathbf P(S(X)&gt; s)\n\\end{aligned}\n\\] et l’ordonnée \\[%\n\\begin{aligned}\ny(s)=&\\mathbf P(S(X)\\geq s |Y=1) = \\frac{\\mathbf P(S(X)\\geq s \\cap Y=1)}{\\mathbf P(Y=1)}\n\\end{aligned}\n\\] Comme le score est parfait nous avons en \\(s\\geq s^\\star\\) \\[%\n\\begin{aligned}\n\\mathbf P( Y=1|S(X)\\geq s) &= 1\\\\\n\\mathbf P( Y=0|S(X)\\geq s) &= 0\\\\\n\\frac{\\mathbf P(S(X)\\geq s \\cap Y=0)}{\\mathbf P(S(X)\\geq s )}&=0\\\\\n\\mathbf P(S(X)\\geq s \\cap Y=0)&=0\n\\end{aligned}\n\\] nous pouvons donc ajouter \\(\\mathbf P(S(X)\\geq s \\cap Y=0)\\) et obtenir \\[%\n\\begin{aligned}\ny(s)=& \\frac{\\mathbf P(S(X)\\geq s \\cap Y=1) + \\mathbf P(S(X)\\geq s \\cap Y=0)}{\\mathbf P(Y=1)}= \\frac{\\mathbf P(S(X)\\geq s)}{\\mathbf P(Y=1)}\\\\\n&=\\frac{x(s)}{\\mathbf P(Y=1)}\n\\end{aligned}\n\\] On en déduit donc que le coefficient directeur de la droite est \\(1/\\mathbf P(Y=1)\\) et qu’elle passe par le point \\((x(s^\\star), y(s^\\star))\\). Trouvons maintenant le point quand \\(s\\rightarrow+\\infty\\). Nous savons que \\(x(s)=\\mathbf P(S(X)&gt; s)=1-\\mathbf P(S(X)\\geq s)\\) et que \\(prob(S(X)\\geq s)\\) est une fonction de répartition, nous avons donc que \\(\\mathbf P(S(X)\\geq s)\\rightarrow+1\\) et donc que \\(x(s)\\rightarrow 0\\). Comme \\(y(s)=x(s)/\\mathbf P(Y=1)\\) on en déduit que \\(y(s)\\rightarrow 0\\). Conlusion: entre \\(s^\\star\\) et \\(+\\infty\\) le point \\((x(s), y(s))\\) parcours le segment \\((x(s^\\star), y(s^\\star))\\) \\((0,0)\\).\n\nQuand \\(s= s^\\star\\) le score parfait on a \\[%\n\\begin{aligned}\ny(s^\\star)=&\\mathbf P(S(X)\\geq s^\\star |Y=1)=1\n\\end{aligned}\n\\] Si nous avons \\(s\\leq s^\\star\\) l’évènement le score est plus grand que \\(s\\) contient forcément l’évènement le score est plus grand que \\(s^\\star\\) et donc en en déduit que \\[%\n\\begin{aligned}\ny(s)=&\\mathbf P(S(X)\\geq s|Y=1)\\geq \\mathbf P(S(X)\\geq s^\\star|Y=1)=1\n\\end{aligned}\n\\] Donc puisqu’il s’agit d’une probabilité nous avons \\(y(s)=1 \\ \\forall s\\leq s^\\star\\). Par ailleurs l’abscisse est par définition \\[%\n\\begin{aligned}\nx(s)= & \\mathbf P(S(X)&gt; s) = 1 - \\mathbf P(S(X)\\leq s)\n\\end{aligned}\n\\] et quand \\(s\\rightarrow-\\infty\\) la fonction de répartition tend vers 0 et donc \\(x(s)\\rightarrow 1\\)\n\n\n\n\nExercice 7 (Calculs de scores avec R)  \n\nOn propose d’utiliser la version empirique de \\[\nAUC(S)=\\mathbf P(S(X_1)\\geq S(X_2)|(Y_1,Y_2)=(1,-1)),\n\\] c’est-à-dire d’estimer l’AUC en calculant, parmi les paires qui vérifient \\((Y_1,Y_2)=(1,-1)\\), la proportion de paires qui vérifient \\(S(X_1)\\lt S(X_2)\\). Si on note \\[\n\\mathcal I=\\{(i,j),y_i=1,y_j=0\\},\n\\] alors l’estimateur s’écrit \\[\n\\widehat{AUC}(S)=\\frac{1}{|\\mathcal I|}\\sum_{(i,j)\\in\\mathcal I}\\mathbf 1_{S(X_i)&gt;S(S_j)}.\n\\]\nOn importe les données\n\ndf &lt;- read.csv(\"../donnees/logit_ex6.csv\")\n\n\n\n\n\nset.seed(1234)\nind_app &lt;- sample(nrow(df),300)\ndapp &lt;- df[ind_app,]\ndtest &lt;- df[-ind_app,]\n\n\n\n\n\nlogit &lt;- glm(Y~.,data=dapp,family=\"binomial\")\n\n\n\n\n\nscore &lt;- predict(logit,newdata=dtest,type=\"response\")\n\n\n\n\n\nD0 &lt;- which(dtest$Y==0)\nD1 &lt;- which(dtest$Y==1)\nscore0 &lt;- score[D0]\nscore1 &lt;- score[D1]\nscore.01 &lt;- expand.grid(S0=score0,S1=score1)\nmean(score.01$S1&gt;score.01$S0)\n\n[1] 0.7385817\n\n\n\n\n\n\npROC::roc(dtest$Y,score)\n\n\nCall:\nroc.default(response = dtest$Y, predictor = score)\n\nData: score in 104 controls (dtest$Y 0) &lt; 96 cases (dtest$Y 1).\nArea under the curve: 0.7386\n\n\n\n\n\nExercice 8 (Validation croisée)  \n\nImportons les données de détection d’images publicitaires sur internet\n\npub &lt;- read.table(\"../donnees/ad_data.txt\", header = FALSE, sep = \",\",\ndec = \".\", na.strings = \"?\", strip.white = TRUE)\nnames(pub)[ncol(pub)] &lt;- \"Y\"\ntable(pub$Y)\n\n\n   ad. nonad. \n   459   2820 \n\n\nCréons un facteur pour la variable à expliquer en gérant l’ordre des facteurs.\n\npub$Y &lt;- factor(pub$Y, labels=c(\"1\",\"0\"))\npub$Y &lt;- relevel(pub$Y, \"0\")\npub &lt;- pub[,-(1:4)]\ntable(pub$Y)\n\n\n   0    1 \n2820  459 \n\n\nConstruisons nos blocs.\n\nOn crée une variable discrète bloc qui donnera les appartenances au bloc (donc des valeurs entre 1 et 10)\nPour chaque niveau de \\(Y\\), on crée une variable d’appartenance au bloc valant 1, 2, …, 10, 1, 2, …, 10, 1…. Il y a donc par construction autant (ou un de moins) de 1 que de 2, …, que de 10 pour chaque niveau de \\(Y\\). L’appartenance au bloc étant séquentielle ce n’est pas trop souhaitable\nOn permute aléatoirement cette variable pour chaque niveau et on affecte ces valeurs aux coordonnées de chaque niveau de \\(Y\\)\n\n\nind0 &lt;- which(pub$Y==0)\nind1 &lt;- which(pub$Y==1)\nset.seed(1234)\nnbbloc &lt;- 10\nbloc &lt;- 1:nrow(pub)\nbloc[ind0] &lt;- sample(rep(1:nbbloc, length = length(ind0)))\nbloc[ind1] &lt;- sample(rep(1:nbbloc, length = length(ind1)))\n\nProbabilité estimée d’une publicité Nous ajustons/estimons chaque modèle sur tous les blocs sauf le \\(i^e\\) et nous calculons les probabilités estimées du label 1 sur le bloc \\(i\\). Ceci est fait pour tous les blocs. Nous rangeons les probabilités estimées dans l’objet SCORE:\n\nlibrary(glmnet)\nnomsmodeles &lt;- c(\"glm\", \"glmchoix\", paste0(rep(c(\"ridge\", \"lasso\", \"elast\"), 3), rep(c(\"\", \"C\", \"A\"), each=3)))\n## resultats des modeles + Y a prevoir\nSCORE &lt;- data.frame(matrix(0, length(bloc), length(nomsmodeles)+1))\nnames(SCORE) &lt;- c(\"Y\",  nomsmodeles)\n## premiere colonne la var Y a prevoir\nSCORE$Y &lt;- pub$Y\nmodels &lt;- list() # si on veut voir les variables choisies\n## VC 10 fold\nfor(i in 1:nbbloc){\n  print(i)\n  pubA &lt;- pub[bloc!=i,]\n  pubT &lt;- pub[bloc==i,]\n  ##Logistique global\n  reglog &lt;- glm(Y~.,data=pubA,family=\"binomial\")\n  SCORE[bloc==i,\"glm\"] &lt;- predict(reglog,pubT,type=\"response\")\n  ##Logistique choix \n  reg1 &lt;- glm(Y~1,data=pubA,family=\"binomial\")\n  choix &lt;- step(reg1,scope=list(lower=formula(reg1),upper=formula(reglog)),trace=0)\n  models[[i]] &lt;- formula(choix)\n  SCORE[bloc==i,\"glmchoix\"] &lt;- predict(choix,pubT,type=\"response\")\n  ##\n  XA &lt;- pub.X[bloc!=i,]\n  YA &lt;- pub.Y[bloc!=i]\n  XT &lt;- pub.X[bloc==i,]\n  ## ridge\n  mod &lt;- cv.glmnet(XA,YA,alpha=0,family=\"binomial\")\n  SCORE[bloc==i,\"ridge\"] &lt;- as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ##  lasso\n  mod &lt;- cv.glmnet(XA,YA,alpha=1,family=\"binomial\")\n  SCORE[bloc==i,\"lasso\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))\n  ##  elast\n  mod &lt;- cv.glmnet(XA,YA,alpha=.5,family=\"binomial\")\n  SCORE[bloc==i,\"elast\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))  \n  ## ridge  CV class\n  mod &lt;- cv.glmnet(XA,YA,alpha=0,family=\"binomial\",type.measure=\"class\")\n  SCORE[bloc==i,\"ridgeC\"] &lt;- as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ## lasso CV class\n  mod &lt;- cv.glmnet(XA,YA,alpha=1,family=\"binomial\",type.measure=\"class\")\n  SCORE[bloc==i,\"lassoC\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))  \n  ## elast CV class\n  mod &lt;- cv.glmnet(XA,YA,alpha=.5,family=\"binomial\",type.measure=\"class\")\n  SCORE[bloc==i,\"elastC\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))\n  ## ridge  CV AUC\n  mod &lt;- cv.glmnet(XA,YA,alpha=0,family=\"binomial\",type.measure=\"auc\")\n  SCORE[bloc==i,\"ridgeA\"] &lt;- as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ## lasso CV AUC\n  mod &lt;- cv.glmnet(XA,YA,alpha=1,family=\"binomial\",type.measure=\"auc\")\n  SCORE[bloc==i,\"lassoA\"] &lt;-  as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ## elast CV AUC\n  mod &lt;- cv.glmnet(XA,YA,alpha=.5,family=\"binomial\",type.measure=\"auc\")\n  SCORE[bloc==i,\"elastA\"] &lt;-  as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n}\nsaveRDS(SCORE,\"../data_aux/scorepubm4_1VC.RDS\")\n\nOn calcule la courbe ROC On peut utiliser pROC et créer l’objet de classe roc puis sur cet objet calculer l’AUC pour chaque modèle:\n\nlibrary(pROC)\nSCORE &lt;- readRDS(\"../data_aux/scorepubm4_1VC.RDS\")\nrocCV &lt;- roc(Y~.,data=SCORE)\nsort(round(unlist(lapply(rocCV, auc)), 3), decreasing=TRUE)[1:6]\n\n ridge ridgeC ridgeA elastA lassoA elastC \n 0.982  0.982  0.963  0.963  0.961  0.959 \n\n\nPour aller plus loin on peut répéter la même procédure 20 fois (en parallèle)\n\ndonnées et cluster\n\n\n library(glmnet)\n library(doParallel)\n pub &lt;- read.table(\"ad_data.txt\",header=FALSE,sep=\",\",dec=\".\",na.strings = \"?\",strip.white = TRUE)\n names(pub)[ncol(pub)] &lt;- \"Y\"\n pub$Y &lt;- factor(pub$Y,labels=c(\"1\",\"0\"))\n pub$Y &lt;- relevel(pub$Y,\"0\")\n pub &lt;- pub[,-(1:4)]\n ind0 &lt;- which(pub$Y==0)\n ind1 &lt;- which(pub$Y==1)\n ##les 0 sont au début les 1 à la fin\n ##\n bloc &lt;- rep(0,nrow(pub))\n pub.X &lt;- model.matrix(Y~.,data=pub)[,-1]\n pub.Y &lt;- pub[,\"Y\"]\n niter &lt;- 20\n coredispo &lt;- max(detectCores()-1,1)\n cl &lt;- makeCluster(min(coredispo,niter))\n clusterSetRNGStream(cl,iseed=78)\n registerDoParallel(cl) \n\n\n\nboucle Attention, c’est «un peu» long (sur une machine avec plus de 20 coeurs, il faut 1-2 jours)\n\nres &lt;- foreach(j=1:niter) %dopar% {\nlibrary(glmnet)\nnomsmodeles &lt;- c(\"glm\", \"glmchoix\", paste0(rep(c(\"ridge\", \"lasso\", \"elast\"), 3), rep(c(\"\", \"C\", \"A\"), each=3)))\n## resultats des modeles + Y a prevoir\nSCORE &lt;- data.frame(matrix(0, length(bloc), length(nomsmodeles)+1))\nnames(SCORE) &lt;- c(\"Y\",  nomsmodeles)\n## premiere colonne la var Y a prevoir\nSCORE &lt;- data.frame(Y=pub$Y)\nnbbloc &lt;- 10\nset.seed(1234+j)\nbloc[ind0] &lt;- sample(rep(1:nbbloc, length = length(ind0)))\nbloc[ind1] &lt;- sample(rep(1:nbbloc, length = length(ind1)))\nfor(i in 1:nbbloc){\n  pubA &lt;- pub[bloc!=i,]\n  pubT &lt;- pub[bloc==i,]\n  ##Logistique global\n  reglog &lt;- glm(Y~.,data=pubA,family=\"binomial\")\n  SCORE[bloc==i,\"glm\"] &lt;- predict(reglog,pubT,type=\"response\")\n  ##Logistique choix \n  reg1 &lt;- glm(Y~1,data=pubA,family=\"binomial\")\n  choix &lt;- step(reg1,scope=list(lower=formula(reg1),upper=formula(reglog)),trace=0)\n  SCORE[bloc==i,\"glmchoix\"] &lt;- predict(choix,pubT,type=\"response\")\n  ##\n  XA &lt;- pub.X[bloc!=i,]\n  YA &lt;- pub.Y[bloc!=i]\n  XT &lt;- pub.X[bloc==i,]\n  ## ridge\n  mod &lt;- cv.glmnet(XA,YA,alpha=0,family=\"binomial\")\n  SCORE[bloc==i,\"ridge\"] &lt;- as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ##  lasso\n  mod &lt;- cv.glmnet(XA,YA,alpha=1,family=\"binomial\")\n  SCORE[bloc==i,\"lasso\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))\n  ##  elast\n  mod &lt;- cv.glmnet(XA,YA,alpha=.5,family=\"binomial\")\n  SCORE[bloc==i,\"elast\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))  \n  ## ridge  CV class\n  mod &lt;- cv.glmnet(XA,YA,alpha=0,family=\"binomial\",type.measure=\"class\")\n  SCORE[bloc==i,\"ridgeC\"] &lt;- as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ## lasso CV class\n  mod &lt;- cv.glmnet(XA,YA,alpha=1,family=\"binomial\",type.measure=\"class\")\n  SCORE[bloc==i,\"lassoC\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))  \n  ## elast CV class\n  mod &lt;- cv.glmnet(XA,YA,alpha=.5,family=\"binomial\",type.measure=\"class\")\n  SCORE[bloc==i,\"elastC\"] &lt;-  as.vector(predict(mod,XT, \"lambda.1se\", type=\"response\"))\n  ## ridge  CV AUC\n  mod &lt;- cv.glmnet(XA,YA,alpha=0,family=\"binomial\",type.measure=\"auc\")\n  SCORE[bloc==i,\"ridgeA\"] &lt;- as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ## lasso CV AUC\n  mod &lt;- cv.glmnet(XA,YA,alpha=1,family=\"binomial\",type.measure=\"auc\")\n  SCORE[bloc==i,\"lassoA\"] &lt;-  as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  ## elast CV AUC\n  mod &lt;- cv.glmnet(XA,YA,alpha=.5,family=\"binomial\",type.measure=\"auc\")\n  SCORE[bloc==i,\"elastA\"] &lt;-  as.vector(predict(mod, XT, \"lambda.1se\", type=\"response\"))\n  write(paste(\"rep.\",j,\"bloc\",j),file=\"progress.txt\",append=TRUE)\n}\nSCORE\n}\nsaveRDS(res,\"../data_aux/scorepubm4_20VC.RDS\")\nstopCluster(cl)\n\nanalyse résultats\n\nffroc  &lt;- function(x) roc(Y~., data=x)\nttm4_20 &lt;- lapply(scorepubm4_20VC, ffroc)\n\nffauc  &lt;- function(x) unlist(lapply(x, auc))\naucm4_20 &lt;- do.call(rbind, lapply(ttm4_20, ffauc))\n\nprint(sort(apply(aucm4_20,2,mean), decreasing=TRUE)[1:6])\n\n    ridge    ridgeC    elastA    ridgeA    lassoA     elast \n0.9800319 0.9792532 0.9631016 0.9626010 0.9590724 0.9537869 \n\nnom &lt;- names(sort(apply(aucm4_20,2,mean), decreasing=TRUE)[1:6])\n## boxplot\nboxplot(aucm4_20[,-1][,c(1,2,5,8,3,6,9,4,7,10)])\n\n\n\n\n\n\n\n## export \nboxplot(aucm4_20[,-1][,c(1,2,5,8,3,6,9,4,7,10)])",
    "crumbs": [
      "Correction des exercices",
      "IV Le modèle linéairé généralisé",
      "14 Comparaison en classification supervisée"
    ]
  },
  {
    "objectID": "correction/chap12.html",
    "href": "correction/chap12.html",
    "title": "12 Régression de Poisson",
    "section": "",
    "text": "Exercice 1 (Questions de cours) C, A, B, A, B, B, C, A\n\n\nExercice 2  \n\nLe modèle linéaire gaussien s’écrit \\[%\n\\begin{align*}\n  Y_i=X_{i.}'\\beta + \\varepsilon_i, \\ \\ \\ \\varepsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\n\\end{align*}\n\\] c’est-à-dire \\[%\n\\begin{align*}\n  Y_i\\sim\\mathcal{N}(X_{i.}'\\beta,\\sigma^2)\n\\end{align*}\n\\] La densité en \\(y\\) est \\[%\n\\begin{align*}\nf(y)&=\\frac{1}{\\sqrt{2\\pi}}\\exp(\\frac{-(y-x'\\beta)^2}{2\\sigma^2})\n=\\exp(\\frac{(y (x'\\beta) - (x'\\beta)^2/2)}{\\sigma^2} - \\frac{1}{2}(\\frac{y^2}{\\sigma^2} + \\log(2\\pi\\sigma^2) ))\n\\end{align*}\n\\] On a donc \\[%\n\\begin{align*}\n  \\alpha&=x'\\beta\\\\\n  b(\\alpha)&=\\alpha^2/2\\\\\n  \\phi&=\\sigma^2\\\\\n  a(\\phi)&=\\sigma^2\\\\\n  c(y, \\phi)&=- \\frac{1}{2}(\\frac{y^2}{\\phi} + \\log(2\\pi\\phi) ).\n\\end{align*}\n\\] La fonction de lien canonique est par définition \\[%\n\\begin{align*}\n  g(u)=(b')^{-1}(u)\n  b'(\\alpha)=(\\alpha^2/2)'=\\alpha\n\\end{align*}\n\\] L’inverse de \\(b'(.): \\alpha\\mapsto u=b'(\\alpha)=\\alpha\\) est donc \\(b'(.)^{-1}: u\\mapsto \\alpha=u\\), c’est le lien identité.\nLe modèle logistique avec répétition en \\(\\tilde x_t\\) (voir paragraphe 11.4.1, p. 270) s’écrit \\[%\n\\begin{align}\n  \\tilde Y_t=\\tilde y_t | X=\\tilde x_t \\sim \\mathcal{B}(n_t, p_\\beta(x_t))\n\\end{align}\n\\] où \\(p_\\beta(x_t)=\\text{logit}(x_t\\beta).\\) La probabilité que \\(\\tilde Y_t\\) vaille \\(\\tilde y_t\\) ou densité par rapport à la mesure de comptage au point \\(\\tilde y_t\\) en s’écrit \\[%\n\\begin{align*}\n  f(\\tilde y_t)= P(\\tilde Y_t=\\tilde y_t)=\\binom{n_t}{ \\tilde y_t} p_\\beta(x_t)^{\\tilde y_t} (1-p_\\beta(x_t))^{\\tilde n_t-\\tilde y_t}\n\\end{align*}\n\\] Si il n’y a pas de répétitions en \\(\\tilde x_t\\) on a \\(n_t=1\\) et \\(\\tilde y_t=0\\) ou \\(1\\).\nEn modèle GLM on s’intéresse non pas au comptage \\(\\tilde Y_t\\) mais à la moyenne \\(\\bar Y_t=\\tilde Y_t/n_t\\), nous avons donc \\[%\n\\begin{align*}\n  f(\\bar y_t)= P(\\bar Y_t=\\bar y_t)=\\binom{n_t}{n_t \\bar y_t} p_\\beta(x_t)^{n_t\\bar y_t} (1-p_\\beta(x_t))^{n_t-n_t\\bar y_t}\n\\end{align*}\n\\] En introduisant l’exponentielle: \\[%\n\\begin{align*}\n  f(\\bar  y_t) &= \\exp\\biggl(\\log\\binom{n_t}{n_t \\bar y_t}\\biggr)\n\\exp\\biggl(\\log \\bigl(p_\\beta(x_t)^{n_t\\bar y_t}\\bigr)\\biggr) \\exp\\biggl(\\log\\bigl( (1-p_\\beta(x_t))^{n_t-n_t\\bar y_t}\\bigr)\\biggr)\\\\\n&=  \\exp\\biggl(n_t\\bar y_t\\log p_\\beta(x_t) + (n_t-n_t\\bar y_t )\\log (1-p_\\beta(x_t)) + \\log\\binom{n_t}{n_t \\bar y_t}\\biggr)\\\\\n&=\\exp\\biggl(n_t\\bar y_t(\\log p_\\beta(x_t)-\\log (1-p_\\beta(x_t))) - n_t\\log (1-p_\\beta(x_t)) + \\log\\binom{n_t}{n_t \\bar y_t}\\biggr)\\\\\n&=\\exp\\biggl(n_t\\bigl(\\bar y_t\\log\\frac{ p_\\beta(x_t)}{1-p_\\beta(x_t)} - \\log (1-p_\\beta(x_t))\\bigr) + \\log\\binom{n_t}{n_t \\bar y_t}\\biggr)\n\\end{align*}\n\\] Ici nous avons \\(\\alpha=\\) ou \\[%\n\\begin{align*}\n  \\alpha&=\\log\\frac{ p_\\beta(x_t)}{1-p_\\beta(x_t)}\\\\\n  b(\\alpha)&=\\log(1+\\exp\\alpha)=\\log (1-p_\\beta(x_t))\\\\\n  \\phi&=1/n_t\\\\\n  a(\\phi)&=\\phi\\\\\n  c(\\bar y_t, \\phi)&=\\log\\binom{1/\\phi}{\\bar y_t/\\phi}.\n\\end{align*}\n\\] Pour calculer la fonction de lien canonique nous devons dériver \\(b(.)\\) et inverser la fonction \\[%\n\\begin{align*}\n  b'(\\alpha)=\\log(1+\\exp \\alpha)'=\\frac{\\exp(\\alpha)}{1+\\exp \\alpha}=u\\\\\n  (b')^{-1}(u)=\\log\\frac{u}{1-u}=\\alpha\n\\end{align*}\n\\] c’est le lien logistique.\nLe modèle de poisson s’écrit \\[%\n\\begin{align*}\n  Y_i=y_i | X=x_i \\sim \\mathcal{P}(\\lambda_\\beta(x_i))\n\\end{align*}\n\\] La probabilité que \\(Y_i\\) vaille $y_i ou densité par rapport à la mesure de comptage au point \\(y_i\\) en s’écrit \\[%\n\\begin{align*}\n  f(y_i)&= P(\\tilde Y_i=\\tilde y_i)=\\exp(-\\lambda_\\beta(x_i)) \\frac{\\lambda_\\beta(x_i)^{y_i}}{y_i!}\\\\\n  &=\\exp\\bigl(-\\lambda_\\beta(x_i) + \\log (\\lambda_\\beta(x_i)^{y_i}) - \\log (y_i!)\\bigr)\\\\\n  &=\\exp\\bigl(y_i\\log (\\lambda_\\beta(x_i)) - \\lambda_\\beta(x_i) - \\log (y_i!)\\bigr)    \n\\end{align*}\n\\] Ici nous avons \\[%\n\\begin{align*}\n  \\alpha&=\\log (\\lambda_\\beta(x_i))\\\\\n  b(\\alpha)&=\\exp(\\alpha)=\\lambda_\\beta(x_i)\\\\\n  \\phi&=1\\\\\n  a(\\phi)&=1\\\\\n  c(\\bar y_t, \\phi)&=- \\log (y_i!).\n\\end{align*}\n\\] Pour calculer la fonction de lien canonique nous devons dériver \\(b(.)\\) et inverser la fonction \\[%\n\\begin{align*}\n  b'(\\alpha)=\\exp(\\alpha)=u\\\\\n  (b')^{-1}(u)=\\log u=\\alpha\n\\end{align*}\n\\] c’est le lien log.\n\n\n\nExercice 3  \n\nLes moments factoriels d’ordre \\(r\\) d’une variable aléatoire suivant une loi de Poisson valent: \\[%\n\\begin{align*}\n  \\mathbf E[X(X-1)\\dots(X-r+1)] &= \\sum_{k=0}^\\infty k(k-1)\\dots(k-r+1)\n  \\frac{\\lambda^k}{k!} e^{-\\lambda}\\\\\n  &=  e^{-\\lambda} \\sum_{k=r}^\\infty \\frac{k(k-1)\\dots(k-r+1)}{k!} \\lambda^k \\\\\n  &=  e^{-\\lambda} \\lambda^r \\sum_{k=r}^\\infty \\frac{1}{(k-r)!} \\lambda^{k-r}\\\\\n  &= \\lambda^r.\n\\end{align*}\n\\]\n\nPour \\(r=1\\) nous avons donc \\[%\n\\begin{align*}\n  \\mathbf E(X) = \\lambda\n\\end{align*}\n\\] Pour \\(r=2\\) nous avons donc \\[%\n\\begin{align*}\n  \\mathbf E(X(X-1)) &= \\mathbf E(X^2) - \\mathbf E(X) =\\lambda^2\\\\\n  \\mathbf E(X^2) &= \\lambda^2 + \\lambda\n\\end{align*}\n\\] Et nous avons la variance \\[%\n\\begin{align*}\n  \\mathop{\\mathrm{V}}(X) &= \\mathbf E(X^2) - \\mathbf E(X)^2 =\\lambda^2 + \\lambda - \\lambda^2 = \\lambda.\n\\end{align*}\n\\]\n\n\n\n\nExercice 4 (Stabilisation de la variance) Tirons notre échantillon de taille \\(n=10^6\\) selon des loi de poisson de paramètre \\(\\lambda\\in\\{1,2,\\dotsc, 20\\}\\)\n\nn &lt;- 1e7\nlambdas &lt;- 1:20\nvarX &lt;- rep(0, length(lambdas))\nvarZ &lt;- rep(0, length(lambdas))\nfor (l in 1:length(lambdas)){\n    ech &lt;- rpois(n, lambdas[l])\n    varX[l] &lt;- var(ech)\n    ech &lt;- sqrt(ech)\n    varZ[l] &lt;- var(ech)\n}\nprint(varX)\n\n [1]  0.9997369  1.9997818  2.9984626  4.0031435  4.9959479  5.9992567\n [7]  7.0043691  7.9954055  8.9991719  9.9990725 10.9885709 12.0038720\n[13] 12.9821786 13.9998158 14.9955777 16.0014936 16.9962143 17.9969313\n[19] 19.0054412 19.9929461\n\nprint(varZ)\n\n [1] 0.4022116 0.3897837 0.3396863 0.3059119 0.2856383 0.2753928 0.2693276\n [8] 0.2652610 0.2629826 0.2613118 0.2598299 0.2590222 0.2578215 0.2575848\n[15] 0.2568114 0.2565663 0.2560164 0.2556442 0.2554479 0.2549726\n\n\n\n\nExercice 5 (Stabilisation de la variance) Comme \\(X_i\\stackrel{\\mathrm{iid}}{\\sim} \\mathcal{P}(\\lambda)\\) nous avons par le TCL que la moyenne d’un \\(n\\) échantillon \\(\\bar X_n\\) converge vers la loi normale et plus exactement \\(\\sqrt{n}(\\bar X_n - \\lambda) \\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(0, \\lambda)\\). Posons la fonction \\(g(.)\\) de classe \\(C^1\\) et utilisons un développement de Taylor autour de \\(\\lambda\\): \\[\n\\begin{align*}\n  g(x)&=g(\\lambda) + (x-\\lambda) g'(\\lambda) + o(x-\\lambda)\n\\end{align*}\n\\] Ici nous nous intéressons à stabiliser \\(\\bar X_n\\) et nous posons la transformation \\(g(\\bar X_n)\\) ce qui donne \\[\n\\begin{align*}\ng(\\bar X_n)&=g(\\lambda) + (\\bar X_n-\\lambda) g'(\\lambda) + o(\\bar X_n-\\lambda)\\\\\n\\sqrt{n}(g(\\bar X_n)- g(\\lambda))&= \\sqrt{n}(\\bar X_n-\\lambda) g'(\\lambda) + \\sqrt{n}o(\\bar X_n-\\lambda)\n\\end{align*}\n\\] Comme \\(\\sqrt{n}(\\bar X_n - \\lambda)\\) converge en distribution donc \\((\\bar X_n - \\lambda)=O_P(1/\\sqrt{n})\\) donc \\(\\sqrt{n}o(\\bar X_n-\\lambda)=o_P(1)\\) et donc \\[\n\\begin{align*}\n  \\sqrt{n}(g(\\bar X_n)- g(\\lambda))&= \\sqrt{n}(\\bar X_n-\\lambda) g'(\\lambda) + o_P(1)\n\\end{align*}\n\\] Si \\(g'(\\lambda)\\neq 0\\) on a \\[\n\\begin{align*}\n  \\sqrt{n}(g(\\bar X_n)- g(\\lambda))\\stackrel{\\mathcal{D}}{\\rightarrow} \\mathcal{N}(0, \\lambda g'(\\lambda)^2)\n\\end{align*}\n\\] Pour avoir la variance unité, on choisit \\(\\lambda g'(\\lambda)^2)=1\\) c’est à dire \\[\n\\begin{align*}\n  g'(\\lambda)&=\\frac{1}{\\sqrt{\\lambda}}\\\\\n  g(\\lambda)&=2\\sqrt{\\lambda}\n\\end{align*}\n\\] On peut donc prendre la transformée \\(g(X)=2\\sqrt{X}\\) (pour avoir la variance unité) ou \\(g(X)=\\sqrt{X}\\) et on aura une variance de \\(1/4\\).\n\n\nExercice 6  \n\nGraphique\n\nMalaria &lt;- read.csv(\"../donnees/poissonData3.csv\")\ntab &lt;- lapply( split( Malaria$N.malaria, Malaria$Sexe ), table )\nTab &lt;- matrix( 0,2, max( Malaria$N.malaria )+1 )\ncolnames(Tab) &lt;- 0:max( Malaria$N.malaria )\nTab[1, names(tab[[1]]) ] &lt;- tab[[1]]\nTab[2, names(tab[[2]]) ] &lt;- tab[[2]]\nbarplot( Tab, offset = -Tab[1,])\n\n\n\n\n\n\n\n\nLes barres sont superposées : les filles puis les garçons. Comme on soustrait à la hauteur totale les garçons (argument offset), on a les effectifs des filles en dessous et ceux des garçons au dessus.\nLes moyennes par groupe\n\naggregate(Malaria$N.malaria, list(Malaria$Sexe), mean)\n\n  Group.1        x\n1       F 4.579012\n2       M 4.794370\n\n\net la différence des logarithme népérien\n\ndiff(log(aggregate(Malaria$N.malaria, list(Malaria$Sexe), mean)[,\"x\"]))\n\n[1] 0.04595891\n\n\nUn autre code\n\nmm &lt;- sapply( split( Malaria$N.malaria, list(Malaria$Sexe) ),mean,na.rm=T)\nround( c( log( mm[1] ), diff( log( mm ) ) ), 5)\n\n      F       M \n1.52148 0.04596 \n\n\nRégression de Poisson\n\nmod &lt;- glm(N.malaria ~ 1 + Sexe, data=Malaria, family = poisson)\nmod\n\n\nCall:  glm(formula = N.malaria ~ 1 + Sexe, family = poisson, data = Malaria)\n\nCoefficients:\n(Intercept)        SexeM  \n    1.52148      0.04596  \n\nDegrees of Freedom: 1626 Total (i.e. Null);  1625 Residual\nNull Deviance:      5710 \nResidual Deviance: 5706     AIC: 10510\n\n\nNous retrouvons que le coefficient constant (Intercept) est le logarithme népérien de la moyenne du nombre de visites chez les filles. La modalité fille est la première modalité de la variable Sexe par ordre alphabétique et constitue la modalité de référence. Le coefficient constant est ici le logarithme (qui est la fonction de lien) de la moyenne du nombre de visites chez les filles. L’effet Age est ici la différence des logarithmes ce que nous retrouvons dans le second coefficient.\n\n\n\nExercice 7 (Table de contingence et loi de Poisson)  \n\nImportons les données\n\nMalaria &lt;- read.table(\"../donnees/poissonData3.csv\", sep=\",\", header=TRUE, stringsAsFactors=TRUE)\n\net le tableau de contingence est\n\ntable(Malaria$Sexe, Malaria$Prevention)\n\n\n    Autre Moustiquaire Rien Serpentin/Spray\n  F     2          557  223              28\n  M     6          543  233              35\n\n\nLien entre Sexe et Prévention\n\nchisq.test(Malaria$Sexe,Malaria$Prevention)\n\n\n    Pearson's Chi-squared test\n\ndata:  Malaria$Sexe and Malaria$Prevention\nX-squared = 3.1452, df = 3, p-value = 0.3698\n\n\nLa probabilité critique est de 0.3698 donc nous conservons \\({\\mathrm{H_0}}\\) il y a indépendance entre Sexe et Prévention.\nConstitution du data-frame\n\nY &lt;- as.vector(table(Malaria$Sexe, Malaria$Prevention))\neff &lt;- table(Malaria$Sexe, Malaria$Prevention)\nSexe &lt;- factor(rep(c(\"F\", \"M\"), 4))\nPrevention &lt;- factor(rep(levels(Malaria$Prevention), each=2))\ndon &lt;- data.frame(Y, Sexe, Prevention)\n\nModèle de Poisson avec interaction\n\nmod1 &lt;- glm( Y ~ -1 + Sexe:Prevention, data=don, family=poisson)\n\n\nIl n’y a pas de contraintes ici. On a \\(2\\times 4\\) niveaux d’interactions et 8 coefficients estimés. Il s’agit d’une situation analogue à une analyse de variance à 2 facteurs (à 2 et 4 niveaux) avec interaction et sans répétitions. Dans chaque niveau d’interaction \\(i\\times j\\) on observe un comptage \\(Y_{ij}\\) (l’effectif) qui est modélisé par un modèle de Poisson. Les comptages suivent une loi de Poisson de paramètre \\(\\lambda_{ij}\\) dont le log est fonction du niveau de l’interaction \\[\n\\begin{align*}\n\\log \\lambda_{ij}= \\gamma_{ij}\n\\end{align*}\n\\] En ANOVA c’est un modèle normal et le lien est l’identité.\nLe modèle est saturé, il y a une \\(n=8\\) observations et 8 points distincts dans le design: 8 niveaux d’interactions. Par ailleurs on a bien \\(p=8\\) paramètres.\nComme le modèle est saturé nous avons que les items ci-dessous représentent la même chose:\n\nles ajustements du modèle mod1\nles observations en chaque point du design (chaque niveau d’interaction)\nla moyenne en chaque point du design car il n’y a qu’un point par niveau d’interaction\n\n\nall(abs(fitted(mod1) - don$Y)&lt;1e-10)\n\n[1] TRUE\n\nmoyparcellule &lt;- aggregate(don$Y, list(don$Sexe,don$Prevention), mean)[,\"x\"]\nall(abs(fitted(mod1) - moyparcellule)&lt;1e-10)\n\n[1] TRUE\n\n\nEn passant au log, les ajustements (ou les comptages, ou la moyenne par niveau) valent les coefficients \\(\\beta_{ij}\\):\n\nall(abs(log(don$Y) - coef(mod1))&lt;1e-10)\n\n[1] TRUE\n\n\n\nModèle de Poisson sans interaction\n\nmod2 &lt;- glm( Y ~ 1 + Sexe + Prevention, data=don, family=poisson)\n\nLe comptage \\(Y_{ij}\\) (l’effectif) est modélisé par un modèle de Poisson : les comptages suivent une loi de Poisson de paramètre \\(\\lambda_{ij}\\) dont le log est modélisé par \\[\n\\begin{align*}\n\\log \\lambda_{ij}= \\mu + \\alpha_i + \\beta_{j}\n\\end{align*}\n\\]\n\nIl y a des contraintes identifiantes qui sont celles choisies par défaut: \\(\\beta_1=\\alpha_1=0\\)\n\n\n      mod2\n\n\nCall:  glm(formula = Y ~ 1 + Sexe + Prevention, family = poisson, data = don)\n\nCoefficients:\n              (Intercept)                      SexeM  \n                 1.381983                   0.008605  \n   PreventionMoustiquaire             PreventionRien  \n                 4.923624                   4.043051  \nPreventionSerpentin/Spray  \n                 2.063693  \n\nDegrees of Freedom: 7 Total (i.e. Null);  3 Residual\nNull Deviance:      1998 \nResidual Deviance: 3.24     AIC: 60.92\n\n\n\nLe modèle n’est pas saturé, il y a toujours 8 points distincts dans le design et \\(p=5\\) paramètres.\nLes estimations du modèle tendent à modéliser les effectifs par un effet Sexe et Prevention sans interaction.\n\nComparaison via AIC\n\nAIC(mod1)\n\n[1] 63.67658\n\nAIC(mod2)\n\n[1] 60.91634\n\n\nLe modèle avec le plus petit AIC est retenu: le modèle sans interaction. Ce choix est en accord avec la décision prise en question 2.\n\n\n\nExercice 8 (Table de contingence et probabilité)  \n\nLa probabilité d’être une femme \\[\n\\begin{align*}\n\\pi_{1.}&=\\Pr(F)=\\Pr(F\\cup \\mathrm{Autre}) +\n\\Pr(F\\cup \\mathrm{Moust} +\n\\Pr(F\\cup \\mathrm{Rien}+\n\\Pr(F\\cup \\mathrm{Ser/Sp}\\\\\n&=\\sum_{j=1}^4\\pi_{1j}   \n\\end{align*}\n\\]\nLa probabilité d’utiliser au moyen Autre est \\[\n\\begin{align*}\n\\pi_{.1}&=\\Pr(\\mathrm{Autre})=\\Pr(F\\cup \\mathrm{Autre}) +\\Pr(M\\cup \\mathrm{Autre}) \\\\\n&=\\sum_{i=1}^2\\pi_{i1}   \n\\end{align*}\n\\] Toutes les autres lignes ou colonnes sont traitées de manière similaire.\nContraintes Il s’agit de probabilité donc leur somme vaut 1: \\[\n\\begin{align*}\n  \\pi_{.1}+\\pi_{.2}+\\pi_{.3}+\\pi_{.4}&=1\\\\\n  \\pi_{1.}+\\pi_{2.}&=1\n\\end{align*}\n\\] On a aussi d’autres probabilités : \\[\n\\begin{align*}\n  \\sum{i,j} \\pi_{ij}&=1\\\\\n  \\sum{i} \\pi_{i|j}&=1, \\forall j\\in\\{1,2,3,4\\}\\\\\n  \\sum{j} \\pi_{j|i}&=1, \\forall i\\in\\{1,2\\}\\\\\n\\end{align*}\n\\]\nIndépendance \\[\n\\begin{align*}\n  \\pi_{ij}&=\\Pr(\\mathrm{Sexe}=i \\cup \\mathrm{Prévention}=j)=\\Pr(\\mathrm{Sexe}=i)\\Pr(\\mathrm{Prévention}=j)\\pi_{i.}\\pi_{.j}\n\\end{align*}\n\\]\nOn part de l’écart proposé et on multiplie par \\(N\\): \\[\n\\begin{align*}\n  \\pi_{ij}&=\\pi_{i.}\\pi_{.j}\\frac{\\pi_{ij}}{\\pi_{i.}\\pi_{.j}}\\\\\n  N\\pi_{ij}&=N\\pi_{i.} N \\pi_{.j}\\frac{N\\pi_{ij}}{N\\pi_{i.}N\\pi_{.j}}\n  \\log(N\\pi_{ij}) &= \\log N\\pi_{i.} + \\log N\\pi_{.j} +\n  \\log \\frac{N\\pi_{ij}}{N\\pi_{i.}N\\pi_{.j}}\\\\\n\\log(\\lambda_{ij}) &= \\alpha_i + \\beta_j + \\gamma_{ij}.\n\\end{align*}\n\\] On a comme contrainte \\[\n\\begin{align*}\n  \\pi_{1.}+\\pi_{2.}&=1=\\frac{1}{N}(\\exp(\\alpha_1)+ \\exp(\\alpha_2))\n  \\pi_{.1}+\\pi_{.2}+\\pi_{.3}+\\pi_{.4}&=1=\n  \\frac{1}{N}(\\sum_{j=1}^4\\exp(\\alpha_1\\pi_{.j})\\\\\n\\end{align*}\n\\] On peut aussi exprimer les contraintes sur les \\(\\{\\gamma_{ij}\\}\\) avec les contraintes sur les probabilités conditionnelles.\n\n\n\nExercice 9 (Loi Multinomiale) Soit \\(N\\) qui suit une multinomiale : \\[%\n\\begin{align*}\n\\Pr(N_1=n_1,\\dotsc,N_K=n_K)&=\\frac{n!}{n_1! \\dotsc n_K!}\\prod_{k=1}^K \\pi_k^{n_k}.\n\\end{align*}\n\\] en introduisant l’exponentielle \\[%\n\\begin{align*}\n  f(n1,\\dotsc,n_K)&=\\exp\\biggl(\\log\\Bigl(\\frac{n!}{n_1! \\dotsc n_K!}\\Bigr)\\biggr)\n  \\exp\\biggl(\\log\\Bigl(\\prod_{k=1}^K \\pi_k^{n_k}\\Bigr)\\biggr)\\\\\n  f(n1, \\dotsc, n_K)&=\\exp\\biggl(\\sum_{k=1}^K n_k\\log\\pi_k + \\log\\Bigl(\\frac{n!}{n_1! \\dotsc n_K!}\\Bigr)\\biggr)\n\\end{align*}\n\\]\n\n\nExercice 10  \n\nNous souhaitons modéliser des effectifs répartis dans des cases \\(i\\times j\\) ce qui représente le champs d’action de la loi multinomiale.\n\nUn premier modèle possible est de modéliser toute la table d’un coup. On désigne par \\(N_{ij},i\\in\\{1,2\\},j\\in\\{1,2,3\\}\\) l’effectif (aléatoire) du croisement \\((i,j)\\) et par \\(\\pi_{ij}\\) la probabilité (inconnue) d’appartenir à ce croisement (voir le tableau de l’énoncé). Ainsi \\(N_{1,3}\\) correspondra par exemple au croisement (,). Nous avons donc l’effectif total de la table \\(n=631\\) et nous utilisons toute la table d’un coup et nous modélisons les effectifs \\(N_{ij}\\) par une loi multinomiale \\(\\mathcal{M}(n,\\pi_{11}, \\pi_{12}, \\dotsc, \\pi_{23})\\). Pour une case \\(i,j\\) nous avons pour les probabilités: \\[%\n\\begin{align*}\n  \\Pr(\\mathtt{Gr}=i,\\mathtt{In}=j)&=\\pi_{ij}.\n\\end{align*}\n\\] Si nous soupçonnons qu’il n’y ait pas de lien entre Groupe et Intention nous pouvons simplifier le modèle via l’indépendance et avoir pour une case \\(i,j\\) \\[%\n\\begin{align*}\n  \\Pr(\\mathtt{Gr}=i,\\mathtt{In}=j)&=\\Pr(\\mathtt{Gr}=i)\\Pr(\\mathtt{In}=j)=\\pi_{i.}\\pi_{.j}.\n\\end{align*}\n\\] Cette approche est celle du test du \\(\\chi^2\\) et nous ne la développerons pas dans cet exercice.\nUne seconde approche (utilisée ici) est de modéliser chaque ligne du tableau.\n\nSi nous soupçonnons qu’il n’y ait pas de lien entre Groupe et Intention nous pouvons dire que chaque ligne est modélisée par une loi multinomiale \\(\\mathcal{M}(n_{i}, \\pi_{.1}, \\pi_{.2}, \\pi_{.3})\\). D’une ligne à l’autre la loi est presque la même (l’effectif \\(n_{i}\\) change): la répartition dans les cases suit les mêmes probabilité puisque l’on soupçonne qu’il n’y a pas de lien entre Groupe et Intention.\nSi nous soupçonnons qu’il y ait un lien entre Groupe et Intention nous pouvons dire que chaque ligne est modélisée par une loi multinomiale \\(\\mathcal{M}(n_{i}, \\pi_{i1}, \\pi_{i2}, \\pi_{i3})\\). D’une ligne à l’autre la loi est différente: l’effectif \\(n_{i}\\) change et la répartition dans les cases (\\(\\pi_{i1}, \\pi_{i2}, \\pi_{i3}\\)) change aussi ; c’est l’objet de la question 2.\n\n\nPour la ligne \\(i\\) on a comme observation les effectifs (d’une multinomiale) \\(n_{i1}, n_{i2}, n_{i3}\\). Leur somme vaut \\(n_{i.}\\). On doit répartir dans 3 groupes mais les probabilités de répartitions sont les mêmes d’une ligne à l’autre elles sont notées \\(\\pi_{.j}\\). On a donc la probabilité d’observer \\(n_{i1}, n_{i2}, n_{i3}\\): \\[%\n\\begin{align*}\n  \\Pr(N_{i1}=n_{i1},N_{i2}=n_{i2},N_{i3}=n_{i3})\n  &=\\frac{n_{i.}!}{n_{i1}! n_{i1}! n_{i3}!}\\prod_{j} (\\pi_{.j})^{n_{ij}}.\n\\end{align*}  \n\\] Pour toutes les données nous avons alors \\[%\n\\begin{align*}\n   \\Pr(N_{11}=n_{11},N_{12}=n_{12},,N_{13}=n_{13})\\Pr(N_{21}=n_{11},N_{22}=n_{22}, N_{23}=n_{23})&=\n  =\\prod_{i=1}^2\\frac{n_{i.}!}{n_{i1}! n_{i1}! n_{i3}!}\\prod_{j} (\\pi_{.j})^{n_{ij}}.\n\\end{align*}\n\\]\nLe modèle avec lien entre Groupe et Intention permet d’écrire la probabilité de l’échantillon comme: \\[%\n\\begin{align*}\n   \\Pr(N_{11}=n_{11},N_{12}=n_{12},,N_{13}=n_{13})\\Pr(N_{21}=n_{11},N_{22}=n_{22}, N_{23}=n_{23})&=\n  =\\prod_{i=1}^2\\frac{n_{i.}!}{n_{i1}! n_{i1}! n_{i3}!}\\prod_{j} (\\pi_{ij})^{n_{ij}}.\n\\end{align*}\n\\]\nCalculons les log-vraisemblances pour les deux modèles.\n\nModèle sans effet Groupe: \\[%\n\\begin{align*}\n\\mathcal L(Y,\\pi_{ij})=  & \\sum_{i,j} n_{ij}\\log\\pi_{.j} + \\mathrm{cte}.\n\\end{align*}\n\\] sous les contraintes \\(\\sum_{j} \\pi_{.j}=1\\) et les probabilités sont positives. La positivité peut être imposée en écrivant \\(\\pi_{.j}=\\exp(\\mu_{j})\\) et la somme à 1 impose alors \\[%\n\\pi_{.j}=\\frac{\\exp(\\mu_{j})}{\\sum_j \\exp(\\mu_{j})}.\n\\] En utilisant cette paramétrisation nous avons \\[%\n\\begin{align*}\n  \\mathcal L(Y,\\mu_{j})=& \\sum_{i,j} n_{ij}\\mu_{j} - \\sum_{i,j} n_{ij}\\log(\\sum_j\\exp(\\mu_{j}))  + \\mathrm{cte}\\\\\n  \\mathcal L(Y,\\mu_{j})=& \\sum_{j} n_{i.}\\mu_{j} - \\sum_{i} n_{i.}\\log(\\sum_j\\exp(\\mu_{j}))  + \\mathrm{cte}\\\\\n\\end{align*}\n\\]\nModèle avec effet Groupe: \\[%\n\\begin{align*}\n\\mathcal L(Y,\\pi)=& \\sum_{i,j} n_{ij}\\log\\pi_{ij} +\n\\sum_{i,j} \\log(n!)-\\sum_{i,j}\\log(n_{ij}!) =  \\sum_{i,j} n_{ij}\\log\\pi_{ij} + \\mathrm{cte}.,\n\\end{align*}\n\\] sous les contraintes \\(\\sum_{j} \\pi_{ij}=1, \\forall i\\) et les probabilités sont positives. La positivité peut être imposée en écrivant \\(\\pi_{ij}=\\exp(\\eta_{ij})\\) et la somme à 1 impose alors \\[%\n\\pi_{ij}=\\frac{\\exp(\\eta_{ij})}{\\sum_j \\exp(\\eta_{ij})}.\n\\] En utilisant cette paramétrisation nous avons \\[%\n\\begin{align*}\n  \\mathcal L(Y,\\eta_{ij})=& \\sum_{i,j} n_{ij}\\eta_{ij} - \\sum_{i,j} n_{ij}\\log(\\sum_j\\exp(\\eta_{ij}))  + \\mathrm{cte}\\\\\n  \\mathcal L(Y,\\eta_{ij})=& \\sum_{j} n_{ij}\\eta_{ij} - \\sum_{i} n_{i.}\\log(\\sum_j\\exp(\\eta_{ij}))  + \\mathrm{cte}\\\\\n\\end{align*}\n\\tag{1}\\]\n\nLe modèle de poisson complet est \\[%\n\\begin{align*}\n   Y&\\sim\\mathcal{P}(\\lambda_{ij})\\\\\n   \\log(\\lambda_{ij})&=\\mu + \\alpha_i + \\beta_j + \\gamma_{ij}\n\\end{align*}\n\\] Ce modèle est sur-paramétré et nécessite des contraintes pour être estimable. On peut choisir \\[%\n\\begin{align*}\n  \\alpha_1&=0,\\\\\n  \\beta_1&=0,\\\\\n  \\gamma_{11}=\\gamma_{12}=\\dotsc=\\gamma{31}=0\n\\end{align*}\n\\] Il donne les mêmes prévisions que le modèle plus simple suivant: \\[%\n\\begin{align*}\n   Y&\\sim\\mathcal{P}(\\lambda_{ij})\\\\\n   \\log(\\lambda_{ij})&=\\mu + \\alpha_i + \\gamma_{ij}\n\\end{align*}\n\\] Ce modèle est sur-paramétré et nécessite des contraintes pour être estimable. On peut choisir $$% \\[\\begin{align*}\n  \\alpha_1&=0,\\\\\n  \\gamma_{11}=\\gamma_{12}=\\dotsc=\\gamma{31}=0\n  \\gamma_{11}=\\gamma_{21}=0\n\\end{align*}\\]\nIls donnent les mêmes prévisions que le modèle sans contraintes suivant: \\[%\n\\begin{align*}\n   Y&\\sim\\mathcal{P}(\\lambda_{ij})\\\\\n   \\log(\\lambda_{ij})&=m_{ij}\n\\end{align*}\n\\] Pour ce dernier modèle, nous avons l’écriture suivante (qui correspond bien à l’écriture de l’équation ci-dessus) et où l’on voit apparaître sous forme matricielle le modèle saturé (la matrice de design est \\(6\\times 6\\) et de plein rang): \\[%\n\\begin{align*}\n  \\begin{bmatrix}\n\\log(\\lambda_{11}) \\\\\n\\log(\\lambda_{21}) \\\\\n\\log(\\lambda_{12}) \\\\\n\\log(\\lambda_{22}) \\\\\n\\log(\\lambda_{13}) \\\\\n\\log(\\lambda_{23}) \\\\\n\\end{bmatrix} &=\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\nm_{11} \\\\\nm_{21} \\\\\nm_{12} \\\\\nm_{22} \\\\\nm_{13} \\\\\nm_{23} \\\\\n  \\end{bmatrix}\n\\end{align*}\n\\] Pour le second modèle nous avons \\[%\n\\begin{align*}\n  \\begin{bmatrix}\n\\log(\\lambda_{11}) \\\\\n\\log(\\lambda_{21}) \\\\\n\\log(\\lambda_{12}) \\\\\n\\log(\\lambda_{22}) \\\\\n\\log(\\lambda_{13}) \\\\\n\\log(\\lambda_{23}) \\\\\n\\end{bmatrix} &=\n\\begin{bmatrix}\n1&0&0&0&0&0\\\\\n1&1&0&0&0&0\\\\\n1&0&1&0&0&0\\\\\n1&1&0&1&0&0\\\\\n1&0&0&0&1&0\\\\\n1&1&0&0&0&1\\\\\n  \\end{bmatrix} \\begin{bmatrix}\n\\mu\\\\\n\\alpha_2\\\\\n\\gamma_{12} \\\\\n\\gamma_{22} \\\\\n\\gamma_{13} \\\\\n\\gamma_{23} \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mu +\\alpha_{1} + \\gamma_{11} \\\\\n\\mu +\\alpha_{1} + \\gamma_{21} \\\\\n\\mu +\\alpha_{2} + \\gamma_{12} \\\\\n  \\mu +\\alpha_{2} + \\gamma_{22} \\\\\n  \\mu +\\alpha_{2} + \\gamma_{13} \\\\\n\\mu +\\alpha_{3} + \\gamma_{23} \\\\\n  \\end{bmatrix}\n\\end{align*}\n\\] Pour le premier modèle nous avons \\[%\n\\begin{align*}\n  \\begin{bmatrix}\n\\log(\\lambda_{11}) \\\\\n\\log(\\lambda_{21}) \\\\\n\\log(\\lambda_{12}) \\\\\n\\log(\\lambda_{22}) \\\\\n\\log(\\lambda_{13}) \\\\\n\\log(\\lambda_{23}) \\\\\n\\end{bmatrix} &=\n\\begin{bmatrix}\n1&0&0&0&0&0\\\\\n1&1&0&0&0&0\\\\\n1&0&1&0&0&0\\\\\n1&1&1&0&1&0\\\\\n1&0&0&1&0&0\\\\\n1&1&0&1&0&1\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mu\\\\\n\\alpha_2\\\\\n\\beta_2\\\\\n\\beta_3\\\\\n\\gamma_{13} \\\\\n\\gamma_{23} \\\\\n\\end{bmatrix}\n=\n  \\begin{bmatrix}\n\\mu +\\alpha_{1} +\\beta_{1} + \\gamma_{11} \\\\\n\\mu +\\alpha_{1}+\\beta_{2} + \\gamma_{21} \\\\\n\\mu +\\alpha_{2}+\\beta_{3} + \\gamma_{12} \\\\\n  \\mu +\\alpha_{2}+\\beta_{1} + \\gamma_{22} \\\\\n  \\mu +\\alpha_{2}+\\beta_{2} + \\gamma_{13} \\\\\n\\mu +\\alpha_{3}+\\beta_{3} + \\gamma_{23} \\\\\n  \\end{bmatrix}\n\\end{align*}\n\\]\nCalculons la vraisemblance du modèle avec contrainte pour les observations \\(\\{n_{ij}\\}\\): \\[%\n\\begin{align*}\n   \\prod_{i,j}e^{-\\lambda_{ij}}\\frac{\\lambda_{ij}^{n_{ij}}}{n_{ij}}\n\\end{align*}\n\\] dont le logarithme est \\[%\n\\begin{align*}\n   \\mathcal{L}(n,\\lambda)&=-\\sum_{i,j}\\lambda_{ij} + \\sum_{i,j}n_{ij}\\log\\lambda_{ij} + cte.\n\\end{align*}\n\\] Comme \\(\\log(\\lambda_{ij})=\\mu + \\alpha_i + \\gamma_{ij}\\), on en déduit \\[%\n\\begin{align*}\n   \\mathcal{L}(n,\\mu,\\alpha_i,\\gamma_{ij})&=-\\sum_{i}\\sum_{j}\\exp(\\mu + \\alpha_i + \\gamma_{ij}) + \\sum_{i,j}n_{ij}(\\mu + \\alpha_i + \\gamma_{ij}) + cte.\n\\end{align*}\n\\] Posons \\(\\tau_{i}=\\sum_j \\exp((\\mu+ \\alpha_i) + \\gamma_{ij})\\) et on a donc \\[%\n\\begin{align*}\n   \\mathcal{L}(n,\\mu,\\tau_i,\\gamma_{ij})&=-\\sum_{i}\\tau_{i} + \\sum_{i,j}n_{ij}(\\mu + \\alpha_i)  + \\sum_{i,j}n_{ij}\\gamma_{ij} + cte.\n\\end{align*}\n\\] Exprimons \\((\\mu+ \\alpha_i)\\) en fonction de \\(\\tau_i\\): \\[%\n\\begin{align*}\n  \\tau_{i}&=\\sum_j \\exp((\\mu+ \\alpha_i) + \\gamma_{ij}) = \\exp(\\mu+ \\alpha_i) \\sum_j \\exp(\\gamma_{ij})\\\\\n  \\exp(\\mu+ \\alpha_i)&=\\frac{\\tau_{i}}{\\sum_j \\exp(\\gamma_{ij})}\\\\\n  \\mu+ \\alpha_i&=\\log\\frac{\\tau_{i}}{\\sum_j \\exp(\\gamma_{ij})}=\\log\\tau_{i} - \\log\\bigl(\\sum_j \\exp(\\gamma_{ij})\\bigr)\n\\end{align*}\n\\] et nous introduisons cette dernière équation dans la log-vraisemblance: \\[%\n\\begin{align*}\n  \\mathcal{L}(n,\\mu,\\tau_i,\\gamma_{ij})&=-\\sum_{i}\\tau_{i} + \\sum_{i,j}n_{ij} \\Bigl(\\log\\tau_{i} - \\log\\bigl(\\sum_j \\exp(\\gamma_{ij})\\bigr)\\Bigr) + \\sum_{i,j}n_{ij}\\gamma_{ij} + cte\\\\\n  \\mathcal{L}(n,\\mu,\\tau_i,\\gamma_{ij})&=\\sum_{i}\\bigl(n_{i.} \\log\\tau_{i}-\\tau_{i}\\bigr) +  \\sum_{i,j}n_{ij}\\gamma_{ij} - \\sum_{i}n_{i.}\\log\\bigl(\\sum_j \\exp(\\gamma_{ij})\\bigr)+ cte.\n\\end{align*}\n\\] La vraisemblance est séparée en 2 parties: le premier terme \\(\\sum_{i}\\bigl(n_{i.} \\log\\tau_{i}-\\tau_{i}\\bigr)\\) dépend de \\(\\tau_i\\) alors que les 2 derniers dépendent de \\(\\gamma_{ij}\\). On reconnaît dans les 2 derniers termes la vraisemblance de la modélisation multinomiale avec effet Groupe (équation 1). La maximisation donnerait les mêmes \\(\\gamma_{ij}\\) si il n’y avait pas les contraintes… Regardons cela:\n\nDu côté de la régression Poisson on a pour chaque cellule \\((i,j)\\) la modélisation \\(\\mu + \\alpha_i + \\gamma_{ij}\\) avec comme contrainte par exemple \\[%\n\\begin{align*}\n  \\alpha_1&=0,\\\\\n  \\beta_1&=0,\\\\\n  \\gamma_{11}=\\gamma_{12}=\\gamma_{21}=\\gamma_{22}=0\n\\end{align*}\n\\] Pour passer sur l’échelle des probabilités on utilise toujours l’exponentielle et on divise par la somme sur chaque ligne \\(i\\) (les probabilités doivent sommer à 1 pour chaque ligne) \\[%\n\\begin{align*}\n  \\pi_{ij}&=\\frac{\\exp(\\mu + \\alpha_i + \\gamma_{ij})}{\\sum_j \\exp(\\mu + \\alpha_i + \\gamma_{ij})}\n\\end{align*}\n\\]\nDu côté de la régression Multinomiale on a pour chaque cellule \\((i,j)\\) directement \\(\\gamma_{ij}\\) et pour passer sur l’échelle des probabilités on utilise toujours l’exponentielle et on divise par la somme sur chaque ligne \\(i\\) (les probabilités doivent sommer à 1 pour chaque ligne) \\[%\n\\begin{align*}\n  \\pi_{ij}&=\\frac{\\exp(\\gamma_{ij})}{\\sum_j \\exp(\\gamma_{ij})}\n\\end{align*}\n\\]\n\nLe même calcul que ci-dessus peut être conduit avec le modèle sans contrainte avec les \\(m_{ij}\\) et on obtient (non demandé) \\[%\n\\begin{align*}\n  \\mathcal{L}(n,m_{ij})&=-\\sum_{i,j}\\exp(m_{ij}) + \\sum_{i,j} n_{ij}m_{ij} + cte\n\\end{align*}\n\\] Pour passer sur l’échelle des probabilités on utilise toujours l’exponentielle et on divise par la somme sur chaque ligne \\(i\\) (les probabilités doivent sommer à 1 pour chaque ligne) \\[%\n\\begin{align*}\n\\pi_{ij}&=\\frac{\\exp(m_{ij})}{\\sum_j \\exp(m_{ij})}\n\\end{align*}\n\\]",
    "crumbs": [
      "Correction des exercices",
      "IV Le modèle linéairé généralisé",
      "12 Régression de Poisson"
    ]
  },
  {
    "objectID": "code/chap10.html",
    "href": "code/chap10.html",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "",
    "text": "Importation de l’ozone\n\nozone &lt;- read.table(\"../donnees/ozone_complet.txt\", header = T, sep = \";\")\ndim(ozone)\n\n[1] 1464   23\n\n\nÉlimination des individus avec une valeur manquante\n\nindNA &lt;- which(is.na(ozone), arr.ind = T)[,1]\nozone2 &lt;- ozone[-indNA,]\n\nImportation du fichier d’ozone sans valeurs manquantes avec les projections\n\nozone &lt;- read.table(\"../donnees/ozone_transf.txt\", header = T, sep = \";\")\n\net préparation du data-frame qui contiendra les résultats de chaque méthode\n\nRES &lt;- data.frame(Y = ozone$maxO3)\n\nPour le moment il ne contient qu’une seule colonne avec les données à prévoir.",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#régression-multiple",
    "href": "code/chap10.html#régression-multiple",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régression multiple",
    "text": "Régression multiple\nChargement du package pour la sélection de variables\n\nlibrary(leaps)\n\nEvaluation de la qualité prédictive de la régression linéaire et de la sélection de variables via BIC (algorithme exhaustif)\n\nfor(i in 1:nbbloc){\n  ###MCO global\n  reg &lt;- lm(maxO3~.,data=ozone[bloc!=i,])\n  RES[bloc==i,\"MCO\"] &lt;- predict(reg,ozone[bloc==i,])\n  ###MCO choix\n  recherche &lt;- regsubsets(maxO3~., int=T, nbest=1, nvmax=22, \n                                        data=ozone[bloc!=i,])\n  resume &lt;- summary(recherche)\n  nomselec &lt;- colnames(resume$which)[\n                       resume$which[which.min(resume$bic),] ][-1]\n  formule &lt;- formula(paste(\"maxO3~\",paste(nomselec,collapse=\"+\")))\n  regbic &lt;- lm(formule,data=ozone[bloc!=i,])\n  RES[bloc==i,\"choix\"] &lt;- predict(regbic,ozone[bloc==i,])\n}",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#lasso-ridge-et-elasticnet",
    "href": "code/chap10.html#lasso-ridge-et-elasticnet",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Lasso, ridge et elasticnet",
    "text": "Lasso, ridge et elasticnet\nChargement du package pour lasso, ridge et elasticnet et création des matrices nécessaires à son utilisation :\n\nlibrary(glmnet)\nozone.X &lt;- model.matrix(maxO3~.,data=ozone)[,-1]\nozone.Y &lt;- ozone[,\"maxO3\"]\n\nÉvaluation de la qualité prédictive des régressions lasso, ridge et elasticnet:\n\nfor(i in 1:nbbloc){  \n  XA &lt;- ozone.X[bloc!=i,]\n  YA &lt;- ozone.Y[bloc!=i]\n  XT &lt;- ozone.X[bloc==i,]\n  ###ridge\n  tmp &lt;- cv.glmnet(XA,YA,alpha=0)\n  mod &lt;- glmnet(XA,YA,alpha=0,lambda=tmp$lambda.min)\n  RES[bloc==i,\"ridge\"] &lt;- predict(mod,XT)\n  ###lasso\n  tmp &lt;- cv.glmnet(XA,YA,alpha=1)\n  mod &lt;- glmnet(XA,YA,alpha=0,lambda=tmp$lambda.min)\n  RES[bloc==i,\"lasso\"] &lt;- predict(mod,XT)\n  ###elastic\n  tmp &lt;- cv.glmnet(XA,YA,alpha=0.5)\n  mod &lt;- glmnet(XA,YA,alpha=.5,lambda=tmp$lambda.min)\n  RES[bloc==i,\"elastic\"] &lt;- predict(mod,XT)\n}",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#régressions-sur-composantes",
    "href": "code/chap10.html#régressions-sur-composantes",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régressions sur composantes",
    "text": "Régressions sur composantes\nChargement du package pour les régressions sur composantes\n\nlibrary(pls)\n\nÉvaluation de la qualité prédictive des régressions PCR et PLS\n\nfor(i in 1:nbbloc){\n   #####PLS\n   tmp &lt;- plsr(maxO3~.,data=ozone[bloc!=i,],ncomp=20,\n                                 validation=\"CV\",scale=TRUE)\n   mse &lt;- MSEP(tmp,estimate=c(\"train\",\"CV\"))\n   npls &lt;- which.min(mse$val[\"CV\",,])-1 \n   mod &lt;- plsr(maxO3~.,ncomp=npls,data=ozone[bloc!=i,],scale=TRUE)\n   RES[bloc==i,\"PLS\"] &lt;- predict(mod,ozone[bloc==i,],ncomp=npls)\n   #####PCR\n   tmp &lt;- pcr(maxO3~.,data=ozone[bloc!=i,],ncomp=20,\n                                    validation=\"CV\",scale=TRUE)\n   mse &lt;- MSEP(tmp,estimate=c(\"train\",\"CV\"))\n   npcr &lt;- which.min(mse$val[\"CV\",,])-1 \n   mod &lt;- pcr(maxO3~.,ncomp=npcr,data=ozone[bloc!=i,],scale=TRUE)\n   RES[bloc==i,\"PCR\"] &lt;- predict(mod,ozone[bloc==i,],ncomp=npcr)\n }\n\nLes résultats :\n\nRES |&gt; \n  dplyr::summarise(across(-Y,~mean((Y-.)^2)))\n\n       MCO    choix    ridge    lasso  elastic      PLS      PCR\n1 187.2726 188.8491 187.8304 187.1023 187.0389 187.2622 187.2685",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#régression-linéaire",
    "href": "code/chap10.html#régression-linéaire",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régression linéaire",
    "text": "Régression linéaire\nLa fonction\n\nsse_reg &lt;- function(don,bloc,b) {\n    m_reg &lt;- lm(maxO3~.,data=don[bloc!=b,])\n    previsions &lt;- predict(m_reg,don[bloc==b,])\n    return(sum((don[bloc==b,\"maxO3\"]-previsions)^2))\n}\n\nLa qualité de la modélisation\n\nset.seed(1234)\nssereg  &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n    ssereg[r] &lt;- ssereg[r] + sse_reg(ozone,bloc,b)\n  }\n}\nres_rep$MCO &lt;- round(mean(ssereg/nrow(ozone)),2)",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#choix-de-variables",
    "href": "code/chap10.html#choix-de-variables",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Choix de variables",
    "text": "Choix de variables\nLa fonction\n\nlibrary(leaps)\nsse_regbic &lt;- function(don,bloc,b,nvmax,method) {\n    recherche &lt;- regsubsets(maxO3~., int=T, nbest=1,data=don[bloc!=b,],\n                           nvmax=nvmax,method=method)\n    resume &lt;- summary(recherche)\n    nomselec &lt;- colnames(resume$which)[resume$which[which.min(resume$bic),]][-1]\n    formule &lt;- formula(paste(\"maxO3 ~\", paste(nomselec, collapse = \"+\")))\n    m_reg &lt;- lm(formule,data=don[bloc!=b,])\n    previsions &lt;- predict(m_reg,don[bloc==b,])\n    return(sum((don[bloc==b,\"maxO3\"]-previsions)^2))\n}\n\nLa qualité de la modélisation\n\nset.seed(1234)\nsseregbic &lt;-  rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n    sseregbic[r] &lt;- sseregbic[r] + sse_regbic(ozone,bloc,b,22,\"exhaustive\")\n  }\n}\nres_rep$choix &lt;- mean(sseregbic/nrow(ozone))",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#lasso",
    "href": "code/chap10.html#lasso",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Lasso",
    "text": "Lasso\nLa fonction\n\nlibrary(glmnet)\nsse_glmnet &lt;- function(X,Y,bloc,b,a) {\n  rech &lt;- cv.glmnet(X[bloc!=b,], Y[bloc!=b,drop=FALSE], alpha=a)\n  prev &lt;- predict(rech, newx=X[bloc==b,], s=rech$lambda.min)\n  return(sum((Y[bloc==b,\"maxO3\"] - as.vector(prev))^2))\n}\n\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsselasso &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sselasso[r] &lt;- sselasso[r] + sse_glmnet(X,Y,bloc,b,a=1)\n  }\n}\nres_rep$lasso &lt;- round(mean(sselasso/nrow(ozone)),2)",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#ridge",
    "href": "code/chap10.html#ridge",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "RIDGE",
    "text": "RIDGE\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsseridge &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sseridge[r] &lt;- sseridge[r] + sse_glmnet(X,Y,bloc,b,a=0)\n  }\n}\nres_rep$ridge &lt;- mean(sseridge/nrow(ozone))",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap10.html#elastic-net",
    "href": "code/chap10.html#elastic-net",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Elastic-net",
    "text": "Elastic-net\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsseelasticnet &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sseelasticnet[r] &lt;- sseelasticnet[r] + sse_glmnet(X,Y,bloc,b,a=0.5)\n  }\n}\nres_rep$elastic &lt;- mean(sseelasticnet/nrow(ozone))\n\nLes résultats\n\nres_rep\n\n     MCO    choix    ridge  lasso  elastic\n1 188.36 189.6025 188.6624 187.92 187.8242",
    "crumbs": [
      "Codes R",
      "III Réduction de dimension",
      "10 Comparaison des différentes méthodes, étude de cas réels"
    ]
  },
  {
    "objectID": "code/chap11.html",
    "href": "code/chap11.html",
    "title": "11 Régression logistique",
    "section": "",
    "text": "Présentation du modèle\n\nartere &lt;- read.table(\"../donnees/artere.txt\",header=T)\nplot(chd~age,data=artere,pch=16)\n\n\n\n\n\n\n\n\n\ntab_freq &lt;- table(artere$agrp,artere$chd)\nfreq &lt;- tab_freq[,2]/apply(tab_freq,1,sum)\ncbind(tab_freq,round(freq,3))\n\n   0  1      \n1  9  1 0.100\n2 13  2 0.133\n3  9  3 0.250\n4 10  5 0.333\n5  7  6 0.462\n6  3  5 0.625\n7  4 13 0.765\n8  2  8 0.800\n\nx.age &lt;- c(19,29,34,39,44,49,54,59)\nplot(x.age,c(freq),type=\"s\",xlim=c(18,80),ylim=c(0,1),xlab=\"âge\",ylab=\"freq\")\nlines(c(59,80),rep(freq[length(freq)],2))\nx &lt;- seq(15,80,by=0.01)\ny &lt;- exp(-5.31+0.11*x)/(1+exp(-5.31+0.11*x))\nlines(x,y,lty=3)\n\n\n\n\n\n\n\n\n\nglm(chd~age,data=artere,family=binomial)\n\n\nCall:  glm(formula = chd ~ age, family = binomial, data = artere)\n\nCoefficients:\n(Intercept)          age  \n    -5.3095       0.1109  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      136.7 \nResidual Deviance: 107.4    AIC: 111.4\n\n\n\nset.seed(12345)\nX &lt;- factor(sample(c(\"A\",\"B\",\"C\"),100,replace=T))\n#levels(X) &lt;- c(\"A\",\"B\",\"C\")\nY &lt;- rep(0,100)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),1,0.9)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),1,0.1)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),1,0.9)\ndonnees &lt;- data.frame(X,Y)\nmodel &lt;- glm(Y~.,data=donnees,family=binomial)\ncoef(model)\n\n(Intercept)          XB          XC \n  2.0794415  -3.6549779   0.3772942 \n\nmodel1 &lt;- glm(Y~C(X,sum),data=donnees,family=binomial)\ncoef(model1)\n\n(Intercept)  C(X, sum)1  C(X, sum)2 \n  0.9868803   1.0925612  -2.5624167 \n\n\n\n\nIntervalles de confiance et tests\n\nlibrary(bestglm)\ndata(SAheart)\nnew.SAheart &lt;- SAheart[c(2,408,35),]\nrow.names(new.SAheart) &lt;- NULL\nSAheart &lt;- SAheart[-c(2,408,35),]\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nround(summary(model)$coefficients,4)\n\n               Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     -6.0837     1.3141 -4.6294   0.0000\nsbp              0.0065     0.0058  1.1268   0.2598\ntobacco          0.0814     0.0269  3.0232   0.0025\nldl              0.1794     0.0600  2.9891   0.0028\nadiposity        0.0184     0.0295  0.6224   0.5337\nfamhistPresent   0.9325     0.2291  4.0694   0.0000\ntypea            0.0392     0.0123  3.1845   0.0015\nobesity         -0.0637     0.0446 -1.4300   0.1527\nalcohol          0.0002     0.0045  0.0346   0.9724\nage              0.0439     0.0122  3.5923   0.0003\n\nconfint.default(model)\n\n                      2.5 %       97.5 %\n(Intercept)    -8.659355064 -3.507983650\nsbp            -0.004797560  0.017773140\ntobacco         0.028628110  0.134174033\nldl             0.061770934  0.297043348\nadiposity      -0.039461469  0.076187468\nfamhistPresent  0.483354369  1.381572764\ntypea           0.015090098  0.063396115\nobesity        -0.151054913  0.023612482\nalcohol        -0.008639577  0.008950096\nage             0.019931097  0.067793230\n\n\n\nn &lt;- 1000\nset.seed(123)\nX1 &lt;- sample(c(\"A\",\"B\",\"C\"),n,replace=TRUE)\nX2 &lt;- rnorm(n)\nX3 &lt;- runif(n)\ncl &lt;- 1+0*(X1==\"A\")+1*(X1==\"B\")-3*(X1==\"C\")+2*X2\nY &lt;- rbinom(n,1,exp(cl)/(1+exp(cl)))\ndonnees &lt;- data.frame(X1,X2,X3,Y)\n\n\nm1 &lt;- glm(Y~.,data=donnees,family=binomial)\nlibrary(car)\nAnova(m1,type=3,test.statistic=\"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n            Df    Chisq Pr(&gt;Chisq)    \n(Intercept)  1  28.4698  9.517e-08 ***\nX1           2 212.5061  &lt; 2.2e-16 ***\nX2           1 210.3902  &lt; 2.2e-16 ***\nX3           1   0.3096     0.5779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m1,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n   LR Chisq Df Pr(&gt;Chisq)    \nX1   376.74  2     &lt;2e-16 ***\nX2   417.66  1     &lt;2e-16 ***\nX3     0.31  1     0.5778    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm01 &lt;- glm(Y~X2+X3,data=donnees,family=binomial)\nm02 &lt;- glm(Y~X1+X3,data=donnees,family=binomial)\nm03 &lt;- glm(Y~X1+X2,data=donnees,family=binomial)\nanova(m01,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X2 + X3\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       997    1109.67                          \n2       995     732.93  2   376.74 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m02,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       996    1150.59                          \n2       995     732.93  1   417.66 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m03,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X1 + X2\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       996     733.24                     \n2       995     732.93  1  0.30976   0.5778\n\nlibrary(aod)\nwald.test(Sigma=vcov(m1),b=coef(m1),Terms=c(2,3))\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 212.5, df = 2, P(&gt; X2) = 0.0\n\n\n\n\nPrévisions\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\n\n\nnew.SAheart &lt;- SAheart[c(2,408,35),-10]\nrow.names(new.SAheart) &lt;- NULL\nnew.SAheart\n\n  sbp tobacco  ldl adiposity famhist typea obesity alcohol age\n1 118    0.08 3.48     32.28 Present    52   29.14    3.81  46\n2 178   20.00 9.78     33.55  Absent    37   27.29    2.88  62\n3 140    3.90 7.32     25.05  Absent    47   27.36   36.77  32\n\n\n\npredict(model, newdata=new.SAheart)\n\n         1          2          3 \n-0.9599837  1.5028033 -1.5743496 \n\n\n\npredict(model, newdata=new.SAheart,type=\"response\")\n\n        1         2         3 \n0.2768815 0.8179922 0.1715972 \n\n\n\nprev &lt;- predict(model,newdata=new.SAheart,type=\"link\",se.fit = TRUE)\ncl_inf &lt;- prev$fit-qnorm(0.975)*prev$se.fit\ncl_sup &lt;- prev$fit+qnorm(0.975)*prev$se.fit\nbinf &lt;- exp(cl_inf)/(1+exp(cl_inf))\nbsup &lt;- exp(cl_sup)/(1+exp(cl_sup))\ndata.frame(binf,bsup)\n\n       binf      bsup\n1 0.1774323 0.4046504\n2 0.6040315 0.9297800\n3 0.1024782 0.2731474\n\n\n\nunique(artere[,\"age\"])\n\n [1] 20 23 24 25 26 28 29 30 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n[26] 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 69\n\nsature &lt;- aggregate(artere[,\"chd\"],by=list(artere$age),FUN=mean)\nnames(sature) &lt;- c(\"age\",\"p\")\nndesign &lt;- aggregate(artere[,\"chd\"],by=list(artere$age),FUN=length)\nnames(ndesign) &lt;- c(\"age\",\"n\")\nmerge(sature,ndesign,by=\"age\")[1:5,]\n\n  age   p n\n1  20 0.0 1\n2  23 0.0 1\n3  24 0.0 1\n4  25 0.5 2\n5  26 0.0 2\n\nplot(chd~age,data=artere,pch=15+chd,col=chd+1)\nlines(p~age,data=sature)\n\n\n\n\n\n\n\n\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nlibrary(generalhoslem)\nlogitgof(obs= SAheart$chd, exp = fitted(model))\n\n\n    Hosmer and Lemeshow test (binary model)\n\ndata:  SAheart$chd, fitted(model)\nX-squared = 6.6586, df = 8, p-value = 0.5739\n\n\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nprev_lin &lt;- predict(model)\nres_P &lt;- residuals(model,type=\"pearson\") #Pearson\nres_PS &lt;- rstandard(model,type=\"pearson\") #Pearson standard\nres_D &lt;- residuals(model,type=\"deviance\")  #Deviance\nres_DS &lt;- rstandard(model,type=\"deviance\") #Deviance standard\n\n\npar(mfrow=c(2,2),pch=20,mai = c(0.1,0.15,0.1,0.1),mar=c(3,3,1,1),cex.axis=0.6,cex.lab=0.7,mgp=c(1.5,0.3,0),oma=c(1,0,0,0),tcl=-0.4)\nplot(res_PS,cex=0.3,xlab=\"index\",ylab=\"Pearson Standard\")\nplot(prev_lin,cex=0.3,res_PS,xlab=\"Prevision lineaire\",ylab=\"Pearson Standard\")\nplot(res_DS,cex=0.3,xlab=\"index\",ylab=\"Deviance Standard\")\nplot(prev_lin,cex=0.3,res_DS,xlab=\"Prevision lineaire\",ylab=\"Deviance Standard\")\n\n\n\n\n\n\n\n\n\n\nChoix de variables\n\nmodel0 &lt;- glm(chd~sbp+ldl,data=SAheart,family=binomial)\nmodel1 &lt;- glm(chd~sbp+ldl+famhist+alcohol,data=SAheart,family=binomial)\nanova(model0,model1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: chd ~ sbp + ldl\nModel 2: chd ~ sbp + ldl + famhist + alcohol\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       456     548.18                          \n2       454     522.64  2   25.545 2.838e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ndata(SAheart)\nmod_sel &lt;- bestglm(SAheart,family=binomial,IC=\"BIC\")\nmod_sel$BestModels\n\n    sbp tobacco   ldl adiposity famhist typea obesity alcohol  age Criterion\n1 FALSE    TRUE  TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  506.3634\n2 FALSE    TRUE FALSE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  509.2566\n3 FALSE    TRUE  TRUE     FALSE    TRUE FALSE   FALSE   FALSE TRUE  509.9861\n4 FALSE   FALSE  TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  510.5745\n5 FALSE    TRUE  TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  510.7933\n\nmod_sel1 &lt;- bestglm(SAheart,family=binomial,IC=\"AIC\")\nmod_sel1$BestModels\n\n    sbp tobacco  ldl adiposity famhist typea obesity alcohol  age Criterion\n1 FALSE    TRUE TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  485.6856\n2 FALSE    TRUE TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  485.9799\n3  TRUE    TRUE TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  486.5490\n4  TRUE    TRUE TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  486.6548\n5 FALSE    TRUE TRUE      TRUE    TRUE  TRUE   FALSE   FALSE TRUE  487.4435\n\n\n\n\nScoring\n\nset.seed(1234)\nind.app &lt;- sample(nrow(SAheart),300)\ndapp &lt;- SAheart[ind.app,]\ndval &lt;- SAheart[-ind.app,]\n#Construction des modeles\nmodel1 &lt;- glm(chd~tobacco+famhist,data=dapp,family=binomial)\nmodel2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,\n                data=dapp,family=binomial)  \nround(coef(model1),3)\n\n   (Intercept)        tobacco famhistPresent \n        -1.784          0.140          1.095 \n\nround(coef(model2),3)\n\n   (Intercept)        tobacco famhistPresent      adiposity        alcohol \n        -3.180          0.117          1.022          0.059         -0.002 \n\nprev1 &lt;- round(predict(model1,newdata=dval,type=\"response\"))\nprev2 &lt;- round(predict(model2,newdata=dval,type=\"response\"))\nmean(prev1!=dval$chd)\n\n[1] 0.3395062\n\nmean(prev2!=dval$chd)\n\n[1] 0.3395062\n\n\n\nset.seed(1245)\nbloc &lt;- sample(1:10,nrow(SAheart),replace=TRUE)\ntable(bloc)\n\nbloc\n 1  2  3  4  5  6  7  8  9 10 \n52 39 44 62 49 36 47 38 53 42 \n\n\n\nprev &lt;- data.frame(matrix(0,nrow=nrow(SAheart),ncol=2))\nnames(prev) &lt;- c(\"model1\",\"model2\")\nfor (k in 1:10){\n  ind.val &lt;- bloc==k\n  dapp.k &lt;- SAheart[!ind.val,]\n  dval.k &lt;- SAheart[ind.val,]\n  model1 &lt;- glm(chd~tobacco+famhist,data=dapp.k,family=binomial)\n  model2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,data=dapp.k,family=binomial)  \n  prev[ind.val,1] &lt;- round(predict(model1,newdata=dval.k,type=\"response\"))\n  prev[ind.val,2] &lt;- round(predict(model2,newdata=dval.k,type=\"response\"))\n}\napply(sweep(prev,1,SAheart$chd,FUN=\"!=\"),2,mean)\n\n   model1    model2 \n0.3203463 0.3073593 \n\n\n\nscore1 &lt;- predict(model1,newdata=dval)\nscore2 &lt;- predict(model2,newdata=dval)\n\n\nlibrary(pROC)\nR1 &lt;- roc(dval$chd,score1)\nR2 &lt;- roc(dval$chd,score2)\nplot(R1,lwd=3,legacy.axes=TRUE)\nplot(R2,lwd=3,col=\"red\",lty=2,legacy.axes=TRUE,add=TRUE)\ncouleur &lt;- c(\"black\",\"red\")\nlegend(\"bottomright\",legend=c(\"score1\",\"score2\"),col=couleur,lty=1:2,lwd=2,cex=0.75)\n\n\n\n\n\n\n\n\n\nauc(R1)\n\nArea under the curve: 0.7356\n\nauc(R2)\n\nArea under the curve: 0.7372\n\n\n\nscore &lt;- data.frame(matrix(0,nrow=nrow(SAheart),ncol=2))\nnames(score) &lt;- c(\"score1\",\"score2\")\nfor (k in 1:10){\n  ind.val &lt;- bloc==k\n  dapp.k &lt;- SAheart[!ind.val,]\n  dval.k &lt;- SAheart[ind.val,]\n  model1 &lt;- glm(chd~tobacco+famhist,data=dapp.k,family=binomial)\n  model2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,data=dapp.k,family=binomial)  \n  score[ind.val,1] &lt;- predict(model1,newdata=dval.k)\n  score[ind.val,2] &lt;- predict(model2,newdata=dval.k)\n}\n\n\nscore$obs &lt;- SAheart$chd\nroc.cv &lt;- roc(obs~score1+score2,data=score)\ncouleur &lt;- c(\"black\",\"red\")\nmapply(plot,roc.cv,col=couleur,lty=1:2,add=c(F,T),lwd=3,legacy.axes=TRUE)\n\n                   score1      score2     \npercent            FALSE       FALSE      \nsensitivities      numeric,356 numeric,463\nspecificities      numeric,356 numeric,463\nthresholds         numeric,356 numeric,463\ndirection          \"&lt;\"         \"&lt;\"        \ncases              numeric,160 numeric,160\ncontrols           numeric,302 numeric,302\nfun.sesp           ?           ?          \nauc                0.7159872   0.7271937  \ncall               expression  expression \noriginal.predictor numeric,462 numeric,462\noriginal.response  integer,462 integer,462\npredictor          numeric,462 numeric,462\nresponse           integer,462 integer,462\nlevels             character,2 character,2\npredictor.name     \"score1\"    \"score2\"   \nresponse.name      \"obs\"       \"obs\"      \n\nlegend(\"bottomright\",legend=c(\"score1\",\"score2\"),col=couleur,lty=1:2,lwd=2,cex=0.75)\n\n\n\n\n\n\n\n\n\nsort(round(unlist(lapply(roc.cv,auc)),3),decreasing=TRUE)\n\nscore2 score1 \n 0.727  0.716",
    "crumbs": [
      "Codes R",
      "IV Le modèle linéairé généralisé",
      "11 Régression logistique"
    ]
  },
  {
    "objectID": "code/chap15.html",
    "href": "code/chap15.html",
    "title": "15 Données déséquilibrées",
    "section": "",
    "text": "df &lt;- data.frame(MALADE=c(208,42),\n                 NON_MALADE=c(48,202),\n                 FUMEUR=c(\"OUI\",\"NON\"))\nmodel &lt;- glm(cbind(MALADE,NON_MALADE)~FUMEUR,data=df,family=binomial)\ncoef(model)\n\n(Intercept)   FUMEUROUI \n  -1.570598    3.036935 \n\n\n\nnewX &lt;- data.frame(FUMEUR=c(\"OUI\",\"NON\"))\nrownames(newX) &lt;- c(\"OUI\",\"NON\")\npredict(model,newdata = newX,type=\"response\")\n\n      OUI       NON \n0.8125000 0.1721311 \n\n\n\nbeta1_cor &lt;- coef(model)[1]-log(0.995/0.005)\nbeta2 &lt;- coef(model)[2]\n\n\nexp(beta1_cor+beta2)/(1+exp(beta1_cor+beta2))\n\n(Intercept) \n 0.02131148 \n\nexp(beta1_cor)/(1+exp(beta1_cor))\n\n(Intercept) \n0.001043738 \n\n\n\ntau &lt;- c(0.05,0.95)\nind0 &lt;- which(df$Y==0)\nind1 &lt;- which(df$Y==1)\nchoix0 &lt;- sample(ind0, size=length(ind0)*tau[1], replace = F)\nchoix1 &lt;- sample(ind1, size=length(ind1)*tau[2], replace = F)\ndff &lt;- rbind(df[choix0,], df[choix1,])\n\n\nmod &lt;- glm(Y~., data=dff, family=\"binomial\")\n\n\ngamma &lt;- coef(mod)\ngamma[1] - log(tau[2]/tau[1])\n\n\nglm(Y~. + offset(rep(log(tau[2]/tau[1]), nrow(dff))),\n    data=dff, family=\"binomial\")",
    "crumbs": [
      "Codes R",
      "IV Le modèle linéairé généralisé",
      "15 Données déséquilibrées"
    ]
  },
  {
    "objectID": "code/chap15.html#quelques-méthodes-de-rééquilibrage",
    "href": "code/chap15.html#quelques-méthodes-de-rééquilibrage",
    "title": "15 Données déséquilibrées",
    "section": "Quelques méthodes de rééquilibrage",
    "text": "Quelques méthodes de rééquilibrage\n\nlibrary(tidyverse)\n\n\nset.seed(123458)\nn1 &lt;- 10\nX11 &lt;- runif(n1,0,0.25)\nX21 &lt;- runif(n1,0,1)\nX12 &lt;- runif(n1,0,1)\nX22 &lt;- runif(n1,0.75,1)\nn2 &lt;- 80\nX13 &lt;- runif(n2,0.25,1)\nX23 &lt;- runif(n2,0,0.75)\nX1 &lt;- c(X11,X12,X13)\nX2 &lt;- c(X21,X22,X23)\nY &lt;- c(rep(1,2*n1),rep(0,n2)) %&gt;% as.factor()\ndf &lt;- data.frame(X1,X2,Y,id=as.character(1:100))\ndf$Y[c(1,16)] &lt;- 0\n#df$Y[c(41,48,59)] &lt;- 1\ndf$Y[c(41,48)] &lt;- 1\ndf &lt;- df[,1:3]\nggplot(df)+aes(x=X1,y=X2)+geom_point(aes(color=Y))\n\n\n\n\n\n\n\n\n\nlibrary(UBL)\nover1 &lt;- RandOverClassif(Y~., dat=df)\nover2 &lt;- RandOverClassif(Y~., dat=df, C.perc=list(\"0\"=1,\"1\"=2))\nsummary(over1$Y)\n\n 0  1 \n80 80 \n\nsummary(over2$Y)\n\n 0  1 \n80 40 \n\n\n\nset.seed(1234)\nsmote1 &lt;- SmoteClassif(Y~.,dat=df,k=4)\nsmote2 &lt;- SmoteClassif(Y~.,dat=df,k=4,C.perc=list(\"0\"=1,\"1\"=2))\nsummary(smote1$Y)\n\n 0  1 \n50 50 \n\nsummary(smote2$Y)\n\n 0  1 \n80 40 \n\n\n\nnewsm1 &lt;- anti_join(smote1,df)\nnewsm2 &lt;- anti_join(smote2,df)\nnewsm &lt;- bind_rows(\"smote1\"=newsm1,\"smote2\"=newsm2,.id=\"algo\")\n\n\ndf3 &lt;- bind_rows(\"smote1\"=smote1,\"smote2\"=smote2,.id=\"algo\")\nggplot(df3)+aes(x=X1,y=X2,color=Y)+geom_point(aes(shape=Y),size=1.5)+facet_wrap(~algo)+\n  geom_point(data=newsm,shape=1,size=4) + theme(legend.position='none')\n\n\n\n\n\n\n\n\n\nunder1 &lt;- RandUnderClassif(Y~.,dat=df)\nunder2 &lt;- RandUnderClassif(Y~.,dat=df,C.perc=list(\"0\"=0.5,\"1\"=1))\nsummary(under1$Y)\n\n 0  1 \n20 20 \n\nsummary(under2$Y)\n\n 0  1 \n40 20 \n\n\n\ntomek1 &lt;- TomekClassif(Y~.,dat=df)\ntomek2 &lt;- TomekClassif(Y~.,dat=df,rem=\"maj\")\ntomek1[[2]]\n\n[1]   1   7  12  69  14 100  16  17\n\ntomek2[[2]]\n\n[1]   1  69 100  16\n\n\n\nind1 &lt;- tomek1[[2]]\nind2 &lt;- tomek2[[2]]\nXS1 &lt;- df[ind1,]\nXS2 &lt;- df[ind2,]\nXS &lt;- bind_rows(\"tomek1\"=XS1,\"tomek2\"=XS2,.id=\"algo\")\ndf5 &lt;- bind_rows(\"tomek1\"=df,\"tomek2\"=df,.id=\"algo\")\nggplot(df5)+aes(x=X1,y=X2,color=Y)+geom_point(aes(color=Y,shape=Y),size=1.5)+facet_wrap(~algo)+\n  geom_point(data=XS,shape=1,size=4) + theme(legend.position='none')",
    "crumbs": [
      "Codes R",
      "IV Le modèle linéairé généralisé",
      "15 Données déséquilibrées"
    ]
  },
  {
    "objectID": "code/chap15.html#critères-pour-données-déséquilibrées",
    "href": "code/chap15.html#critères-pour-données-déséquilibrées",
    "title": "15 Données déséquilibrées",
    "section": "Critères pour données déséquilibrées",
    "text": "Critères pour données déséquilibrées\n\n\n   P1\nY     0   1\n  0 468   0\n  1  31   1\n\n\n   P2\nY     0   1\n  0 407  61\n  1   4  28\n\n\n\nlibrary(yardstick)\ndf &lt;- data.frame(Y,P2)\nmulti_metric &lt;- metric_set(accuracy,bal_accuracy,f_meas,kap)\nmulti_metric(df,truth=Y,estimate=P2,event_level = \"second\")\n\n# A tibble: 4 × 3\n  .metric      .estimator .estimate\n  &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy     binary         0.87 \n2 bal_accuracy binary         0.872\n3 f_meas       binary         0.463\n4 kap          binary         0.407",
    "crumbs": [
      "Codes R",
      "IV Le modèle linéairé généralisé",
      "15 Données déséquilibrées"
    ]
  },
  {
    "objectID": "code/chap15.html#application-aux-données-dimages-publicitaires",
    "href": "code/chap15.html#application-aux-données-dimages-publicitaires",
    "title": "15 Données déséquilibrées",
    "section": "Application aux données d’images publicitaires",
    "text": "Application aux données d’images publicitaires\n\nsummary(ad.data1$Y)\n\n   ad. nonad. \n   459   2820 \n\nad.data1 &lt;- ad.data1 %&gt;% \n  transform(Y=fct_recode(Y,\"0\"=\"nonad.\",\"1\"=\"ad.\")) %&gt;% \n  transform(Y=fct_inseq(Y))\n\n\nset.seed(1234)\nbloc &lt;- sample(1:10,nrow(ad.data1),replace=TRUE)\ntable(bloc)\n\nbloc\n  1   2   3   4   5   6   7   8   9  10 \n309 327 329 310 358 354 304 337 342 309 \n\n\n\nscore &lt;- data.frame(matrix(0,nrow=nrow(ad.data1),ncol=3))\nnames(score) &lt;- c(\"logit\",\"lasso\",\"ridge\")\nSCORE &lt;- list(brute=score,over=score,smote=score,under=score,tomek=score)\n\n\nset.seed(4321)\nlibrary(glmnet)\nscore &lt;- data.frame(matrix(0,nrow=nrow(ad.data1),ncol=3))\nnames(score) &lt;- c(\"logit\",\"lasso\",\"ridge\")\nSCORE &lt;- list(brute=score,over=score,smote=score,under=score,tomek=score)\n\nfor (k in 1:10){\n  print(k)\n  ind.test &lt;- bloc==k\n  dapp &lt;- ad.data1[!ind.test,]\n  dtest &lt;- ad.data1[ind.test,]\n  X.test &lt;- model.matrix(Y~.,data=dtest)[,-1]\n  \n  ech.app &lt;- list(norm=dapp,\n                  over=RandOverClassif(Y~.,dat=dapp),\n                  smote=SmoteClassif(Y~.,dat=dapp),\n                  under=RandUnderClassif(Y~.,dat=dapp),\n                  tomek=TomekClassif(Y~.,dat=dapp)[[1]])\n\n  mod.mat.list &lt;- function(df){model.matrix(Y~.,data=df)[,-1]}\n  Y.list &lt;- function(df) df$Y \n  \n  X.app &lt;- lapply(ech.app,mod.mat.list)\n  Y.app &lt;- lapply(ech.app,Y.list)\n  \n  for (j in 1:5){\n    print(j)\n    lasso &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=\"binomial\")\n    ridge &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=\"binomial\",alpha=0)\n    logit &lt;- glm(Y~.,data=ech.app[[j]],family=\"binomial\")\n    SCORE[[j]][ind.test,] &lt;- data.frame(\n      logit=predict.glm(logit,newdata=dtest,type=\"response\"),\n      lasso=as.vector(predict(lasso,newx=X.test,type=\"response\")),\n      ridge=as.vector(predict(ridge,newx=X.test,type=\"response\"))\n    )\n  }\n}\n\n\nmat.score &lt;- bind_rows(brutes=SCORE[[1]],\n                       over=SCORE[[2]],\n                       smote=SCORE[[3]],\n                       under=SCORE[[4]],\n                       tomek=SCORE[[5]],.id=\"meth\") %&gt;% \n  mutate(obs=rep(ad.data1$Y,5))  %&gt;% \n  pivot_longer(c(logit,lasso,ridge),\n               names_to = \"algo\",values_to = \"score\")\n\n\nmat.score %&gt;% group_by(meth,algo) %&gt;% \n  roc_auc(truth = obs,score,event_level = \"second\") %&gt;%\n  pivot_wider(-c(.metric,.estimator),\n              names_from = algo,values_from = .estimate)\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.943 0.831 0.980\n2 over   0.973 0.790 0.977\n3 smote  0.973 0.680 0.977\n4 tomek  0.950 0.763 0.979\n5 under  0.956 0.787 0.964\n\n\n\nmat.score &lt;- mat.score %&gt;% mutate(prev=as.factor(round(score)))\n\n\nAccuracy :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  accuracy(truth = obs,prev) %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.969 0.886 0.970\n2 over   0.961 0.847 0.963\n3 smote  0.960 0.699 0.960\n4 tomek  0.969 0.818 0.970\n5 under  0.954 0.808 0.955\n\n\nBalanced accuracy :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  bal_accuracy(truth = obs,prev) %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.898 0.832 0.900\n2 over   0.931 0.806 0.935\n3 smote  0.933 0.680 0.933\n4 tomek  0.896 0.797 0.900\n5 under  0.921 0.792 0.899\n\n\nF1 score :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  f_meas(truth = obs,prev,event_level = \"second\") %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.879 0.650 0.884\n2 over   0.864 0.579 0.873\n3 smote  0.863 0.378 0.862\n4 tomek  0.876 0.542 0.882\n5 under  0.843 0.529 0.837\n\n\nKappa de Cohen :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  kap(truth = obs,prev) %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.862 0.584 0.867\n2 over   0.842 0.491 0.851\n3 smote  0.839 0.224 0.838\n4 tomek  0.859 0.440 0.866\n5 under  0.816 0.422 0.811\n\n\n\n\ngrille.score(mat.score,nom_algo=\"ridge\",meth=\"norm\")\n\n# A tibble: 11 × 7\n   seuil  sens  spec accuracy bal_accuracy f_meas   kap\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 0     1     0        0.14         0.5    0.246 0    \n 2 0.1   0.955 0.671    0.711        0.813  0.48  0.343\n 3 0.2   0.932 0.745    0.771        0.839  0.533 0.416\n 4 0.3   0.911 0.851    0.86         0.881  0.645 0.566\n 5 0.4   0.885 0.918    0.913        0.901  0.74  0.69 \n 6 0.5   0.843 0.983    0.964        0.913  0.867 0.846\n 7 0.6   0.797 0.991    0.964        0.894  0.861 0.841\n 8 0.7   0.739 0.995    0.959        0.867  0.834 0.811\n 9 0.8   0.672 0.997    0.952        0.835  0.796 0.77 \n10 0.9   0.562 0.999    0.938        0.78   0.717 0.685\n11 1     0     1        0.86         0.5   NA     0    \n\ngrille.score(mat.score,nom_algo=\"ridge\",meth=\"over\")\n\n# A tibble: 11 × 7\n   seuil  sens  spec accuracy bal_accuracy f_meas   kap\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 0     1     0        0.14         0.5    0.246 0    \n 2 0.1   0.955 0.671    0.711        0.813  0.48  0.343\n 3 0.2   0.932 0.745    0.771        0.839  0.533 0.416\n 4 0.3   0.911 0.851    0.86         0.881  0.645 0.566\n 5 0.4   0.885 0.918    0.913        0.901  0.74  0.69 \n 6 0.5   0.843 0.983    0.964        0.913  0.867 0.846\n 7 0.6   0.797 0.991    0.964        0.894  0.861 0.841\n 8 0.7   0.739 0.995    0.959        0.867  0.834 0.811\n 9 0.8   0.672 0.997    0.952        0.835  0.796 0.77 \n10 0.9   0.562 0.999    0.938        0.78   0.717 0.685\n11 1     0     1        0.86         0.5   NA     0    \n\ngrille.score(mat.score,nom_algo=\"ridge\",meth=\"smote\")\n\n# A tibble: 11 × 7\n   seuil  sens  spec accuracy bal_accuracy f_meas   kap\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 0     1     0        0.14         0.5    0.246 0    \n 2 0.1   0.955 0.671    0.711        0.813  0.48  0.343\n 3 0.2   0.932 0.745    0.771        0.839  0.533 0.416\n 4 0.3   0.911 0.851    0.86         0.881  0.645 0.566\n 5 0.4   0.885 0.918    0.913        0.901  0.74  0.69 \n 6 0.5   0.843 0.983    0.964        0.913  0.867 0.846\n 7 0.6   0.797 0.991    0.964        0.894  0.861 0.841\n 8 0.7   0.739 0.995    0.959        0.867  0.834 0.811\n 9 0.8   0.672 0.997    0.952        0.835  0.796 0.77 \n10 0.9   0.562 0.999    0.938        0.78   0.717 0.685\n11 1     0     1        0.86         0.5   NA     0",
    "crumbs": [
      "Codes R",
      "IV Le modèle linéairé généralisé",
      "15 Données déséquilibrées"
    ]
  }
]