[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Présentation",
    "section": "",
    "text": "3ème édition - Mai 2023\n\n\nLes auteurs\n\nPierre-André Cornillon\nNicolas Hengartner\nÉric Matzner-Løber\nLaurent Rouvière\n\n\n\nDescriptif\n\n4e de couverture\nAvant-propos\nSommaire détaillé\n\n\n\nBoutique en ligne\nPar ici\n\n\n\n\nRésumé\nCet ouvrage expose de manière détaillée, exemples à l’appui, différentes façons de répondre à un des problèmes statistiques les plus courants : la régression. Cette nouvelle édition se décompose en cinq parties.\n\nLa première donne les grands principes des régressions simple et multiple par moindres carrés. Les fondamentaux de la méthode, tant au niveau des choix opérés que des hypothèses et leur utilité, sont expliqués.\nLa deuxième partie est consacrée à l’inférence et présente les outils permettant de vérifier les hypothèses mises en œuvre. Les techniques d’analyse de la variance et de la covariance sont également présentées dans cette partie.\nLe cas de la grande dimension est ensuite abordé dans la troisième partie. Différentes méthodes de réduction de la dimension telles que la sélection de variables, les régressions sous contraintes (lasso, elasticnet ou ridge) et sur composantes (PLS ou PCR) sont notamment proposées. Un dernier chapitre propose des algorithmes, basés sur des méthodes de rééchantillonnage comme l’apprentissage/validation ou la validation croisée, qui permettent d’établir une comparaison entre toutes ces méthodes.\nLa quatrième partie se concentre sur les modèles linéaires généralisés et plus particulièrement sur les régressions logistique et de Poisson avec ou sans technique de régularisation. Une section particulière est consacrée aux comparaisons de méthodes en classification supervisée. Elle introduit notamment des critères de performance pour scorer des individus comme les courbes ROC et lift et propose des stratégies de choix seuil (Younden, macro F1…) pour les classer. Ces notions sont ensuite mises en œuvre sur des données réelles afin de sélectionner une méthode de prévision parmi plusieurs algorithmes basés sur des modèles logistiques (régularisés ou non). Une dernière section aborde le problème des données déséquilibrées qui est souvent rencontré en régression binaire.\nEnfin, la dernière partie présente l’approche non paramétrique à travers les splines, les estimateurs à noyau et des plus proches voisins. La présentation témoigne d’un réel souci pédagogique des auteurs qui bénéficient d’une expérience d’enseignement auprès de publics très variés. Les résultats exposés sont replacés dans la perspective de leur utilité pratique grâce à l’analyse d’exemples concrets. Les commandes permettant le traitement des exemples sous R figurent dans le corps du texte.\n\nEnfin, chaque chapitre est complété par une suite d’exercices corrigés."
  },
  {
    "objectID": "donnees.html",
    "href": "donnees.html",
    "title": "Jeux de données",
    "section": "",
    "text": "ad_data.txt\n\nartere.txt\ncourbe_lasso.csv\ndd_exo3_1.csv\ndd_exo3_2.csv\ndd_exo3_3.csv\nechan_lasso.csv\neucalyptus.txt\n\nlogit_ex6.csv\nlogit_ridge_lasso.csv\nozone.txt\nozone_complet.txt\nozone_long.txt\nozone_simple.txt\nozone_transf.txt\npanne.txt\npoissonData.csv\npoissonData3.csv"
  },
  {
    "objectID": "code/chap1.html",
    "href": "code/chap1.html",
    "title": "1 Régression simple",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=TRUE,sep=\";\")\nplot(O3~T12, data=ozone, xlab=\"T12\", ylab=\"O3\")\n\n\n\n\n\nreg &lt;- lm(O3~T12, data=ozone)\nsummary(reg)\n\n\nCall:\nlm(formula = O3 ~ T12, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-45.256 -15.326  -3.461  17.634  40.072 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.4150    13.0584   2.406     0.02 *  \nT12           2.7010     0.6266   4.311 8.04e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.5 on 48 degrees of freedom\nMultiple R-squared:  0.2791,    Adjusted R-squared:  0.2641 \nF-statistic: 18.58 on 1 and 48 DF,  p-value: 8.041e-05\n\n\n\nplot(O3~T12, data=ozone)\nT12 &lt;- seq(min(ozone[,\"T12\"]), max(ozone[,\"T12\"]), length = 100)\ngrille &lt;- data.frame(T12)\nICdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nmatlines(grille$T12, cbind(ICdte), lty = c(1,2,2), col = 1)\n\n\n\n\n\nplot(O3~T12, data = ozone, ylim = c(0,150))\nT12 &lt;- seq(min(ozone[,\"T12\"]), max(ozone[,\"T12\"]), length = 100)\ngrille &lt;- data.frame(T12)\nICdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nICprev &lt;- predict(reg, new=grille, interval=\"pred\", level=0.95)\nmatlines(T12, cbind(ICdte,ICprev[,-1]), lty=c(1,2,2,3,3), col=1)\nlegend(\"topleft\", lty=2:3, c(\"Y\",\"E(Y)\"))\n\n\n\n\n\nIC &lt;- confint(reg, level = 0.95)\nIC\n\n               2.5 %   97.5 %\n(Intercept) 5.159232 57.67071\nT12         1.441180  3.96089\n\n\n\nlibrary(ellipse)\nplot(ellipse(reg, level=0.95), type = \"l\", xlab = \"\", ylab = \"\")\npoints(coef(reg)[1], coef(reg)[2], pch = 3)\nlines(IC[1,c(1,1,2,2,1)], IC[2,c(1,2,2,1,1)], lty = 2)\n\n\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nplot(ht~circ, data = eucalypt, xlab = \"circ\", ylab = \"ht\")\n\n\n\n\n\nreg &lt;- lm(ht~circ, data = eucalypt)\nsummary(reg)\n\n\nCall:\nlm(formula = ht ~ circ, data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.7659 -0.7802  0.0557  0.8271  3.6913 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 9.037476   0.179802   50.26   &lt;2e-16 ***\ncirc        0.257138   0.003738   68.79   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.199 on 1427 degrees of freedom\nMultiple R-squared:  0.7683,    Adjusted R-squared:  0.7682 \nF-statistic:  4732 on 1 and 1427 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(ht~circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),\n                                max(eucalypt[,\"circ\"]), length = 100))\nICdte &lt;- predict(reg, new=grille, interval=\"confi\", level=0.95)\nmatlines(grille$circ, ICdte, lty = c(1,2,2), col = 1)\n\n\n\n\n\nplot(ht~circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ncirc &lt;- seq(min(eucalypt[,\"circ\"]),max(eucalypt[,\"circ\"]), len = 100)\ngrille &lt;- data.frame(circ)\nCdte &lt;- predict(reg, new=grille, interval=\"conf\", level=0.95)\nICprev &lt;- predict(reg, new=grille, interval=\"pred\", level=0.95)\nmatlines(circ, cbind(ICdte,ICprev[,-1]),lty=c(1,2,2,3,3), col=1)"
  },
  {
    "objectID": "code/chap2.html",
    "href": "code/chap2.html",
    "title": "2 La régression linéaire multiple",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nlibrary(\"scatterplot3d\")\nscatterplot3d(ozone[,\"T12\"],ozone[,\"Vx\"],ozone[,\"O3\"],\n              type=\"h\",pch=16, box=FALSE, xlab=\"T12\", ylab=\"Vx\", zlab=\"O3\")\n\n\n\n\n\nregmulti &lt;- lm(O3~T12+Vx, data = ozone)\nsummary(regmulti)\n\n\nCall:\nlm(formula = O3 ~ T12 + Vx, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.984 -10.152  -2.407  11.710  34.494 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  35.4530    10.7446   3.300  0.00185 ** \nT12           2.5380     0.5151   4.927 1.08e-05 ***\nVx            0.8736     0.1772   4.931 1.06e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.82 on 47 degrees of freedom\nMultiple R-squared:  0.5249,    Adjusted R-squared:  0.5047 \nF-statistic: 25.96 on 2 and 47 DF,  p-value: 2.541e-08\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nplot(ht~circ, data = eucalypt, xlab = \"circ\", ylab = \"ht\")\n\n\n\n\n\nregmult &lt;- lm(ht ~ circ + I(sqrt(circ)), data = eucalypt)\nresume.mult &lt;- summary(regmult)\nresume.mult\n\n\nCall:\nlm(formula = ht ~ circ + I(sqrt(circ)), data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1881 -0.6881  0.0427  0.7927  3.7481 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -24.35200    2.61444  -9.314   &lt;2e-16 ***\ncirc           -0.48295    0.05793  -8.336   &lt;2e-16 ***\nI(sqrt(circ))   9.98689    0.78033  12.798   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.136 on 1426 degrees of freedom\nMultiple R-squared:  0.7922,    Adjusted R-squared:  0.7919 \nF-statistic:  2718 on 2 and 1426 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(ht ~ circ, data = eucalypt, pch = \"+\", col = \"grey60\")\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),max(eucalypt[,\"circ\"]), length = 100))\nlines(grille[,\"circ\"], predict(regmult, grille))"
  },
  {
    "objectID": "code/chap3.html",
    "href": "code/chap3.html",
    "title": "3 Validation du modèle",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone_long.txt\", header = T, sep = \";\")\nmod.lin6v &lt;- lm(O3~T6 + T12 + Ne12 + Ne15 + Vx + O3v,data=ozone)\n\n\nplot(rstudent(mod.lin6v), pch = \".\",ylab = \"Résidus studentisés par VC\")\nabline(h = c(-2,2))\nlines(lowess(rstudent(mod.lin6v)))\n\n\n\n\n\nplot(mod.lin6v, which = 2, sub = \"\", main = \"\")\nabline(0,1)\n\n\n\n\n\nplot(cooks.distance(mod.lin6v),type=\"h\",ylab=\"Distance de Cook\")\np &lt;- ncol(ozone) ; n &lt;- nrow(ozone)\nseuil1 &lt;- qf(0.1,p,n-p) ; abline(h=seuil1)\n\n\n\ninfl.ozone &lt;- influence.measures(mod.lin6v)\nplot(infl.ozone$infmat[,\"hat\"],type=\"h\",ylab=\"hii\")\nseuil1 &lt;- 3*p/n ; abline(h=seuil1,col=1,lty=2)\nseuil2 &lt;- 2*p/n ; abline(h=seuil2,col=1,lty=3)\n\n\n\n\n\nresidpartiels &lt;- resid(mod.lin6v, type = \"partial\")\nprov &lt;- loess(residpartiels[,\"O3v\"] ~ ozone$O3v)\nordre &lt;- order(ozone$O3v)\nplot(ozone$O3v, residpartiels[,\"O3v\"], pch=\".\",ylab=\"\",xlab=\"\")\nmatlines(ozone$O3v[ordre], predict(prov)[ordre])\nabline(lsfit(ozone$O3v, residpartiels[,\"O3v\"]), lty = 2)"
  },
  {
    "objectID": "code/chap5.html",
    "href": "code/chap5.html",
    "title": "5 Inférence dans le modèle Gaussien",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\nresume3 &lt;- summary(modele3)\ncoef3 &lt;- coef(resume3)\nIC3 &lt;- t(confint(modele3, level = 0.95))\nIC3\n\n       (Intercept)       T12        Vx      Ne12\n2.5 %     57.15842 0.3138112 0.1491857 -6.960609\n97.5 %   111.93625 2.3162807 0.8237055 -2.826137\n\n\n\nlibrary(ellipse)\npar(mfrow=c(3,2))\nfor(i in 1:3){\n  for(j in (i+1):4){\n    plot(ellipse(modele3,c(i,j),level=0.95),type=\"l\",\n         xlab=paste(\"beta\",i,sep=\"\"),ylab=paste(\"beta\",j,sep=\"\"))\n    points(coef(modele3)[i], coef(modele3)[j],pch=3)\n    lines(c(IC3[1,i],IC3[1,i],IC3[2,i],IC3[2,i],IC3[1,i]),\n          c(IC3[1,j],IC3[2,j],IC3[2,j],IC3[1,j],IC3[1,j]),lty=2)\n  }\n}\n\n\n\n\n\nc(resume3$sigma^2*modele3$df.res/qchisq(0.975,modele3$df.res),\n  resume3$sigma^2*modele3$df.res/qchisq(0.025,modele3$df.res))\n\n[1] 133.6699 305.3706\n\n\n\nExemple 1 : la concentration en ozone\n\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\nresume3 &lt;- summary(modele3)\nresume3\n\n\nCall:\nlm(formula = O3 ~ T12 + Vx + Ne12, data = ozone)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.0461  -8.4824   0.7861   7.7024  28.2916 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  84.5473    13.6067   6.214 1.38e-07 ***\nT12           1.3150     0.4974   2.644  0.01117 *  \nVx            0.4864     0.1675   2.903  0.00565 ** \nNe12         -4.8934     1.0270  -4.765 1.93e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.91 on 46 degrees of freedom\nMultiple R-squared:  0.6819,    Adjusted R-squared:  0.6611 \nF-statistic: 32.87 on 3 and 46 DF,  p-value: 1.663e-11\n\n\n\nmodele2 &lt;- lm(O3 ~ T12 + Vx, data = ozone)\nanova(modele2, modele3)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ T12 + Vx\nModel 2: O3 ~ T12 + Vx + Ne12\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     47 13299.4                                  \n2     46  8904.6  1    4394.8 22.703 1.927e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nExemple 2 : la hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\nregsimple &lt;- lm(ht ~ circ, data = eucalypt)\nregM &lt;- lm(ht ~ circ + I(sqrt(circ)), data = eucalypt)\nanova(regsimple, regM)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ circ\nModel 2: ht ~ circ + I(sqrt(circ))\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1   1427 2052.1                                 \n2   1426 1840.7  1    211.43 163.8 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(regM)\n\n\nCall:\nlm(formula = ht ~ circ + I(sqrt(circ)), data = eucalypt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1881 -0.6881  0.0427  0.7927  3.7481 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -24.35200    2.61444  -9.314   &lt;2e-16 ***\ncirc           -0.48295    0.05793  -8.336   &lt;2e-16 ***\nI(sqrt(circ))   9.98689    0.78033  12.798   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.136 on 1426 degrees of freedom\nMultiple R-squared:  0.7922,    Adjusted R-squared:  0.7919 \nF-statistic:  2718 on 2 and 1426 DF,  p-value: &lt; 2.2e-16\n\n\n\ngrille &lt;- data.frame(circ = seq(min(eucalypt[,\"circ\"]),\n                                max(eucalypt[,\"circ\"]), len = 100))\nICdte &lt;- predict(regM,new=grille,interval=\"conf\",level=0.95)\nICpre &lt;- predict(regM,new=grille,interval=\"pred\",level=0.95)\nplot(ht ~ circ, data = eucalypt, pch=\"+\", col=\"grey60\")\nmatlines(grille,cbind(ICdte,ICpre[,-1]),lty=c(1,2,2,3,3),col=1)\nlegend(\"topleft\", lty=2:3, c(\"E(Y)\",\"Y\"))\n\n\n\n\n\n\nIntervalle de confiance bootstrap\n\nmodele3 &lt;- lm(O3 ~ T12 + Vx + Ne12, data = ozone)\n\n\nresume3 &lt;- summary(modele3)\nresume3$coef[,1:2]\n\n              Estimate Std. Error\n(Intercept) 84.5473326 13.6067253\nT12          1.3150459  0.4974102\nVx           0.4864456  0.1675496\nNe12        -4.8933729  1.0269960\n\n\n\nres &lt;- residuals(modele3)\nychap &lt;- predict(modele3)\nCOEFF &lt;- matrix(0, ncol = 4, nrow = 1000)\ncolnames(COEFF) &lt;- names(coef(modele3))\nozone.boot &lt;- ozone\n\n\nfor(i in 1:nrow(COEFF)){\n  resetoile &lt;- sample(res, length(res), replace=T)\n  O3etoile &lt;- ychap + resetoile\n  ozone.boot[,\"O3\"] &lt;- O3etoile\n  regboot &lt;- lm(formula(modele3), data=ozone.boot)\n  COEFF[i,] &lt;- coef(regboot)\n }\n\n\napply(COEFF, 2, quantile, probs = c(0.025,0.975))\n\n      (Intercept)       T12        Vx      Ne12\n2.5%     59.31069 0.3940245 0.2036312 -6.775764\n97.5%   108.67459 2.2270745 0.7974850 -2.900857\n\n\n\nhist(COEFF[,\"T12\"], main = \"\", xlab = \"Coefficient de T12\")"
  },
  {
    "objectID": "code/chap6.html",
    "href": "code/chap6.html",
    "title": "6 Variables qualitatives : ANCOVA et ANOVA",
    "section": "",
    "text": "La concentration en ozone\n\nozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nplot(ozone[,\"T12\"], ozone[,\"O3\"],pch=as.numeric(ozone[,\"vent\"]),\n     col = as.numeric(ozone[,\"vent\"]))\na1 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"EST\",])\na2 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"NORD\",])\na3 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"OUEST\",])\na4 &lt;- lm(O3 ~ T12, data = ozone[ozone[,\"vent\"]==\"SUD\",])\nabline(a1, col=1)\nabline(a2, col=2)\nabline(a3, col=3)\nabline(a4, col=4)\n\n\n\n\n\nmod1b &lt;- lm(formula = O3 ~ -1 + vent + T12:vent, data = ozone)\nsummary(mod1b)\n\n\nCall:\nlm(formula = O3 ~ -1 + vent + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \nventEST        45.6090    13.9343   3.273 0.002133 ** \nventNORD      106.6345    28.0341   3.804 0.000456 ***\nventOUEST      64.6840    24.6208   2.627 0.011967 *  \nventSUD       -27.0602    26.5389  -1.020 0.313737    \nventEST:T12     2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -1.6491     1.6058  -1.027 0.310327    \nventOUEST:T12   0.3407     1.2047   0.283 0.778709    \nventSUD:T12     5.3786     1.1497   4.678 3.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.9773,    Adjusted R-squared:  0.973 \nF-statistic: 226.1 on 8 and 42 DF,  p-value: &lt; 2.2e-16\n\n\n\nmod1 &lt;- lm(formula = O3 ~ vent + T12:vent, data = ozone)\nsummary(mod1)\n\n\nCall:\nlm(formula = O3 ~ vent + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    45.6090    13.9343   3.273  0.00213 ** \nventNORD       61.0255    31.3061   1.949  0.05796 .  \nventOUEST      19.0751    28.2905   0.674  0.50384    \nventSUD       -72.6691    29.9746  -2.424  0.01972 *  \nventEST:T12     2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -1.6491     1.6058  -1.027  0.31033    \nventOUEST:T12   0.3407     1.2047   0.283  0.77871    \nventSUD:T12     5.3786     1.1497   4.678 3.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6212 \nF-statistic: 12.48 on 7 and 42 DF,  p-value: 1.614e-08\n\n\n\nmod2 &lt;- lm(formula = O3 ~ vent + T12, data = ozone)\nmod2b &lt;- lm(formula = O3 ~ -1 + vent + T12, data = ozone)\nmod3 &lt;- lm(formula = O3 ~ vent:T12, data = ozone)\n\n\nanova(mod2,mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent + T12\nModel 2: O3 ~ vent + T12:vent\n  Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     45 12612.0                                \n2     42  9087.4  3    3524.5 5.4298 0.003011 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod3,mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent:T12\nModel 2: O3 ~ vent + T12:vent\n  Res.Df     RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     45 11864.1                              \n2     42  9087.4  3    2776.6 4.2776 0.01008 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nplot(rstudent(mod2) ~ fitted(mod2),xlab=\"ychap\",ylab=\"residus\")\n\n\n\n\n\nlibrary(lattice)\nxyplot(rstudent(mod2)~fitted(mod2)|vent,data = ozone, ylab=\"residus\")\n\n\n\n\n\nmod &lt;- lm(formula = O3 ~ vent + T12 + T12:vent, data = ozone)\n\n\nmod0 &lt;- lm(formula = O3 ~ vent +T12 + T12:vent, data = ozone)\nsummary(mod0)\n\n\nCall:\nlm(formula = O3 ~ vent + T12 + T12:vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.903  -9.163   1.153  10.319  32.638 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    45.6090    13.9343   3.273  0.00213 ** \nventNORD       61.0255    31.3061   1.949  0.05796 .  \nventOUEST      19.0751    28.2905   0.674  0.50384    \nventSUD       -72.6691    29.9746  -2.424  0.01972 *  \nT12             2.7480     0.6342   4.333 8.96e-05 ***\nventNORD:T12   -4.3971     1.7265  -2.547  0.01462 *  \nventOUEST:T12  -2.4073     1.3614  -1.768  0.08429 .  \nventSUD:T12     2.6306     1.3130   2.004  0.05160 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.71 on 42 degrees of freedom\nMultiple R-squared:  0.6753,    Adjusted R-squared:  0.6212 \nF-statistic: 12.48 on 7 and 42 DF,  p-value: 1.614e-08\n\n\n\n\nLa hauteur des eucalyptus\n\neucalypt &lt;- read.table(\"../donnees/eucalyptus.txt\", header = T, sep = \";\")\neucalypt[,\"bloc\"] &lt;- as.factor(eucalypt[,\"bloc\"])\nm.complet &lt;- lm(ht ~ bloc - 1 + bloc:circ, data = eucalypt)\nm.pente &lt;- lm(ht ~ bloc - 1 + circ, data = eucalypt)\nm.ordonne &lt;- lm(ht ~ bloc:circ, data = eucalypt)\nanova(m.pente, m.complet)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ bloc - 1 + circ\nModel 2: ht ~ bloc - 1 + bloc:circ\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   1425 2005.9                           \n2   1423 2005.0  2   0.84752 0.3007 0.7403\n\n\n\nanova(m.ordonne, m.complet)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ bloc:circ\nModel 2: ht ~ bloc - 1 + bloc:circ\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1   1425 2009.2                           \n2   1423 2005.0  2    4.1649 1.4779 0.2285\n\n\n\nm.simple &lt;- lm(ht ~ circ, data = eucalypt)\nanova(m.simple, m.pente)\n\nAnalysis of Variance Table\n\nModel 1: ht ~ circ\nModel 2: ht ~ bloc - 1 + circ\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   1427 2052.1                                  \n2   1425 2005.9  2    46.188 16.406 9.031e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nANOVA\n\nmod1 &lt;- lm(O3~vent-1,data=ozone)\nsummary(mod1)\n\n\nCall:\nlm(formula = O3 ~ vent - 1, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \nventEST    103.850      4.963   20.92  &lt; 2e-16 ***\nventNORD    78.289      6.618   11.83 1.49e-15 ***\nventOUEST   71.578      4.680   15.30  &lt; 2e-16 ***\nventSUD     94.343      7.504   12.57  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.9547,    Adjusted R-squared:  0.9508 \nF-statistic: 242.4 on 4 and 46 DF,  p-value: &lt; 2.2e-16\n\n\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: O3\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent       4 382244   95561  242.44 &lt; 2.2e-16 ***\nResiduals 46  18131     394                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nmod2 &lt;- lm(O3 ~ vent, data = ozone)\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: O3\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent       3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals 46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod2)\n\n\nCall:\nlm(formula = O3 ~ vent, data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  103.850      4.963  20.923  &lt; 2e-16 ***\nventNORD     -25.561      8.272  -3.090  0.00339 ** \nventOUEST    -32.272      6.821  -4.731 2.16e-05 ***\nventSUD       -9.507      8.997  -1.057  0.29616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nlm(O3 ~ C(vent,treatment), data = ozone)\n\n\nCall:\nlm(formula = O3 ~ C(vent, treatment), data = ozone)\n\nCoefficients:\n            (Intercept)   C(vent, treatment)NORD  C(vent, treatment)OUEST  \n                103.850                  -25.561                  -32.272  \n  C(vent, treatment)SUD  \n                 -9.507  \n\n\n\nlm(O3 ~ C(vent,base=2), data = ozone)\n\n\nCall:\nlm(formula = O3 ~ C(vent, base = 2), data = ozone)\n\nCoefficients:\n           (Intercept)    C(vent, base = 2)EST  C(vent, base = 2)OUEST  \n                78.289                  25.561                  -6.711  \n  C(vent, base = 2)SUD  \n                16.054  \n\n\n\nII &lt;- length(levels(as.factor(ozone$vent)))\nnI &lt;- table(ozone$vent)\ncontraste&lt;-matrix(rbind(diag(II-1),-nI[-II]/nI[II]),II,II-1)\nmod3 &lt;- lm(O3 ~ C(vent,contraste), data = ozone)\nanova(mod3)\n\nAnalysis of Variance Table\n\nResponse: O3\n                   Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nC(vent, contraste)  3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals          46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod3)\n\n\nCall:\nlm(formula = O3 ~ C(vent, contraste), data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           86.300      2.808  30.737  &lt; 2e-16 ***\nC(vent, contraste)1   17.550      4.093   4.288 9.15e-05 ***\nC(vent, contraste)2   -8.011      5.993  -1.337 0.187858    \nC(vent, contraste)3  -14.722      3.744  -3.933 0.000281 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nmod4 &lt;- lm(O3 ~ C(vent,sum), data = ozone) \nanova(mod4)\n\nAnalysis of Variance Table\n\nResponse: O3\n             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nC(vent, sum)  3  9859.8  3286.6  8.3383 0.0001556 ***\nResiduals    46 18131.4   394.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(mod4)\n\n\nCall:\nlm(formula = O3 ~ C(vent, sum), data = ozone)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.250 -13.950  -2.233  14.972  39.857 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     87.015      3.027  28.743  &lt; 2e-16 ***\nC(vent, sum)1   16.835      4.635   3.632 0.000705 ***\nC(vent, sum)2   -8.726      5.573  -1.566 0.124284    \nC(vent, sum)3  -15.437      4.485  -3.442 0.001240 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 46 degrees of freedom\nMultiple R-squared:  0.3522,    Adjusted R-squared:   0.31 \nF-statistic: 8.338 on 3 and 46 DF,  p-value: 0.0001556\n\n\n\nresid2 &lt;- resid(mod2)\nozone$vent &lt;- as.factor(ozone$vent)\nplot(resid2~vent,data=ozone, ylab=\"residus\")\n\n\n\nplot(resid2 ~ jitter(fitted(mod2)),xlab=\"ychap\",ylab=\"residus\")\n\n\n\nxyplot(resid2 ~ I(1:50)|vent, data=ozone,\n       xlab=\"index\", ylab=\"residus\")\n\n\n\n\n\npar(mfrow=c(1,2))\nwith(ozone, interaction.plot(vent, nebulosite, O3, col=1:2))\nwith(ozone, interaction.plot(nebulosite, vent, O3, col=1:4))\n\n\n\n\n\nmod1 &lt;- lm(O3 ~ vent + nebulosite + vent:nebulosite, data = ozone)\nmod2 &lt;- lm(O3 ~ vent + nebulosite, data = ozone)\nanova(mod2, mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent + nebulosite\nModel 2: O3 ~ vent + nebulosite + vent:nebulosite\n  Res.Df   RSS Df Sum of Sq     F Pr(&gt;F)\n1     45 11730                          \n2     42 11246  3    483.62 0.602 0.6173\n\n\n\nmod3 &lt;- lm(O3 ~ vent, data = ozone)\nanova(mod3, mod2)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent\nModel 2: O3 ~ vent + nebulosite\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 18131                                  \n2     45 11730  1    6401.5 24.558 1.066e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(mod3, mod2, mod1)\n\nAnalysis of Variance Table\n\nModel 1: O3 ~ vent\nModel 2: O3 ~ vent + nebulosite\nModel 3: O3 ~ vent + nebulosite + vent:nebulosite\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 18131                                  \n2     45 11730  1    6401.5 23.907 1.523e-05 ***\n3     42 11246  3     483.6  0.602    0.6173    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(mod1)\n\nAnalysis of Variance Table\n\nResponse: O3\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nvent             3  9859.8  3286.6  12.274 6.689e-06 ***\nnebulosite       1  6401.5  6401.5  23.907 1.523e-05 ***\nvent:nebulosite  3   483.6   161.2   0.602    0.6173    \nResiduals       42 11246.2   267.8                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "code/chap7.html",
    "href": "code/chap7.html",
    "title": "7 Choix de variables",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\", header = T, sep = \";\")\nlibrary(leaps)\nrecherche &lt;- regsubsets(O3 ~ T12 + T15 + Ne12 + N12 + S12 + E12 + W12 + Vx + O3v, int = T,nbest = 1, nvmax = 10, method = \"exhaustive\", data = ozone)\n\n\nplot(recherche, scale = \"bic\")\n\n\n\nplot(recherche, scale = \"Cp\")\n\n\n\nplot(recherche, scale = \"adjr2\")\n\n\n\nplot(recherche, scale = \"r2\")\n\n\n\n\n\nresume &lt;- summary(recherche)\nnomselec &lt;- colnames(resume$which)[resume$which[which.min(resume$bic),]][-1]\nformule &lt;- formula(paste(\"O3~\",paste(nomselec,collapse=\"+\")))\nmodeleslectionne &lt;- lm(formule, data = ozone)\nmodeleslectionne\n\n\nCall:\nlm(formula = formule, data = ozone)\n\nCoefficients:\n(Intercept)          T15         Ne12           Vx          O3v  \n    61.8252       1.0577      -3.9935       0.3146       0.2629"
  },
  {
    "objectID": "code/chap8.html",
    "href": "code/chap8.html",
    "title": "8 Régularisation des moindres carrés : Ridge, Lasso et elastic net",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names=1)[,-c(11:12)]\nozone.X &lt;- model.matrix(O3 ~ ., data = ozone)[,-1]\nozone.Y &lt;- ozone$O3\n\n\nlibrary(glmnet)\nridge &lt;- glmnet(ozone.X, ozone.Y, alpha = 0)\nlasso &lt;- glmnet(ozone.X, ozone.Y)#par défaut alpha=1\nen &lt;- glmnet(ozone.X, ozone.Y, alpha = 0.5)\n\n\nplot(ridge,main=\"Ridge\",ylim=c(-2,2))\n\n\n\nplot(ridge,xvar=\"lambda\",main=\"Ridge\",ylim=c(-2,2))\n\n\n\nplot(lasso,main=\"Lasso\",ylim=c(-2,2))\n\n\n\nplot(lasso,xvar=\"lambda\",main=\"Lasso\",ylim=c(-2,2))\n\n\n\nplot(en,main=\"Elastic net\",ylim=c(-2,2))\n\n\n\nplot(en,xvar=\"lambda\",main=\"Elastic net\",ylim=c(-2,2))\n\n\n\n\n\nset.seed(1234)\ncv.ridge &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 0)\ncv.lasso &lt;- cv.glmnet(ozone.X, ozone.Y) #alpha=1 par défaut\ncv.en &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 0.5)\n\n\ncv.ridge$lambda.min\n\n[1] 4.619799\n\ncv.ridge$lambda.1se\n\n[1] 29.69641\n\n\n\nplot(cv.ridge, main = \"Ridge\")\n\n\n\nplot(cv.lasso, main = \"Lasso\")\n\n\n\nplot(cv.en, main = \"Elastic net\")\n\n\n\n\n\nxnew &lt;- ozone.X[c(25,30),]\nrownames(xnew) &lt;- NULL\n\n\nxnew\n\n      T12  T15 Ne12 N12 S12 E12 W12   Vx   O3v\n[1,] 13.6 14.4    1   0   0   1   0 3.55  97.8\n[2,] 21.8 23.6    6   4   0   0   0 2.50 112.0\n\npredict(cv.ridge,newx=xnew)\n\n     lambda.1se\n[1,]   90.41122\n[2,]   90.65036\n\n\n\nozone&lt;-read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names = 1)\nozone.X &lt;- model.matrix(O3~.,data=ozone)\nozone.Y &lt;- ozone$O3\ncv.defaut &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassodefaut&lt;-glmnet(ozone.X,ozone.Y,lambda=cv.defaut$lambda.min)\n\n\nozone$vent &lt;- as.factor(ozone$vent)\nozone$vent &lt;- relevel(ozone$vent,ref=\"NORD\")\nozone.X &lt;- model.matrix(O3~.,data=ozone)\ncv.nord &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassonord &lt;- glmnet(ozone.X,ozone.Y,lambda=cv.nord$lambda.min)\n\n\npredict(lassodefaut,ozone.X[1:4,])\n\n               s0\n19960422 73.05686\n19960429 92.67573\n19960506 69.26897\n19960514 80.69977\n\npredict(lassonord,ozone.X[1:4,])\n\n               s0\n19960422 79.43741\n19960429 90.58414\n19960506 74.36556\n19960514 75.06648\n\n\n\nozone.X &lt;- model.matrix(O3~.-vent-nebulosite+C(vent,sum)+\n                          C(nebulosite,sum),data=ozone)\ncv.sum &lt;- cv.glmnet(ozone.X,ozone.Y)\nlassosum &lt;- glmnet(ozone.X,ozone.Y,lambda=cv.sum$lambda.min)\npredict(lassosum,ozone.X[1:4,])\n\n               s0\n19960422 78.05238\n19960429 89.87258\n19960506 75.07092\n19960514 73.93502"
  },
  {
    "objectID": "code/chap9.html",
    "href": "code/chap9.html",
    "title": "9 Régression sur composantes : PCR et PLS",
    "section": "",
    "text": "Régression MCO et choix de variables\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\",row.names=1)\nmodeleinit &lt;- lm(O3 ~ ., data = ozone[,1:10])\nround(coefficients(modeleinit),2)\n\n(Intercept)         T12         T15        Ne12         N12         S12 \n      54.73       -0.35        1.50       -4.19        1.28        3.17 \n        E12         W12          Vx         O3v \n       0.53        2.47        0.61        0.25 \n\nBIC(modeleinit)\n\n[1] 431.8923\n\n\n\nlibrary(leaps)\nchoix &lt;- regsubsets(O3 ~ .,nbest=1,nvmax=10,data=ozone[,1:10])\nresume &lt;- summary(choix)\nindmin &lt;- which.min(resume$bic)\nnomselec &lt;- colnames(resume$which)[resume$which[indmin,]][-1]\nformule &lt;- formula(paste(\"O3~\",paste(nomselec,collapse=\"+\")))\nmodeleBIC &lt;- lm(formule,data=ozone[,1:10])\nround(coefficients(modeleBIC),2)\n\n(Intercept)         T15        Ne12          Vx         O3v \n      61.83        1.06       -3.99        0.31        0.26 \n\nBIC(modeleBIC)\n\n[1] 415.8866\n\n\n\n\nMise en place des données centrées réduites\n\nX &lt;- ozone[,2:10]\nXbar  &lt;- apply(X, 2, mean)\nstdX &lt;- sqrt(apply(X, 2, var))\nXcr &lt;- scale(X, center = Xbar, scale = stdX)\n\n\n\nPCR\n\nlibrary(pls)\nset.seed(87)\ncvseg &lt;- cvsegments(nrow(ozone), k = 4, type = \"random\")\nn.app &lt;- nrow(ozone)\nmodele.pcr &lt;- pcr(O3 ~ ., ncomp=9, data=ozone[,1:10], scale=T,\n                   validation = \"CV\", segments = cvseg)\nmsepcv.pcr &lt;- MSEP(modele.pcr ,estimate=c(\"train\",\"CV\")) \nmsepcv.pcr\n\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\ntrain        559.8    188.7    186.5    185.2    179.8    158.8    152.0\nCV           582.9    260.4    260.6    278.2    271.3    239.3    248.1\n       7 comps  8 comps  9 comps\ntrain    143.9    142.5    139.7\nCV       242.1    244.0    239.4\n\n\n\nnpcr &lt;- which.min(msepcv.pcr$val[\"CV\",,])-1\nmodele.pcr.fin &lt;- pcr(O3 ~ ., ncomp = npcr, scale = TRUE,data = ozone[,1:10])\n\n\nX &lt;- ozone[,2:10]\nY &lt;- ozone[,1]\nn &lt;- nrow(X)\nXbar  &lt;- apply(X,2,mean)\nstdX &lt;- sqrt(apply(X,2,var)*(n-1)/n)\nYbar &lt;- mean(Y)\nmodele.pcr.fin &lt;- pcr(O3~.,ncomp=1,scale=TRUE,data =ozone[,1:10])\nbetafinpcr &lt;- matrix(coefficients(modele.pcr.fin),ncol=1)/stdX\nmu &lt;- mean(Y)-Xbar%*%betafinpcr\nc(mu,betafinpcr)\n\n [1] 48.3373569  0.7884931  0.8236705 -1.7618991 -0.6760911  0.2126764\n [7]  1.6099503 -1.4694047  0.3118176  0.1662505\n\n\n\n\nPLS\n\nset.seed(87)\ncvseg &lt;- cvsegments(nrow(ozone), k = 4, type = \"random\")\nn.app &lt;- nrow(ozone)\nmodele.pls &lt;- plsr(O3 ~ ., ncomp=9, data = ozone[,1:10], scale=T,validation = \"CV\", segments = cvseg)\nmsepcv.pls &lt;- MSEP(modele.pls ,estimate=c(\"train\",\"CV\")) \nmsepcv.pls\n\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\ntrain        559.8    173.9    150.9    146.9    144.2    142.1    141.4\nCV           582.9    251.9    248.4    245.3    234.6    241.7    243.6\n       7 comps  8 comps  9 comps\ntrain    140.9    139.8    139.7\nCV       234.7    239.6    239.4\n\n\n\nnpls &lt;- which.min(msepcv.pls$val[\"CV\",,])-1\nmodele.pls.fin &lt;- plsr(O3~ . , ncomp  =npls, scale = TRUE,data = ozone[,1:10])\n\n\nX &lt;- ozone[,2:10]\nY &lt;- ozone[,1]\nn &lt;- nrow(X)\nXbar  &lt;- apply(X,2,mean)\nstdX &lt;- sqrt(apply(X,2,var)*(n-1)/n)\nYbar &lt;- mean(Y)\nmodele.pls.fin &lt;- plsr(O3~.,ncomp=1,scale=TRUE,data =ozone[,1:10])\nbetafinpls &lt;- matrix(coefficients(modele.pls.fin),ncol=1)/stdX\nmu &lt;- mean(Y)-Xbar%*%betafinpls\nc(mu,betafinpls)\n\n [1] 47.1060475  0.8151834  0.8471154 -2.1910455 -0.5175855  0.3563688\n [7]  1.3005636 -1.2552108  0.2805513  0.1920499"
  },
  {
    "objectID": "code/chap10.html",
    "href": "code/chap10.html",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "",
    "text": "Importation de l’ozone\n\nozone &lt;- read.table(\"../donnees/ozone_complet.txt\", header = T, sep = \";\")\ndim(ozone)\n\n[1] 1464   23\n\n\nÉlimination des individus avec une valeur manquante\n\nindNA &lt;- which(is.na(ozone), arr.ind = T)[,1]\nozone2 &lt;- ozone[-indNA,]\n\nImportation du fichier d’ozone sans valeurs manquantes avec les projections\n\nozone &lt;- read.table(\"../donnees/ozone_transf.txt\", header = T, sep = \";\")\n\net préparation du data-frame qui contiendra les résultats de chaque méthode\n\nRES &lt;- data.frame(Y = ozone$maxO3)\n\nPour le moment il ne contient qu’une seule colonne avec les données à prévoir."
  },
  {
    "objectID": "code/chap10.html#régression-multiple",
    "href": "code/chap10.html#régression-multiple",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régression multiple",
    "text": "Régression multiple\nChargement du package pour la sélection de variables\n\nlibrary(leaps)\n\nEvaluation de la qualité prédictive de la régression linéaire et de la sélection de variables via BIC (algorithme exhaustif)\n\nfor(i in 1:nbbloc){\n  ###MCO global\n  reg &lt;- lm(maxO3~.,data=ozone[bloc!=i,])\n  RES[bloc==i,\"MCO\"] &lt;- predict(reg,ozone[bloc==i,])\n  ###MCO choix\n  recherche &lt;- regsubsets(maxO3~., int=T, nbest=1, nvmax=22, \n                                        data=ozone[bloc!=i,])\n  resume &lt;- summary(recherche)\n  nomselec &lt;- colnames(resume$which)[\n                       resume$which[which.min(resume$bic),] ][-1]\n  formule &lt;- formula(paste(\"maxO3~\",paste(nomselec,collapse=\"+\")))\n  regbic &lt;- lm(formule,data=ozone[bloc!=i,])\n  RES[bloc==i,\"choix\"] &lt;- predict(regbic,ozone[bloc==i,])\n}"
  },
  {
    "objectID": "code/chap10.html#lasso-ridge-et-elasticnet",
    "href": "code/chap10.html#lasso-ridge-et-elasticnet",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Lasso, ridge et elasticnet",
    "text": "Lasso, ridge et elasticnet\nChargement du package pour lasso, ridge et elasticnet et création des matrices nécessaires à son utilisation :\n\nlibrary(glmnet)\nozone.X &lt;- model.matrix(maxO3~.,data=ozone)[,-1]\nozone.Y &lt;- ozone[,\"maxO3\"]\n\nÉvaluation de la qualité prédictive des régressions lasso, ridge et elasticnet:\n\nfor(i in 1:nbbloc){  \n  XA &lt;- ozone.X[bloc!=i,]\n  YA &lt;- ozone.Y[bloc!=i]\n  XT &lt;- ozone.X[bloc==i,]\n  ###ridge\n  tmp &lt;- cv.glmnet(XA,YA,alpha=0)\n  mod &lt;- glmnet(XA,YA,alpha=0,lambda=tmp$lambda.min)\n  RES[bloc==i,\"ridge\"] &lt;- predict(mod,XT)\n  ###lasso\n  tmp &lt;- cv.glmnet(XA,YA,alpha=1)\n  mod &lt;- glmnet(XA,YA,alpha=0,lambda=tmp$lambda.min)\n  RES[bloc==i,\"lasso\"] &lt;- predict(mod,XT)\n  ###elastic\n  tmp &lt;- cv.glmnet(XA,YA,alpha=0.5)\n  mod &lt;- glmnet(XA,YA,alpha=.5,lambda=tmp$lambda.min)\n  RES[bloc==i,\"elastic\"] &lt;- predict(mod,XT)\n}"
  },
  {
    "objectID": "code/chap10.html#régressions-sur-composantes",
    "href": "code/chap10.html#régressions-sur-composantes",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régressions sur composantes",
    "text": "Régressions sur composantes\nChargement du package pour les régressions sur composantes\n\nlibrary(pls)\n\nÉvaluation de la qualité prédictive des régressions PCR et PLS\n\nfor(i in 1:nbbloc){\n   #####PLS\n   tmp &lt;- plsr(maxO3~.,data=ozone[bloc!=i,],ncomp=20,\n                                 validation=\"CV\",scale=TRUE)\n   mse &lt;- MSEP(tmp,estimate=c(\"train\",\"CV\"))\n   npls &lt;- which.min(mse$val[\"CV\",,])-1 \n   mod &lt;- plsr(maxO3~.,ncomp=npls,data=ozone[bloc!=i,],scale=TRUE)\n   RES[bloc==i,\"PLS\"] &lt;- predict(mod,ozone[bloc==i,],ncomp=npls)\n   #####PCR\n   tmp &lt;- pcr(maxO3~.,data=ozone[bloc!=i,],ncomp=20,\n                                    validation=\"CV\",scale=TRUE)\n   mse &lt;- MSEP(tmp,estimate=c(\"train\",\"CV\"))\n   npcr &lt;- which.min(mse$val[\"CV\",,])-1 \n   mod &lt;- pcr(maxO3~.,ncomp=npcr,data=ozone[bloc!=i,],scale=TRUE)\n   RES[bloc==i,\"PCR\"] &lt;- predict(mod,ozone[bloc==i,],ncomp=npcr)\n }\n\nLes résultats :\n\nRES |&gt; \n  dplyr::summarise(across(-Y,~mean((Y-.)^2)))\n\n       MCO    choix    ridge    lasso  elastic      PLS      PCR\n1 187.2726 188.8491 187.8304 187.1023 187.0389 187.2622 187.2685"
  },
  {
    "objectID": "code/chap10.html#régression-linéaire",
    "href": "code/chap10.html#régression-linéaire",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Régression linéaire",
    "text": "Régression linéaire\nLa fonction\n\nsse_reg &lt;- function(don,bloc,b) {\n    m_reg &lt;- lm(maxO3~.,data=don[bloc!=b,])\n    previsions &lt;- predict(m_reg,don[bloc==b,])\n    return(sum((don[bloc==b,\"maxO3\"]-previsions)^2))\n}\n\nLa qualité de la modélisation\n\nset.seed(1234)\nssereg  &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n    ssereg[r] &lt;- ssereg[r] + sse_reg(ozone,bloc,b)\n  }\n}\nres_rep$MCO &lt;- round(mean(ssereg/nrow(ozone)),2)"
  },
  {
    "objectID": "code/chap10.html#choix-de-variables",
    "href": "code/chap10.html#choix-de-variables",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Choix de variables",
    "text": "Choix de variables\nLa fonction\n\nlibrary(leaps)\nsse_regbic &lt;- function(don,bloc,b,nvmax,method) {\n    recherche &lt;- regsubsets(maxO3~., int=T, nbest=1,data=don[bloc!=b,],\n                           nvmax=nvmax,method=method)\n    resume &lt;- summary(recherche)\n    nomselec &lt;- colnames(resume$which)[resume$which[which.min(resume$bic),]][-1]\n    formule &lt;- formula(paste(\"maxO3 ~\", paste(nomselec, collapse = \"+\")))\n    m_reg &lt;- lm(formule,data=don[bloc!=b,])\n    previsions &lt;- predict(m_reg,don[bloc==b,])\n    return(sum((don[bloc==b,\"maxO3\"]-previsions)^2))\n}\n\nLa qualité de la modélisation\n\nset.seed(1234)\nsseregbic &lt;-  rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n    sseregbic[r] &lt;- sseregbic[r] + sse_regbic(ozone,bloc,b,22,\"exhaustive\")\n  }\n}\nres_rep$choix &lt;- mean(sseregbic/nrow(ozone))"
  },
  {
    "objectID": "code/chap10.html#lasso",
    "href": "code/chap10.html#lasso",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Lasso",
    "text": "Lasso\nLa fonction\n\nlibrary(glmnet)\nsse_glmnet &lt;- function(X,Y,bloc,b,a) {\n  rech &lt;- cv.glmnet(X[bloc!=b,], Y[bloc!=b,drop=FALSE], alpha=a)\n  prev &lt;- predict(rech, newx=X[bloc==b,], s=rech$lambda.min)\n  return(sum((Y[bloc==b,\"maxO3\"] - as.vector(prev))^2))\n}\n\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsselasso &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sselasso[r] &lt;- sselasso[r] + sse_glmnet(X,Y,bloc,b,a=1)\n  }\n}\nres_rep$lasso &lt;- round(mean(sselasso/nrow(ozone)),2)"
  },
  {
    "objectID": "code/chap10.html#ridge",
    "href": "code/chap10.html#ridge",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "RIDGE",
    "text": "RIDGE\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsseridge &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sseridge[r] &lt;- sseridge[r] + sse_glmnet(X,Y,bloc,b,a=0)\n  }\n}\nres_rep$ridge &lt;- mean(sseridge/nrow(ozone))"
  },
  {
    "objectID": "code/chap10.html#elastic-net",
    "href": "code/chap10.html#elastic-net",
    "title": "10 Comparaison des différentes méthodes, étude de cas réels",
    "section": "Elastic-net",
    "text": "Elastic-net\nLa qualité de la modélisation\n\nX &lt;-  model.matrix(maxO3~.,data=ozone)[,-1]\nY &lt;- data.matrix(ozone[,\"maxO3\",drop=FALSE])\nset.seed(1234)\nsseelasticnet &lt;- rep(0,20)\nfor (r in 1:20) {\n  bloc &lt;- sample(blocseq)\n  for(b in 1:nbbloc){\n      sseelasticnet[r] &lt;- sseelasticnet[r] + sse_glmnet(X,Y,bloc,b,a=0.5)\n  }\n}\nres_rep$elastic &lt;- mean(sseelasticnet/nrow(ozone))\n\nLes résultats\n\nres_rep\n\n     MCO    choix    ridge  lasso  elastic\n1 188.36 189.6025 188.6624 187.92 187.8242"
  },
  {
    "objectID": "code/chap11.html",
    "href": "code/chap11.html",
    "title": "11 Régression logistique",
    "section": "",
    "text": "Présentation du modèle\n\nartere &lt;- read.table(\"../donnees/artere.txt\",header=T)\nplot(chd~age,data=artere,pch=16)\n\n\n\n\n\ntab_freq &lt;- table(artere$agrp,artere$chd)\nfreq &lt;- tab_freq[,2]/apply(tab_freq,1,sum)\ncbind(tab_freq,round(freq,3))\n\n   0  1      \n1  9  1 0.100\n2 13  2 0.133\n3  9  3 0.250\n4 10  5 0.333\n5  7  6 0.462\n6  3  5 0.625\n7  4 13 0.765\n8  2  8 0.800\n\nx.age &lt;- c(19,29,34,39,44,49,54,59)\nplot(x.age,c(freq),type=\"s\",xlim=c(18,80),ylim=c(0,1),xlab=\"âge\",ylab=\"freq\")\nlines(c(59,80),rep(freq[length(freq)],2))\nx &lt;- seq(15,80,by=0.01)\ny &lt;- exp(-5.31+0.11*x)/(1+exp(-5.31+0.11*x))\nlines(x,y,lty=3)\n\n\n\n\n\nglm(chd~age,data=artere,family=binomial)\n\n\nCall:  glm(formula = chd ~ age, family = binomial, data = artere)\n\nCoefficients:\n(Intercept)          age  \n    -5.3095       0.1109  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      136.7 \nResidual Deviance: 107.4    AIC: 111.4\n\n\n\nset.seed(12345)\nX &lt;- factor(sample(c(\"A\",\"B\",\"C\"),100,replace=T))\n#levels(X) &lt;- c(\"A\",\"B\",\"C\")\nY &lt;- rep(0,100)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),1,0.9)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),1,0.1)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),1,0.9)\ndonnees &lt;- data.frame(X,Y)\nmodel &lt;- glm(Y~.,data=donnees,family=binomial)\ncoef(model)\n\n(Intercept)          XB          XC \n  2.0794415  -3.6549779   0.3772942 \n\nmodel1 &lt;- glm(Y~C(X,sum),data=donnees,family=binomial)\ncoef(model1)\n\n(Intercept)  C(X, sum)1  C(X, sum)2 \n  0.9868803   1.0925612  -2.5624167 \n\n\n\n\nIntervalles de confiance et tests\n\nlibrary(bestglm)\ndata(SAheart)\nnew.SAheart &lt;- SAheart[c(2,408,35),]\nrow.names(new.SAheart) &lt;- NULL\nSAheart &lt;- SAheart[-c(2,408,35),]\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nround(summary(model)$coefficients,4)\n\n               Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     -6.0837     1.3141 -4.6294   0.0000\nsbp              0.0065     0.0058  1.1268   0.2598\ntobacco          0.0814     0.0269  3.0232   0.0025\nldl              0.1794     0.0600  2.9891   0.0028\nadiposity        0.0184     0.0295  0.6224   0.5337\nfamhistPresent   0.9325     0.2291  4.0694   0.0000\ntypea            0.0392     0.0123  3.1845   0.0015\nobesity         -0.0637     0.0446 -1.4300   0.1527\nalcohol          0.0002     0.0045  0.0346   0.9724\nage              0.0439     0.0122  3.5923   0.0003\n\nconfint.default(model)\n\n                      2.5 %       97.5 %\n(Intercept)    -8.659355064 -3.507983650\nsbp            -0.004797560  0.017773140\ntobacco         0.028628110  0.134174033\nldl             0.061770934  0.297043348\nadiposity      -0.039461469  0.076187468\nfamhistPresent  0.483354369  1.381572764\ntypea           0.015090098  0.063396115\nobesity        -0.151054913  0.023612482\nalcohol        -0.008639577  0.008950096\nage             0.019931097  0.067793230\n\n\n\nn &lt;- 1000\nset.seed(123)\nX1 &lt;- sample(c(\"A\",\"B\",\"C\"),n,replace=TRUE)\nX2 &lt;- rnorm(n)\nX3 &lt;- runif(n)\ncl &lt;- 1+0*(X1==\"A\")+1*(X1==\"B\")-3*(X1==\"C\")+2*X2\nY &lt;- rbinom(n,1,exp(cl)/(1+exp(cl)))\ndonnees &lt;- data.frame(X1,X2,X3,Y)\n\n\nm1 &lt;- glm(Y~.,data=donnees,family=binomial)\nlibrary(car)\nAnova(m1,type=3,test.statistic=\"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n            Df    Chisq Pr(&gt;Chisq)    \n(Intercept)  1  28.4698  9.517e-08 ***\nX1           2 212.5061  &lt; 2.2e-16 ***\nX2           1 210.3902  &lt; 2.2e-16 ***\nX3           1   0.3096     0.5779    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(m1,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n   LR Chisq Df Pr(&gt;Chisq)    \nX1   376.74  2     &lt;2e-16 ***\nX2   417.66  1     &lt;2e-16 ***\nX3     0.31  1     0.5778    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm01 &lt;- glm(Y~X2+X3,data=donnees,family=binomial)\nm02 &lt;- glm(Y~X1+X3,data=donnees,family=binomial)\nm03 &lt;- glm(Y~X1+X2,data=donnees,family=binomial)\nanova(m01,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X2 + X3\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       997    1109.67                          \n2       995     732.93  2   376.74 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m02,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X1 + X3\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       996    1150.59                          \n2       995     732.93  1   417.66 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(m03,m1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Y ~ X1 + X2\nModel 2: Y ~ X1 + X2 + X3\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       996     733.24                     \n2       995     732.93  1  0.30976   0.5778\n\nlibrary(aod)\nwald.test(Sigma=vcov(m1),b=coef(m1),Terms=c(2,3))\n\nWald test:\n----------\n\nChi-squared test:\nX2 = 212.5, df = 2, P(&gt; X2) = 0.0\n\n\n\n\nPrévisions\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\n\n\nnew.SAheart &lt;- SAheart[c(2,408,35),-10]\nrow.names(new.SAheart) &lt;- NULL\nnew.SAheart\n\n  sbp tobacco  ldl adiposity famhist typea obesity alcohol age\n1 118    0.08 3.48     32.28 Present    52   29.14    3.81  46\n2 178   20.00 9.78     33.55  Absent    37   27.29    2.88  62\n3 140    3.90 7.32     25.05  Absent    47   27.36   36.77  32\n\n\n\npredict(model, newdata=new.SAheart)\n\n         1          2          3 \n-0.9599837  1.5028033 -1.5743496 \n\n\n\npredict(model, newdata=new.SAheart,type=\"response\")\n\n        1         2         3 \n0.2768815 0.8179922 0.1715972 \n\n\n\nprev &lt;- predict(model,newdata=new.SAheart,type=\"link\",se.fit = TRUE)\ncl_inf &lt;- prev$fit-qnorm(0.975)*prev$se.fit\ncl_sup &lt;- prev$fit+qnorm(0.975)*prev$se.fit\nbinf &lt;- exp(cl_inf)/(1+exp(cl_inf))\nbsup &lt;- exp(cl_sup)/(1+exp(cl_sup))\ndata.frame(binf,bsup)\n\n       binf      bsup\n1 0.1774323 0.4046504\n2 0.6040315 0.9297800\n3 0.1024782 0.2731474\n\n\n\nunique(artere[,\"age\"])\n\n [1] 20 23 24 25 26 28 29 30 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n[26] 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 69\n\nsature &lt;- aggregate(artere[,\"chd\"],by=list(artere$age),FUN=mean)\nnames(sature) &lt;- c(\"age\",\"p\")\nndesign &lt;- aggregate(artere[,\"chd\"],by=list(artere$age),FUN=length)\nnames(ndesign) &lt;- c(\"age\",\"n\")\nmerge(sature,ndesign,by=\"age\")[1:5,]\n\n  age   p n\n1  20 0.0 1\n2  23 0.0 1\n3  24 0.0 1\n4  25 0.5 2\n5  26 0.0 2\n\nplot(chd~age,data=artere,pch=15+chd,col=chd+1)\nlines(p~age,data=sature)\n\n\n\n\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nlibrary(generalhoslem)\nlogitgof(obs= SAheart$chd, exp = fitted(model))\n\n\n    Hosmer and Lemeshow test (binary model)\n\ndata:  SAheart$chd, fitted(model)\nX-squared = 6.6586, df = 8, p-value = 0.5739\n\n\n\nmodel &lt;- glm(chd~.,data=SAheart,family=binomial)\nprev_lin &lt;- predict(model)\nres_P &lt;- residuals(model,type=\"pearson\") #Pearson\nres_PS &lt;- rstandard(model,type=\"pearson\") #Pearson standard\nres_D &lt;- residuals(model,type=\"deviance\")  #Deviance\nres_DS &lt;- rstandard(model,type=\"deviance\") #Deviance standard\n\n\npar(mfrow=c(2,2),pch=20,mai = c(0.1,0.15,0.1,0.1),mar=c(3,3,1,1),cex.axis=0.6,cex.lab=0.7,mgp=c(1.5,0.3,0),oma=c(1,0,0,0),tcl=-0.4)\nplot(res_PS,cex=0.3,xlab=\"index\",ylab=\"Pearson Standard\")\nplot(prev_lin,cex=0.3,res_PS,xlab=\"Prevision lineaire\",ylab=\"Pearson Standard\")\nplot(res_DS,cex=0.3,xlab=\"index\",ylab=\"Deviance Standard\")\nplot(prev_lin,cex=0.3,res_DS,xlab=\"Prevision lineaire\",ylab=\"Deviance Standard\")\n\n\n\n\n\n\nChoix de variables\n\nmodel0 &lt;- glm(chd~sbp+ldl,data=SAheart,family=binomial)\nmodel1 &lt;- glm(chd~sbp+ldl+famhist+alcohol,data=SAheart,family=binomial)\nanova(model0,model1,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: chd ~ sbp + ldl\nModel 2: chd ~ sbp + ldl + famhist + alcohol\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       456     548.18                          \n2       454     522.64  2   25.545 2.838e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ndata(SAheart)\nmod_sel &lt;- bestglm(SAheart,family=binomial,IC=\"BIC\")\nmod_sel$BestModels\n\n    sbp tobacco   ldl adiposity famhist typea obesity alcohol  age Criterion\n1 FALSE    TRUE  TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  506.3634\n2 FALSE    TRUE FALSE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  509.2566\n3 FALSE    TRUE  TRUE     FALSE    TRUE FALSE   FALSE   FALSE TRUE  509.9861\n4 FALSE   FALSE  TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  510.5745\n5 FALSE    TRUE  TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  510.7933\n\nmod_sel1 &lt;- bestglm(SAheart,family=binomial,IC=\"AIC\")\nmod_sel1$BestModels\n\n    sbp tobacco  ldl adiposity famhist typea obesity alcohol  age Criterion\n1 FALSE    TRUE TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  485.6856\n2 FALSE    TRUE TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  485.9799\n3  TRUE    TRUE TRUE     FALSE    TRUE  TRUE    TRUE   FALSE TRUE  486.5490\n4  TRUE    TRUE TRUE     FALSE    TRUE  TRUE   FALSE   FALSE TRUE  486.6548\n5 FALSE    TRUE TRUE      TRUE    TRUE  TRUE   FALSE   FALSE TRUE  487.4435\n\n\n\n\nScoring\n\nset.seed(1234)\nind.app &lt;- sample(nrow(SAheart),300)\ndapp &lt;- SAheart[ind.app,]\ndval &lt;- SAheart[-ind.app,]\n#Construction des modeles\nmodel1 &lt;- glm(chd~tobacco+famhist,data=dapp,family=binomial)\nmodel2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,\n                data=dapp,family=binomial)  \nround(coef(model1),3)\n\n   (Intercept)        tobacco famhistPresent \n        -1.784          0.140          1.095 \n\nround(coef(model2),3)\n\n   (Intercept)        tobacco famhistPresent      adiposity        alcohol \n        -3.180          0.117          1.022          0.059         -0.002 \n\nprev1 &lt;- round(predict(model1,newdata=dval,type=\"response\"))\nprev2 &lt;- round(predict(model2,newdata=dval,type=\"response\"))\nmean(prev1!=dval$chd)\n\n[1] 0.3395062\n\nmean(prev2!=dval$chd)\n\n[1] 0.3395062\n\n\n\nset.seed(1245)\nbloc &lt;- sample(1:10,nrow(SAheart),replace=TRUE)\ntable(bloc)\n\nbloc\n 1  2  3  4  5  6  7  8  9 10 \n52 39 44 62 49 36 47 38 53 42 \n\n\n\nprev &lt;- data.frame(matrix(0,nrow=nrow(SAheart),ncol=2))\nnames(prev) &lt;- c(\"model1\",\"model2\")\nfor (k in 1:10){\n  ind.val &lt;- bloc==k\n  dapp.k &lt;- SAheart[!ind.val,]\n  dval.k &lt;- SAheart[ind.val,]\n  model1 &lt;- glm(chd~tobacco+famhist,data=dapp.k,family=binomial)\n  model2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,data=dapp.k,family=binomial)  \n  prev[ind.val,1] &lt;- round(predict(model1,newdata=dval.k,type=\"response\"))\n  prev[ind.val,2] &lt;- round(predict(model2,newdata=dval.k,type=\"response\"))\n}\napply(sweep(prev,1,SAheart$chd,FUN=\"!=\"),2,mean)\n\n   model1    model2 \n0.3203463 0.3073593 \n\n\n\nscore1 &lt;- predict(model1,newdata=dval)\nscore2 &lt;- predict(model2,newdata=dval)\n\n\nlibrary(pROC)\nR1 &lt;- roc(dval$chd,score1)\nR2 &lt;- roc(dval$chd,score2)\nplot(R1,lwd=3,legacy.axes=TRUE)\nplot(R2,lwd=3,col=\"red\",lty=2,legacy.axes=TRUE,add=TRUE)\ncouleur &lt;- c(\"black\",\"red\")\nlegend(\"bottomright\",legend=c(\"score1\",\"score2\"),col=couleur,lty=1:2,lwd=2,cex=0.75)\n\n\n\n\n\nauc(R1)\n\nArea under the curve: 0.7356\n\nauc(R2)\n\nArea under the curve: 0.7372\n\n\n\nscore &lt;- data.frame(matrix(0,nrow=nrow(SAheart),ncol=2))\nnames(score) &lt;- c(\"score1\",\"score2\")\nfor (k in 1:10){\n  ind.val &lt;- bloc==k\n  dapp.k &lt;- SAheart[!ind.val,]\n  dval.k &lt;- SAheart[ind.val,]\n  model1 &lt;- glm(chd~tobacco+famhist,data=dapp.k,family=binomial)\n  model2 &lt;- glm(chd~tobacco+famhist+adiposity+alcohol,data=dapp.k,family=binomial)  \n  score[ind.val,1] &lt;- predict(model1,newdata=dval.k)\n  score[ind.val,2] &lt;- predict(model2,newdata=dval.k)\n}\n\n\nscore$obs &lt;- SAheart$chd\nroc.cv &lt;- roc(obs~score1+score2,data=score)\ncouleur &lt;- c(\"black\",\"red\")\nmapply(plot,roc.cv,col=couleur,lty=1:2,add=c(F,T),lwd=3,legacy.axes=TRUE)\n\n                   score1      score2     \npercent            FALSE       FALSE      \nsensitivities      numeric,356 numeric,463\nspecificities      numeric,356 numeric,463\nthresholds         numeric,356 numeric,463\ndirection          \"&lt;\"         \"&lt;\"        \ncases              numeric,160 numeric,160\ncontrols           numeric,302 numeric,302\nfun.sesp           ?           ?          \nauc                0.7159872   0.7271937  \ncall               expression  expression \noriginal.predictor numeric,462 numeric,462\noriginal.response  integer,462 integer,462\npredictor          numeric,462 numeric,462\nresponse           integer,462 integer,462\nlevels             character,2 character,2\npredictor.name     \"score1\"    \"score2\"   \nresponse.name      \"obs\"       \"obs\"      \n\nlegend(\"bottomright\",legend=c(\"score1\",\"score2\"),col=couleur,lty=1:2,lwd=2,cex=0.75)\n\n\n\n\n\nsort(round(unlist(lapply(roc.cv,auc)),3),decreasing=TRUE)\n\nscore2 score1 \n 0.727  0.716"
  },
  {
    "objectID": "code/chap12.html",
    "href": "code/chap12.html",
    "title": "12 Régression de Poisson",
    "section": "",
    "text": "Le modèle de Poisson\n\nMalaria &lt;- read.table(\"../donnees/poissonData3.csv\", sep=\",\", header=T)\nsummary(Malaria)\n\n     Sexe                Age            Altitude     Prevention       \n Length:1627        Min.   :  10.0   Min.   :1129   Length:1627       \n Class :character   1st Qu.: 220.0   1st Qu.:1266   Class :character  \n Mode  :character   Median : 361.0   Median :1298   Mode  :character  \n                    Mean   : 419.4   Mean   :1295                     \n                    3rd Qu.: 555.0   3rd Qu.:1320                     \n                    Max.   :1499.0   Max.   :1515                     \n                                     NA's   :105                      \n     Duree          N.malaria     \n Min.   :   0.0   Min.   : 0.000  \n 1st Qu.: 172.0   1st Qu.: 1.000  \n Median : 721.0   Median : 4.000  \n Mean   : 619.3   Mean   : 4.687  \n 3rd Qu.:1011.0   3rd Qu.: 7.000  \n Max.   :1464.0   Max.   :26.000  \n                                  \n\n\n\nmodP &lt;- glm(N.malaria ~ Duree, data = Malaria, family = poisson)\nmodP\n\n\nCall:  glm(formula = N.malaria ~ Duree, family = poisson, data = Malaria)\n\nCoefficients:\n(Intercept)        Duree  \n   0.429459     0.001508  \n\nDegrees of Freedom: 1626 Total (i.e. Null);  1625 Residual\nNull Deviance:      5710 \nResidual Deviance: 3325     AIC: 8125\n\n\n\nplot(N.malaria ~ Duree, data = Malaria,pch=20,cex=0.5)\nmod.lin &lt;- lm(N.malaria ~ Duree, data = Malaria)\nabline(a=coef(mod.lin)[1],b=coef(mod.lin)[2],lwd=2)\nx &lt;- seq(0,1500,by=1)\ny &lt;- exp(coef(modP)[1]+coef(modP)[2]*x)\nlines(x,y,col=\"red\",lty=2,lwd=2.5)\n\n\n\n\n\nmodP3 &lt;- glm( N.malaria ~ Duree + Sexe + Prevention, \n              data = Malaria,family = poisson )\n\n\n\nTests et intervalles de confiance\n\nMalaria$Prevention &lt;- as.factor(Malaria$Prevention)\nMalaria$Prevention &lt;- relevel(Malaria$Prevention,ref=\"Rien\")\nmodP3 &lt;- glm( N.malaria ~ Duree + Sexe + Prevention, data = Malaria,\n             family = poisson )\nsummary(modP3)\n\n\nCall:\nglm(formula = N.malaria ~ Duree + Sexe + Prevention, family = poisson, \n    data = Malaria)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                0.3878929  0.0389800   9.951   &lt;2e-16 ***\nDuree                      0.0015101  0.0000343  44.031   &lt;2e-16 ***\nSexeM                      0.0550890  0.0229690   2.398   0.0165 *  \nPreventionAutre           -0.2255828  0.1781379  -1.266   0.2054    \nPreventionMoustiquaire     0.0176850  0.0255967   0.691   0.4896    \nPreventionSerpentin/Spray  0.0196420  0.0590690   0.333   0.7395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5710.4  on 1626  degrees of freedom\nResidual deviance: 3317.3  on 1621  degrees of freedom\nAIC: 8124.6\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nmodP2 &lt;- glm( N.malaria ~ Duree + Sexe, data = Malaria, family = poisson)\n-2*(logLik(modP2)-logLik(modP3))\n\n'log Lik.' 2.448823 (df=3)\n\nqchisq(0.95,df=3)\n\n[1] 7.814728\n\n\n\nanova(modP2,modP3,test=\"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: N.malaria ~ Duree + Sexe\nModel 2: N.malaria ~ Duree + Sexe + Prevention\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1      1624     3319.8                     \n2      1621     3317.3  3   2.4488   0.4846\n\n\n\nlibrary(car)\nAnova(modP2,test=\"LR\")\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: N.malaria\n      LR Chisq Df Pr(&gt;Chisq)    \nDuree  2386.56  1    &lt; 2e-16 ***\nSexe      5.45  1    0.01961 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nround(confint.default(modP3),3)\n\n                           2.5 % 97.5 %\n(Intercept)                0.311  0.464\nDuree                      0.001  0.002\nSexeM                      0.010  0.100\nPreventionAutre           -0.575  0.124\nPreventionMoustiquaire    -0.032  0.068\nPreventionSerpentin/Spray -0.096  0.135\n\nround(confint(modP3),3)\n\n                           2.5 % 97.5 %\n(Intercept)                0.311  0.464\nDuree                      0.001  0.002\nSexeM                      0.010  0.100\nPreventionAutre           -0.596  0.105\nPreventionMoustiquaire    -0.032  0.068\nPreventionSerpentin/Spray -0.098  0.134\n\n\n\n\nSélection de variables\n\nMalaria &lt;- read.table(\"../donnees/poissonData.csv\", sep=\",\", header=T)\nMalaria1 &lt;- na.omit(Malaria)\nMalaria1$Prevention &lt;- as.factor(Malaria1$Prevention)\nMalaria1$Sexe &lt;- as.factor(Malaria1$Sexe)\nlibrary(bestglm)\nmod_sel &lt;- bestglm(Malaria1,family=poisson)\nmod_sel$BestModels\n\n   Sexe   Age Altitude Prevention Duree Criterion\n1 FALSE  TRUE     TRUE      FALSE  TRUE  7384.946\n2 FALSE FALSE     TRUE      FALSE  TRUE  7387.814\n3  TRUE  TRUE     TRUE      FALSE  TRUE  7390.053\n4  TRUE FALSE     TRUE      FALSE  TRUE  7393.119\n5 FALSE  TRUE    FALSE      FALSE  TRUE  7401.021"
  },
  {
    "objectID": "code/chap13.html",
    "href": "code/chap13.html",
    "title": "13 Régularisation de la vraisemblance",
    "section": "",
    "text": "Régressions pénalisées avec glmnet\n\nlibrary(bestglm)\ndata(SAheart)\nSAheart.X &lt;- model.matrix(chd~.,data=SAheart)[,-1]\nSAheart.Y &lt;- SAheart$chd \nlibrary(glmnet)\nridge &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0)\nlasso &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\npar(mfrow=c(2,2))\nplot(ridge,ylim=c(-0.1,0.2))\nplot(lasso,ylim=c(-0.1,0.2))\nplot(ridge,ylim=c(-0.1,0.2),xvar=\"lambda\")\nplot(lasso,ylim=c(-0.1,0.2),xvar=\"lambda\")\n\n\n\nplot(ridge,ylim=c(-0.1,0.2),cex.lab=0.5)\nplot(lasso,ylim=c(-0.1,0.2),cex.lab=0.5)\nplot(ridge,ylim=c(-0.1,0.2),xvar=\"lambda\",cex.lab=0.5)\nplot(lasso,ylim=c(-0.1,0.2),xvar=\"lambda\",cex.lab=0.5)\n\n\n\n\n\n\nValidation croisée\n\nset.seed(2398)\nm1.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0)\nm1.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\nm2.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0,type.measure=\"class\")\nm2.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1,type.measure=\"class\")\nm3.ridge &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=0,type.measure=\"auc\")\nm3.lasso &lt;- cv.glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1,type.measure=\"auc\")\n\n\nm1.ridge$lambda.min\n\n[1] 0.01774595\n\nm1.ridge$lambda.1se\n\n[1] 0.2892148\n\n\n\npar(mfrow=c(3,2))\nplot(m1.ridge,main=\"Ridge\")\nplot(m1.lasso,main=\"Lasso\")\nplot(m2.ridge,main=\"Ridge\")\nplot(m2.lasso,main=\"Lasso\")\nplot(m3.ridge,main=\"Ridge\")\nplot(m3.lasso,main=\"Lasso\")\n\n\n\n\n\n\nGroup-lasso et elastic net\n\nlibrary(gglasso)\nX1 &lt;- c(rep(\"A\",60),rep(\"B\",90),rep(\"C\",50))\nX2 &lt;- c(rep(\"E\",40),rep(\"F\",60),rep(\"G\",55),rep(\"H\",45))\nset.seed(1298)\nX_3 &lt;- runif(200)\nset.seed(2381)\nY &lt;- round(runif(200))\ndonnees &lt;- data.frame(X1,X2,X_3,Y)\n\n\nD &lt;- model.matrix(Y~.,data=donnees)[,-1]\nlasso &lt;- glmnet(D,Y,alpha=1,lambda=exp(seq(-3,-5,length=100)))\ngroupe &lt;- c(1,1,2,2,2,3)\nlibrary(gglasso)\nY1 &lt;- 2*Y-1 \ng.lasso &lt;- gglasso(D,Y1,group=groupe,loss=\"logit\",lambda=exp(seq(-4.5,-5.5,length=100)))\nplot(lasso,xvar=\"lambda\",lwd=2,main=\"Lasso\")\n\n\n\nplot(g.lasso,main=\"Group-lasso\")\n\n\n\n\n\nlibrary(caret)\nalpha &lt;- seq(0,1,by=0.1)\nlambda &lt;- exp(seq(-7,2,length=100))\ngrille &lt;- expand.grid(alpha=alpha,lambda=lambda)\nctrl &lt;- trainControl(method=\"cv\")\nSAheart$chd &lt;- as.factor(SAheart$chd)\nset.seed(1234)\nsel &lt;- train(chd~.,data=SAheart,method=\"glmnet\",family=\"binomial\",trControl=ctrl,tuneGrid=grille)\nsel$bestTune\n\n    alpha     lambda\n747   0.7 0.05971442\n\ngetTrainPerf(sel)\n\n  TrainAccuracy TrainKappa method\n1     0.7489362  0.3759712 glmnet\n\n\n\n\nApplication : détection d’images publicitaires\n\nad.data &lt;- read.table(\"../donnees/ad_data.txt\",header=FALSE,sep=\",\",dec=\".\",na.strings = \"?\",strip.white = TRUE)\nnames(ad.data)[ncol(ad.data)] &lt;- \"Y\"\nad.data$Y &lt;- as.factor(ad.data$Y)\n\nad.data1 &lt;- na.omit(ad.data)\ndim(ad.data1)\n\n[1] 2359 1559\n\n\n\nset.seed(1234)\nind.app &lt;- sample(nrow(ad.data1),1800)\ndapp &lt;- ad.data1[ind.app,]\ndtest &lt;- ad.data1[-ind.app,]\n\n\nX.app &lt;- model.matrix(Y~.,data=dapp)[,-1]\nX.test &lt;- model.matrix(Y~.,data=dtest)[,-1]\nY.app &lt;- dapp$Y\nY.test &lt;- dtest$Y\n\n\nlogit &lt;- glm(Y~.,data=dapp,family=\"binomial\") \n\n\nset.seed(123)\nlasso.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\")\nridge.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\",alpha=0,lambda=exp(seq(-8,0,length=100)))\nen.cv &lt;- cv.glmnet(X.app,Y.app,family=\"binomial\",alpha=0.5)\n\n\npar(mfrow=c(1,3))\nplot(lasso.cv,main=\"Lasso\")\nplot(ridge.cv,main=\"Ridge\")\nplot(en.cv,main=\"Elastic net\")\n\n\n\n\n\nscore &lt;- data.frame(obs=dtest$Y,logit=predict(logit,newdata=dtest,type=\"response\"),\n                                lasso=as.vector(predict(lasso.cv,newx = X.test,type=\"response\")),\n                                ridge=as.vector(predict(ridge.cv,newx = X.test,type=\"response\")),\n                                en=as.vector(predict(en.cv,newx = X.test,type=\"response\")))\n\n\nlibrary(pROC)\nroc.ad &lt;- roc(obs~logit+lasso+ridge+en,data=score)\n\ncouleur &lt;- c(\"black\",\"red\",\"blue\",\"green\")\nmapply(plot,roc.ad,col=couleur,lty=1:4,add=c(F,T,T,T),lwd=2,legacy.axes=TRUE)\n\n                   logit       lasso       ridge       en         \npercent            FALSE       FALSE       FALSE       FALSE      \nsensitivities      numeric,329 numeric,276 numeric,515 numeric,266\nspecificities      numeric,329 numeric,276 numeric,515 numeric,266\nthresholds         numeric,329 numeric,276 numeric,515 numeric,266\ndirection          \"&lt;\"         \"&lt;\"         \"&lt;\"         \"&lt;\"        \ncases              numeric,461 numeric,461 numeric,461 numeric,461\ncontrols           numeric,98  numeric,98  numeric,98  numeric,98 \nfun.sesp           ?           ?           ?           ?          \nauc                0.8635619   0.9695648   0.9805879   0.969587   \ncall               expression  expression  expression  expression \noriginal.predictor numeric,559 numeric,559 numeric,559 numeric,559\noriginal.response  factor,559  factor,559  factor,559  factor,559 \npredictor          numeric,559 numeric,559 numeric,559 numeric,559\nresponse           factor,559  factor,559  factor,559  factor,559 \nlevels             character,2 character,2 character,2 character,2\npredictor.name     \"logit\"     \"lasso\"     \"ridge\"     \"en\"       \nresponse.name      \"obs\"       \"obs\"       \"obs\"       \"obs\"      \n\nlegend(\"bottomright\",legend=c(\"logit\",\"lasso\",\"ridge\",\"elastic net\"),col=couleur,lty=1:4,lwd=2,cex=0.65)\n\n\n\n\n\nsort(round(unlist(lapply(roc.ad,auc)),3),decreasing=TRUE)\n\nridge lasso    en logit \n0.981 0.970 0.970 0.864 \n\n\n\nprev1 &lt;- data.frame(apply(round(score[,-1]),2,factor,labels=c(\"ad.\",\"nonad.\")))\nerr &lt;- apply(sweep(prev1,1,dtest$Y,FUN=\"!=\"),2,mean)\nsort(round(err,3))\n\nridge lasso    en logit \n0.030 0.034 0.036 0.077"
  },
  {
    "objectID": "code/chap15.html",
    "href": "code/chap15.html",
    "title": "15 Données déséquilibrées",
    "section": "",
    "text": "df &lt;- data.frame(MALADE=c(208,42),\n                 NON_MALADE=c(48,202),\n                 FUMEUR=c(\"OUI\",\"NON\"))\nmodel &lt;- glm(cbind(MALADE,NON_MALADE)~FUMEUR,data=df,family=binomial)\ncoef(model)\n\n(Intercept)   FUMEUROUI \n  -1.570598    3.036935 \n\n\n\nnewX &lt;- data.frame(FUMEUR=c(\"OUI\",\"NON\"))\nrownames(newX) &lt;- c(\"OUI\",\"NON\")\npredict(model,newdata = newX,type=\"response\")\n\n      OUI       NON \n0.8125000 0.1721311 \n\n\n\nbeta1_cor &lt;- coef(model)[1]-log(0.995/0.005)\nbeta2 &lt;- coef(model)[2]\n\n\nexp(beta1_cor+beta2)/(1+exp(beta1_cor+beta2))\n\n(Intercept) \n 0.02131148 \n\nexp(beta1_cor)/(1+exp(beta1_cor))\n\n(Intercept) \n0.001043738 \n\n\n\ntau &lt;- c(0.05,0.95)\nind0 &lt;- which(df$Y==0)\nind1 &lt;- which(df$Y==1)\nchoix0 &lt;- sample(ind0, size=length(ind0)*tau[1], replace = F)\nchoix1 &lt;- sample(ind1, size=length(ind1)*tau[2], replace = F)\ndff &lt;- rbind(df[choix0,], df[choix1,])\n\n\nmod &lt;- glm(Y~., data=dff, family=\"binomial\")\n\n\ngamma &lt;- coef(mod)\ngamma[1] - log(tau[2]/tau[1])\n\n\nglm(Y~. + offset(rep(log(tau[2]/tau[1]), nrow(dff))),\n    data=dff, family=\"binomial\")"
  },
  {
    "objectID": "code/chap15.html#quelques-méthodes-de-rééquilibrage",
    "href": "code/chap15.html#quelques-méthodes-de-rééquilibrage",
    "title": "15 Données déséquilibrées",
    "section": "Quelques méthodes de rééquilibrage",
    "text": "Quelques méthodes de rééquilibrage\n\nlibrary(tidyverse)\n\n\nset.seed(123458)\nn1 &lt;- 10\nX11 &lt;- runif(n1,0,0.25)\nX21 &lt;- runif(n1,0,1)\nX12 &lt;- runif(n1,0,1)\nX22 &lt;- runif(n1,0.75,1)\nn2 &lt;- 80\nX13 &lt;- runif(n2,0.25,1)\nX23 &lt;- runif(n2,0,0.75)\nX1 &lt;- c(X11,X12,X13)\nX2 &lt;- c(X21,X22,X23)\nY &lt;- c(rep(1,2*n1),rep(0,n2)) %&gt;% as.factor()\ndf &lt;- data.frame(X1,X2,Y,id=as.character(1:100))\ndf$Y[c(1,16)] &lt;- 0\n#df$Y[c(41,48,59)] &lt;- 1\ndf$Y[c(41,48)] &lt;- 1\ndf &lt;- df[,1:3]\nggplot(df)+aes(x=X1,y=X2)+geom_point(aes(color=Y))\n\n\n\n\n\nlibrary(UBL)\nover1 &lt;- RandOverClassif(Y~., dat=df)\nover2 &lt;- RandOverClassif(Y~., dat=df, C.perc=list(\"0\"=1,\"1\"=2))\nsummary(over1$Y)\n\n 0  1 \n80 80 \n\nsummary(over2$Y)\n\n 0  1 \n80 40 \n\n\n\nset.seed(1234)\nsmote1 &lt;- SmoteClassif(Y~.,dat=df,k=4)\nsmote2 &lt;- SmoteClassif(Y~.,dat=df,k=4,C.perc=list(\"0\"=1,\"1\"=2))\nsummary(smote1$Y)\n\n 0  1 \n50 50 \n\nsummary(smote2$Y)\n\n 0  1 \n80 40 \n\n\n\nnewsm1 &lt;- anti_join(smote1,df)\nnewsm2 &lt;- anti_join(smote2,df)\nnewsm &lt;- bind_rows(\"smote1\"=newsm1,\"smote2\"=newsm2,.id=\"algo\")\n\n\ndf3 &lt;- bind_rows(\"smote1\"=smote1,\"smote2\"=smote2,.id=\"algo\")\nggplot(df3)+aes(x=X1,y=X2,color=Y)+geom_point(aes(shape=Y),size=1.5)+facet_wrap(~algo)+\n  geom_point(data=newsm,shape=1,size=4) + theme(legend.position='none')\n\n\n\n\n\nunder1 &lt;- RandUnderClassif(Y~.,dat=df)\nunder2 &lt;- RandUnderClassif(Y~.,dat=df,C.perc=list(\"0\"=0.5,\"1\"=1))\nsummary(under1$Y)\n\n 0  1 \n20 20 \n\nsummary(under2$Y)\n\n 0  1 \n40 20 \n\n\n\ntomek1 &lt;- TomekClassif(Y~.,dat=df)\ntomek2 &lt;- TomekClassif(Y~.,dat=df,rem=\"maj\")\ntomek1[[2]]\n\n[1]   1   7  12  69  14 100  16  17\n\ntomek2[[2]]\n\n[1]   1  69 100  16\n\n\n\nind1 &lt;- tomek1[[2]]\nind2 &lt;- tomek2[[2]]\nXS1 &lt;- df[ind1,]\nXS2 &lt;- df[ind2,]\nXS &lt;- bind_rows(\"tomek1\"=XS1,\"tomek2\"=XS2,.id=\"algo\")\ndf5 &lt;- bind_rows(\"tomek1\"=df,\"tomek2\"=df,.id=\"algo\")\nggplot(df5)+aes(x=X1,y=X2,color=Y)+geom_point(aes(color=Y,shape=Y),size=1.5)+facet_wrap(~algo)+\n  geom_point(data=XS,shape=1,size=4) + theme(legend.position='none')"
  },
  {
    "objectID": "code/chap15.html#critères-pour-données-déséquilibrées",
    "href": "code/chap15.html#critères-pour-données-déséquilibrées",
    "title": "15 Données déséquilibrées",
    "section": "Critères pour données déséquilibrées",
    "text": "Critères pour données déséquilibrées\n\n\n   P1\nY     0   1\n  0 468   0\n  1  31   1\n\n\n   P2\nY     0   1\n  0 407  61\n  1   4  28\n\n\n\nlibrary(yardstick)\ndf &lt;- data.frame(Y,P2)\nmulti_metric &lt;- metric_set(accuracy,bal_accuracy,f_meas,kap)\nmulti_metric(df,truth=Y,estimate=P2,event_level = \"second\")\n\n# A tibble: 4 × 3\n  .metric      .estimator .estimate\n  &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy     binary         0.87 \n2 bal_accuracy binary         0.872\n3 f_meas       binary         0.463\n4 kap          binary         0.407"
  },
  {
    "objectID": "code/chap15.html#application-aux-données-dimages-publicitaires",
    "href": "code/chap15.html#application-aux-données-dimages-publicitaires",
    "title": "15 Données déséquilibrées",
    "section": "Application aux données d’images publicitaires",
    "text": "Application aux données d’images publicitaires\n\nsummary(ad.data1$Y)\n\n   ad. nonad. \n   459   2820 \n\nad.data1 &lt;- ad.data1 %&gt;% \n  transform(Y=fct_recode(Y,\"0\"=\"nonad.\",\"1\"=\"ad.\")) %&gt;% \n  transform(Y=fct_inseq(Y))\n\n\nset.seed(1234)\nbloc &lt;- sample(1:10,nrow(ad.data1),replace=TRUE)\ntable(bloc)\n\nbloc\n  1   2   3   4   5   6   7   8   9  10 \n309 327 329 310 358 354 304 337 342 309 \n\n\n\nscore &lt;- data.frame(matrix(0,nrow=nrow(ad.data1),ncol=3))\nnames(score) &lt;- c(\"logit\",\"lasso\",\"ridge\")\nSCORE &lt;- list(brute=score,over=score,smote=score,under=score,tomek=score)\n\n\nset.seed(4321)\nlibrary(glmnet)\nscore &lt;- data.frame(matrix(0,nrow=nrow(ad.data1),ncol=3))\nnames(score) &lt;- c(\"logit\",\"lasso\",\"ridge\")\nSCORE &lt;- list(brute=score,over=score,smote=score,under=score,tomek=score)\n\nfor (k in 1:10){\n  print(k)\n  ind.test &lt;- bloc==k\n  dapp &lt;- ad.data1[!ind.test,]\n  dtest &lt;- ad.data1[ind.test,]\n  X.test &lt;- model.matrix(Y~.,data=dtest)[,-1]\n  \n  ech.app &lt;- list(norm=dapp,\n                  over=RandOverClassif(Y~.,dat=dapp),\n                  smote=SmoteClassif(Y~.,dat=dapp),\n                  under=RandUnderClassif(Y~.,dat=dapp),\n                  tomek=TomekClassif(Y~.,dat=dapp)[[1]])\n\n  mod.mat.list &lt;- function(df){model.matrix(Y~.,data=df)[,-1]}\n  Y.list &lt;- function(df) df$Y \n  \n  X.app &lt;- lapply(ech.app,mod.mat.list)\n  Y.app &lt;- lapply(ech.app,Y.list)\n  \n  for (j in 1:5){\n    print(j)\n    lasso &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=\"binomial\")\n    ridge &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=\"binomial\",alpha=0)\n    logit &lt;- glm(Y~.,data=ech.app[[j]],family=\"binomial\")\n    SCORE[[j]][ind.test,] &lt;- data.frame(\n      logit=predict.glm(logit,newdata=dtest,type=\"response\"),\n      lasso=as.vector(predict(lasso,newx=X.test,type=\"response\")),\n      ridge=as.vector(predict(ridge,newx=X.test,type=\"response\"))\n    )\n  }\n}\n\n\nmat.score &lt;- bind_rows(brutes=SCORE[[1]],\n                       over=SCORE[[2]],\n                       smote=SCORE[[3]],\n                       under=SCORE[[4]],\n                       tomek=SCORE[[5]],.id=\"meth\") %&gt;% \n  mutate(obs=rep(ad.data1$Y,5))  %&gt;% \n  pivot_longer(c(logit,lasso,ridge),\n               names_to = \"algo\",values_to = \"score\")\n\n\nmat.score %&gt;% group_by(meth,algo) %&gt;% \n  roc_auc(truth = obs,score,event_level = \"second\") %&gt;%\n  pivot_wider(-c(.metric,.estimator),\n              names_from = algo,values_from = .estimate)\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.943 0.831 0.980\n2 over   0.973 0.790 0.977\n3 smote  0.973 0.680 0.977\n4 tomek  0.950 0.763 0.979\n5 under  0.956 0.787 0.964\n\n\n\nmat.score &lt;- mat.score %&gt;% mutate(prev=as.factor(round(score)))\n\n\nAccuracy :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  accuracy(truth = obs,prev) %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.969 0.886 0.970\n2 over   0.961 0.847 0.963\n3 smote  0.960 0.699 0.960\n4 tomek  0.969 0.818 0.970\n5 under  0.954 0.808 0.955\n\n\nBalanced accuracy :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  bal_accuracy(truth = obs,prev) %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.898 0.832 0.900\n2 over   0.931 0.806 0.935\n3 smote  0.933 0.680 0.933\n4 tomek  0.896 0.797 0.900\n5 under  0.921 0.792 0.899\n\n\nF1 score :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  f_meas(truth = obs,prev,event_level = \"second\") %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.879 0.650 0.884\n2 over   0.864 0.579 0.873\n3 smote  0.863 0.378 0.862\n4 tomek  0.876 0.542 0.882\n5 under  0.843 0.529 0.837\n\n\nKappa de Cohen :\n\nmat.score %&gt;% \n  group_by(meth,algo) %&gt;% \n  kap(truth = obs,prev) %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;%\n  select(-(2:3))\n\n# A tibble: 5 × 4\n  meth   lasso logit ridge\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 brutes 0.862 0.584 0.867\n2 over   0.842 0.491 0.851\n3 smote  0.839 0.224 0.838\n4 tomek  0.859 0.440 0.866\n5 under  0.816 0.422 0.811\n\n\n\n\ngrille.score(mat.score,nom_algo=\"ridge\",meth=\"norm\")\n\n# A tibble: 11 × 7\n   seuil  sens  spec accuracy bal_accuracy f_meas   kap\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 0     1     0        0.14         0.5    0.246 0    \n 2 0.1   0.955 0.671    0.711        0.813  0.48  0.343\n 3 0.2   0.932 0.745    0.771        0.839  0.533 0.416\n 4 0.3   0.911 0.851    0.86         0.881  0.645 0.566\n 5 0.4   0.885 0.918    0.913        0.901  0.74  0.69 \n 6 0.5   0.843 0.983    0.964        0.913  0.867 0.846\n 7 0.6   0.797 0.991    0.964        0.894  0.861 0.841\n 8 0.7   0.739 0.995    0.959        0.867  0.834 0.811\n 9 0.8   0.672 0.997    0.952        0.835  0.796 0.77 \n10 0.9   0.562 0.999    0.938        0.78   0.717 0.685\n11 1     0     1        0.86         0.5   NA     0    \n\ngrille.score(mat.score,nom_algo=\"ridge\",meth=\"over\")\n\n# A tibble: 11 × 7\n   seuil  sens  spec accuracy bal_accuracy f_meas   kap\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 0     1     0        0.14         0.5    0.246 0    \n 2 0.1   0.955 0.671    0.711        0.813  0.48  0.343\n 3 0.2   0.932 0.745    0.771        0.839  0.533 0.416\n 4 0.3   0.911 0.851    0.86         0.881  0.645 0.566\n 5 0.4   0.885 0.918    0.913        0.901  0.74  0.69 \n 6 0.5   0.843 0.983    0.964        0.913  0.867 0.846\n 7 0.6   0.797 0.991    0.964        0.894  0.861 0.841\n 8 0.7   0.739 0.995    0.959        0.867  0.834 0.811\n 9 0.8   0.672 0.997    0.952        0.835  0.796 0.77 \n10 0.9   0.562 0.999    0.938        0.78   0.717 0.685\n11 1     0     1        0.86         0.5   NA     0    \n\ngrille.score(mat.score,nom_algo=\"ridge\",meth=\"smote\")\n\n# A tibble: 11 × 7\n   seuil  sens  spec accuracy bal_accuracy f_meas   kap\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 0     1     0        0.14         0.5    0.246 0    \n 2 0.1   0.955 0.671    0.711        0.813  0.48  0.343\n 3 0.2   0.932 0.745    0.771        0.839  0.533 0.416\n 4 0.3   0.911 0.851    0.86         0.881  0.645 0.566\n 5 0.4   0.885 0.918    0.913        0.901  0.74  0.69 \n 6 0.5   0.843 0.983    0.964        0.913  0.867 0.846\n 7 0.6   0.797 0.991    0.964        0.894  0.861 0.841\n 8 0.7   0.739 0.995    0.959        0.867  0.834 0.811\n 9 0.8   0.672 0.997    0.952        0.835  0.796 0.77 \n10 0.9   0.562 0.999    0.938        0.78   0.717 0.685\n11 1     0     1        0.86         0.5   NA     0"
  },
  {
    "objectID": "code/chap16.html",
    "href": "code/chap16.html",
    "title": "16 Introduction à la régression spline",
    "section": "",
    "text": "ozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=T,sep=\";\")\n\n\npolyreg &lt;- function(donnee,d=3){\n  sigmax &lt;- sd(donnee[,\"T12\"])\n  grillex &lt;- seq(min(donnee[,\"T12\"])-sigmax,max(donnee[,\"T12\"])+sigmax,length=100)\n  aprevoir &lt;- data.frame(T12=grillex)\n  regpol &lt;- lm(O3~poly(T12,degree=d,raw=TRUE),data=donnee)\n  prev &lt;- predict(regpol,aprevoir)\n  return(list(grillex=grillex,grilley=prev))\n}\n\n\nplot(O3~T12,data=ozone,xlab=\"T12\",ylab=\"O3\")\niter &lt;- 1\nfor(ii in c(1,2,3,9)){\n tmp &lt;- polyreg(ozone,d=ii)\n lines(tmp$grillex,tmp$grilley,col=iter,lty=iter)\n iter &lt;- iter+1\n}\nlegend(15,150,c(\"d=1\",\"d=2\",\"d=3\",\"d=9\"),col=1:4,lty=1:4)\n\n\n\n\n\nind &lt;- which(ozone[,2]&lt;23)\nregd &lt;- lm(O3~T12,data=ozone[ind,])\nregf &lt;- lm(O3~T12,data=ozone[-ind,])\ngxd &lt;- seq(3,23,length=50)\ngyd &lt;- regd$coef[1]+gxd*regd$coef[2]\ngxf &lt;- seq(23,35,length=50)\ngyf &lt;- regf$coef[1]+gxf*regf$coef[2]\nplot(O3~T12,data=ozone)\nlines(gxd,gyd,col=2,lty=1,lwd=2)\nlines(gxf,gyf,col=2,lty=1,lwd=2)\nabline(v=23)\n\n\n\n\n\nlibrary(splines)\nXB &lt;- bs(ozone[,2], knots=c(15,23), degree=2,Boundary.knots=c(5,32))\nregs &lt;- lm(ozone[,\"O3\"] ~ XB)\nregs$coef\n\n(Intercept)         XB1         XB2         XB3         XB4 \n  51.101947   61.543761    5.562286   70.459103  106.711539 \n\n\n\ngrillex &lt;- seq(5,32,length=100)\nbgrillex &lt;- bs(grillex, knots=c(15,23), degree=2,Boundary.knots=c(5,32))\nprev &lt;- bgrillex%*%as.matrix(regs$coeff[-1])+regs$coeff[1]\nplot(O3~T12,data=ozone)\nlines(grillex,prev,col=2)\nabline(v=c(15,23))\n\n\n\n\n\nregssplinel1 &lt;- smooth.spline(ozone[,2],ozone[,1],lambda =100)\nprevl1 &lt;- predict(regssplinel1,grillex)\nplot(O3~T12,data=ozone)\nlines(prevl1$x,prevl1$y,col=2)\n\n\n\n\n\nregsspline &lt;- smooth.spline(ozone[,2],ozone[,1])\nprev &lt;- predict(regsspline,grillex)\nplot(O3~T12,data=ozone)\nlines(prev$x,prev$y,col=2)\n\n\n\n\n\nregsspline\n\nCall:\nsmooth.spline(x = ozone[, 2], y = ozone[, 1])\n\nSmoothing Parameter  spar= 0.9410342  lambda= 0.006833357 (15 iterations)\nEquivalent Degrees of Freedom (Df): 4.156771\nPenalized Criterion (RSS): 11036.88\nGCV: 289.8012"
  },
  {
    "objectID": "code/chap17.html",
    "href": "code/chap17.html",
    "title": "17 Estimateurs à noyau et \\(k\\) plus proches voisins",
    "section": "",
    "text": "Estimateurs à noyau\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\")\n\n\nind &lt;- order(ozone[,\"T12\"])\nT12o &lt;- ozone[ind,\"T12\"]\nO3o &lt;- ozone[ind,\"O3\"]\n\n\nreg1 &lt;- lm(O3o~1,weight=c(rep(1,10),rep(0,40)))\nreg2 &lt;- lm(O3o~1,weight=c(rep(0,10),rep(1,10),rep(0,30)))\nreg3 &lt;- lm(O3o~1,weight=c(rep(0,20),rep(1,10),rep(0,20)))\nreg4 &lt;- lm(O3o~1,weight=c(rep(0,30),rep(1,10),rep(0,10)))\nreg5 &lt;- lm(O3o~1,weight=c(rep(0,40),rep(1,10)))\n\n\nplot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\nabline(v=c(14,18),col=\"red\",lwd=2)\nabline(v=c(16),col=\"blue\",lty=2)\npoints(16,mean(O3o[T12o&gt;=14 & T12o&lt;=18]),col=\"blue\",pch=17,cex=1.5)\n\n\n\n\n\nlibrary(ibr)\nx &lt;- seq(7,30,by=0.01)\npar(mfrow=c(1,3))\nh &lt;- c(20,3,0.05)\nfor (i in h){\n  plot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\n  tmp &lt;- npregress(T12o,O3o,bandwidth = i)\n  prev &lt;- predict(tmp,newdata=x)\n  lines(x,prev,col=\"blue\",lwd=2)\n}\n\n\n\n\n\n\nLes \\(k\\) plus proches voisins\n\npar(mfrow=c(1,3))\nlibrary(FNN)\nk &lt;- c(50,10,1)\nfor (i in k){\n  mod &lt;- knn.reg(train=T12o,test=as.matrix(x),y=O3o,k=i)\n  plot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\n  lines(x,mod$pred,col=\"blue\",lwd=2)\n}\n\n\n\n\n\n\nSélection des paramètres\n\nhcv &lt;- npregress(T12o,O3o)$bandwidth\nhcv\n\n[1] 1.688373\n\n\n\nknn.reg(train=T12o,y=O3o,k=10)$PRESS/length(T12o)\n\n[1] 287.6629\n\n\n\nK_cand &lt;- 1:49\nloo &lt;- rep(0,length(K_cand))\nfor (i in 1:length(K_cand)){\n  loo[i] &lt;- knn.reg(train=T12o,y=O3o,k=K_cand[i])$PRESS/length(T12o)\n}\nK_cand[which.min(loo)]\n\n[1] 8\n\n\n\nmod.kppv &lt;- knn.reg(train=T12o,test=as.matrix(x),y=O3o,k=8)\nmod.noyau &lt;- npregress(T12o,O3o,bandwidth = hcv)\nprev.noyau &lt;- predict(mod.noyau,newdata=x)\nplot(T12o,O3o,pch=20,xlab=\"T12\",ylab=\"O3\")\nlines(x,mod.kppv$pred,col=\"blue\",lwd=2)\nlines(x,prev.noyau,col=\"red\",lty=2,lwd=2)\n\n\n\n\n\nmod.noyau$df\n\n[1] 5.339428"
  },
  {
    "objectID": "correction/chap1.html",
    "href": "correction/chap1.html",
    "title": "1 La régression linéaire simple",
    "section": "",
    "text": "Exercice 1 (Questions de cours) B, A, B, A.\n\n\nExercice 2 (Biais des estimateurs) Les \\(\\hat \\beta_j\\) sont fonctions de \\(Y\\) (aléatoire), ce sont donc des variables aléatoires. Une autre façon d’écrire \\(\\hat \\beta_2\\) en fonction de \\(\\beta_2\\) consiste à remplacer \\(y_i\\) par sa valeur soit \\[\\begin{eqnarray*}\n\\hat \\beta_2 &=&\\frac{\\sum (x_i - \\bar x) y_i}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\beta_1\\sum (x_i - \\bar x)+\\beta_2\\sum x_i(x_i - \\bar x)+\n\\sum (x_i - \\bar x) \\varepsilon_i }{\\sum(x_i-\\bar x)^2} \\\\\n&=& \\beta_2 + \\frac{\\sum (x_i - \\bar x) \\varepsilon_i}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] Par hypothèse \\(\\mathbf E(\\varepsilon_i)=0\\), les autres termes ne sont pas aléatoires, le résultat est démontré.\nLe résultat est identique pour \\(\\hat \\beta_1\\) car \\(\\mathbf E(\\hat \\beta_1) = \\mathbf E(\\bar y) -\\bar x \\mathbf E(\\hat \\beta_2)= \\beta_1 +\\bar x \\beta_2 - \\bar x \\beta_2=\\beta_1\\), le résultat est démontré.\n\n\nExercice 3 (Variance des estimateurs) Nous avons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\n\\mathop{\\mathrm{V}}\\left(\\beta_2+\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}\n{\\sum(x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\\] Or \\(\\beta_2\\) est inconnu mais pas aléatoire et les \\(x_i\\) ne sont pas aléatoires donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\\mathop{\\mathrm{V}}\\left(\\frac{\\sum(x_i-\\bar x)\\varepsilon_i}{\\sum(x_i-\\bar x)^2}\\right)\n=\\frac{\\mathop{\\mathrm{V}}\\left(\\sum(x_i-\\bar x)\\varepsilon_i\\right)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}\\\\\n&=&\\frac{\\sum_{i,j}(x_i-\\bar x)(x_j-\\bar x)\\mathop{\\mathrm{Cov}}(\\varepsilon_i,\\varepsilon_j)}{\\left[\\sum(x_i-\\bar x)^2\\right]^2}.\n\\end{eqnarray*}\\] Or \\(\\mathop{\\mathrm{Cov}}(\\varepsilon_i,\\varepsilon_j)=\\delta_{ij}\\sigma^2\\) donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_2)\n&=&\\frac{\\sum_i(x_i-\\bar x)^2\\sigma^2}{\\left[\\sum_i(x_i-\\bar x)^2\\right]^2}\n=\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] Plus les mesures \\(x_i\\) sont dispersées autour de leur moyenne, plus \\(\\mathop{\\mathrm{V}}(\\hat \\beta_2)\\) est faible et plus l’estimation est précise. Bien sûr, plus \\(\\sigma^2\\) est faible, c’est-à-dire plus les \\(y_i\\) sont proches de la droite inconnue, plus l’estimation est précise.\\ Puisque \\(\\hat \\beta_1=\\bar y - \\hat \\beta_2 \\bar x\\), nous avons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_1)\n&=&\n\\mathop{\\mathrm{V}}\\left(\\bar y-\\hat \\beta_2 \\bar x \\right)=\\mathop{\\mathrm{V}}\\left(\\bar y\\right)+V(\\bar x \\hat \\beta_2)-2\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2 \\bar x)\\\\\n&=&\\mathop{\\mathrm{V}}\\left(\\frac{\\sum y_i}{n}\\right)+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2)\\\\\n&=&\\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}-2 \\bar x\\sum_i\\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2).\n\\end{eqnarray*}\\]\nCalculons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\bar y, \\hat \\beta_2)\n&=& \\frac{1}{n}\n\\mathop{\\mathrm{Cov}}\\left(\\sum_{i}\\left(\\beta_1+\\beta_2 x_i+\\varepsilon_i\\right),\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{n}\\sum_{i}\\mathop{\\mathrm{Cov}}\\left(\\varepsilon_i,\\frac{\\sum_j(x_j-\\bar x)\\varepsilon_j}\n{\\sum_j(x_j-\\bar x)^2}\\right)\\\\\n&=&\\frac{1}{\\sum_j(x_j-\\bar x)^2}\n\\sum_{i}\\frac{1}{n}\\mathop{\\mathrm{Cov}}\\left(\\varepsilon_i,\\sum_j(x_j-\\bar x)\\varepsilon_j\\right)\\\\\n&=&\\frac{\\sigma^2 \\frac{1}{n}\\sum_{i}(x_i-\\bar x)}{\\sum_j(x_j-\\bar x)^2}=0.\n\\end{eqnarray*}\\] Nous avons donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_1)\n&=& \\frac{\\sigma^2}{n}+\\bar x^2\\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\n=\\frac{\\sigma^2 \\sum x_i^2}{n\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] Là encore, plus \\(\\sigma^2\\) est faible, c’est-à-dire plus les \\(y_i\\) sont proches de la droite inconnue, plus l’estimation est précise. Plus les valeurs \\(x_i\\) sont dispersées autour de leur moyenne, plus la variance de l’estimateur sera faible. De même, une faible moyenne \\(\\bar x\\) en valeur absolue contribue à bien estimer \\(\\beta_1\\).\n\n\nExercice 4 (Covariance de \\(\\hat\\beta_1\\) et \\(\\hat\\beta_2\\)) Nous avons \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\hat \\beta_1,\\hat \\beta_2) &=&\\mathop{\\mathrm{Cov}}(\\bar y-\\hat \\beta_2 \\bar x,\\hat \\beta_2)\n= \\mathop{\\mathrm{Cov}}(\\bar y,\\hat \\beta_2)- \\bar x \\mathop{\\mathrm{V}}(\\hat \\beta_2)=\n-\\frac{\\sigma^2 \\bar x}{\\sum(x_i-\\bar x)^2}.\n\\end{eqnarray*}\\] La covariance entre \\(\\beta_1\\) et \\(\\beta_2\\) est négative. L’équation \\(\\bar y=\\hat \\beta_1+\\hat \\beta_2\\bar x\\) indique que la droite des MC passe par le centre de gravité du nuage \\((\\bar x, \\bar y)\\). Supposons \\(\\bar x\\) positif, nous voyons bien que, si nous augmentons la pente, l’ordonnée à l’origine va diminuer et vice versa. Nous retrouvons donc le signe négatif pour la covariance entre \\(\\hat \\beta_1\\) et \\(\\hat \\beta_2\\).\n\n\nExercice 5 (Théorème de Gauss-Markov) L’estimateur des MC s’écrit \\(\\hat \\beta_2 = \\sum_{i=1}^n p_i y_i,\\) avec \\(p_i=(x_i-\\bar x)/\\sum(x_i -\\bar x)^2\\).\nConsidérons un autre estimateur \\(\\tilde{\\beta_2}\\) linéaire en \\(y_i\\) et sans biais, c’est-à-dire\n\\[\\tilde{\\beta_2} =\\sum_{i=1}^n \\lambda_i y_i.\\]\nMontrons que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\). L’égalité \\(\\mathbf E(\\tilde{\\beta_2}) = \\beta_1 \\sum \\lambda_i + \\beta_2 \\sum \\lambda_i x_i + \\sum \\lambda_i \\mathbf E(\\varepsilon_i)\\) est vraie pour tout \\(\\beta_2\\) et \\(\\tilde \\beta_2\\) est sans biais donc \\(\\mathbf E(\\tilde \\beta_2)=\\beta_2\\) pour tout \\(\\beta_2\\), c’est-à-dire que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\).\nMontrons que \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2)\\). \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) = \\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2 + \\hat \\beta_2)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2)+\n2\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2).\n\\end{eqnarray*}\\] \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2)\n\\!=\\!\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2},\\hat \\beta_2) -\\mathop{\\mathrm{V}}(-\\hat \\beta_2)\n\\!=\\!\\frac{\\sigma^2\\sum \\lambda_i(x_i-\\bar x)}{\\sum (x_i-\\bar x)^2} -\n\\frac{\\sigma^2}{\\sum (x_i-\\bar x)^2}\n\\!=\\!0,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) =\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Une variance est toujours positive et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Le résultat est démontré. On obtiendrait la même chose pour \\(\\hat \\beta_1\\).\n\n\nExercice 6 (Somme des résidus) Il suffit de remplacer les résidus par leur définition et de remplacer \\(\\hat \\beta_1\\) par son expression \\[\\begin{eqnarray*}\n\\sum_i \\hat \\varepsilon_i\n= \\sum_i (y_i - \\bar y + \\hat \\beta_2 \\bar x - \\hat \\beta_2 x_i)\n= \\sum_i (y_i-\\bar y) - \\hat \\beta_2 \\sum_i (x_i - \\bar x)= 0.\n\\end{eqnarray*}\\]\n\n\nExercice 7 (Estimateur de la variance du bruit) Récrivons les résidus en constatant que \\(\\hat \\beta_1= \\bar y- \\hat \\beta_2 \\bar x\\) et \\(\\beta_1=\\bar y - \\beta_2 \\bar x - \\bar \\varepsilon\\), \\[\\begin{eqnarray*}\n\\hat \\varepsilon_i&=& \\beta_1 + \\beta_2 x_i +\\varepsilon_i - \\hat \\beta_1 - \\hat \\beta_2x_i\\\\\n&=& \\bar{y} - \\beta_2 \\bar{x} - \\bar{\\varepsilon} + \\beta_2 x_i +\\varepsilon_i -\\bar{y} + \\hat \\beta_2 \\bar{x} - \\hat \\beta_2x_i\\\\\n&=& (\\beta_2-\\hat \\beta_2)(x_i -\\bar{x}) + (\\varepsilon_i - \\bar{\\varepsilon}).\n\\end{eqnarray*}\\] En développant et en nous servant de l’écriture de \\(\\hat \\beta_2\\) donnée dans la solution de l’exercice 2, nous avons \\[\\begin{eqnarray*}\n\\sum \\hat \\varepsilon_i^2& \\!=\\!& (\\beta_2-\\hat \\beta_2)^2 \\!\\sum  \\!(x_i \\!-\\!\\bar{x})^2\n\\!+\\!\\sum \\!(\\varepsilon_i \\!-\\! \\bar{\\varepsilon})^2\\!+\\!2 \\!(\\beta_2\\!-\\!\\hat \\beta_2)\n\\!\\sum \\!(x_i \\!-\\!\\bar{x})(\\varepsilon_i \\!- \\!\\bar{\\varepsilon})\\\\\n&=& (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2\n+\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2 - 2 (\\beta_2-\\hat \\beta_2)^2 \\sum  (x_i -\\bar{x})^2.\n\\end{eqnarray*}\\] Prenons en l’espérance \\[\\begin{eqnarray*}\n\\mathbf E\\left( \\sum \\hat{\\varepsilon_i}^2\\right)= \\mathbf E\\left(\\sum (\\varepsilon_i - \\bar{\\varepsilon})^2\\right) -\n\\sum  (x_i -\\bar{x})^2 \\mathop{\\mathrm{V}}(\\hat \\beta_2)\n= (n-2) \\sigma^2.\n\\end{eqnarray*}\\]\n\n\nExercice 8 (Prévision) Calculons la variance \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}\\left(\\hat y^p_{n+1}\\right)\n&=&\\mathop{\\mathrm{V}}\\left(\\hat \\beta_1 + \\hat \\beta_2x_{n+1}\\right)\n=\\mathop{\\mathrm{V}}(\\hat \\beta_1)+x_{n+1}^2 \\mathop{\\mathrm{V}}(\\hat \\beta_2)\n+2 x_{n+1} \\mathop{\\mathrm{Cov}}\\left(\\hat \\beta_1,\\hat \\beta_2\\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum x_i^2}{n}+x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\frac{\\sigma^2}{\\sum(x_i-\\bar x)^2}\\left(\n\\frac{\\sum (x_i-\\bar x)^2}{n}+\\bar x^2 + x_{n+1}^2-2 x_{n+1}\\bar x \\right)\\\\\n&=& \\sigma^2\\left(\\frac{1}{n}+\n\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\\] Plus la valeur à prévoir s’éloigne du centre de gravité, plus la valeur prévue sera variable (i.e. de variance élevée).\nVariance de l’erreur de prévision\\ Nous obtenons la variance de l’erreur de prévision en nous servant du fait que \\(y_{n+1}\\) est fonction de \\(\\varepsilon_{n+1}\\) seulement, alors que \\(\\hat y^p_{n+1}\\) est fonction des autres \\(\\varepsilon_i\\), \\(i=1,\\cdots,n\\). Les deux quantités ne sont pas corrélées. Nous avons alors \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\varepsilon_{n+1}^p) \\!=\\! \\mathop{\\mathrm{V}}\\left(y_{n+1} \\!-\\! \\hat y_{n+1}^p\\right)\n\\!=\\! \\mathop{\\mathrm{V}}(y_{n+1})\\!+\\!\\mathop{\\mathrm{V}}(\\hat y_{n+1}^p)\\!=\\! \\sigma^2\\left(1+\\frac{1}{n}\n\\!+\\!\\frac{(x_{n+1}-\\bar x)^2}{\\sum (x_i-\\bar x)^2}\\right).\n\\end{eqnarray*}\\]\n\n\nExercice 9 (\\(R^2\\) et coefficient de corrélation) Le coefficient \\(\\mathop{\\mathrm{R^2}}\\) s’écrit \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}&=&%\\frac{\\|\\hat{Y} -\\bar{y}\\1\\|^2}{\\|Y-\\bar{y}\\1\\|^2}=\n\\frac{\\sum_{i=1}^{n}{\\left(\\hat \\beta_1 + \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n\\sum_{i=1}^{n}{\\left(y_i-\\bar{y}\\right)^2}}=\n\\frac{\\sum_{i=1}^{n}{\\left( \\bar{y}-\\hat \\beta_2 \\bar{x}+ \\hat \\beta_2 x_i - \\bar{y}\\right)^2}}{\n{\\sum_{i=1}^{n}(y_i-\\bar{y})^2}}\\\\\n&=&\n\\frac{\\hat \\beta_2^2\\sum_{i=1}^{n}{\\left( x_i - \\bar{x}\\right)^2}}{\n\\sum_{i=1}^{n}\\left(y_i-\\bar{y}\\right)^2}\n=\\frac{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}}{\n\\left[\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\right]^2\n\\sum_{i=1}^{n}(y_i-\\bar{y})^2}\\\\\n&=&\\frac{\\left[\n\\sum_{i=1}^{n}{( x_i - \\bar{x})(y_i-\\bar{y})}\\right]^2}{\n\\sum_{i=1}^{n}{( x_i - \\bar{x})^2}\\sum_{i=1}^{n}{(y_i-\\bar{y})^2}}=\\rho^2(X,Y).\n\\end{eqnarray*}\\]\n\n\nExercice 10 (Les arbres) Le calcul donne \\[\\begin{eqnarray*}\n\\hat \\beta_1 =\\frac{6.26}{28.29}=0.22 \\quad \\quad\n\\hat \\beta_0 = 18.34-0.22 \\times 34.9=10.662.\n\\end{eqnarray*}\\] Nous nous servons de la propriété \\(\\sum_{i=1}^n \\hat \\varepsilon_i=0\\) pour obtenir \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}2 &=& \\frac{\\sum_{i=1}^{20} (\\hat y_i - \\bar y)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=\\frac{\\sum_{i=1}^{20}\n(\\hat \\beta_1 x_i - \\hat \\beta_1 \\bar x)^2}\n{\\sum_{i=1}^{20}(y_i - \\bar y)^2}=0.22^2 \\times \\frac{28.29}{2.85}=0.48.\n\\end{eqnarray*}\\] Les statistiques de test valent 5.59 pour \\(\\beta_0\\) et 4.11 pour \\(\\beta_1\\). Elles sont à comparer à un fractile de la loi de Student admettant 18 ddl, soit 2.1. Nous rejetons dans les deux cas l’hypothèse de nullité du coefficient. Nous avons modélisé la hauteur par une fonction affine de la circonférence, il semblerait évident que la droite passe par l’origine (un arbre admettant un diamètre proche de zéro doit être petit), or nous rejetons l’hypothèse \\(\\beta_0=0\\). Les données mesurées indiquent des arbres dont la circonférence varie de 26 à 43 cm, les estimations des paramètres du modèle sont valides pour des données proches de \\([26;43]\\).\n\n\nExercice 11 (Modèle quadratique) Les modèles sont \\[\\begin{eqnarray*}\n\\mathtt{O3} &=& \\beta_1 + \\beta_2 \\mathtt{T12} +\\varepsilon \\quad\n\\hbox{modèle classique,}\\\\\n\\mathtt{O3} &=& \\gamma_1 + \\gamma_2 \\mathtt{T12}^2 +\\varepsilon \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\\] L’estimation des paramètres donne \\[\\begin{eqnarray*}\n\\widehat{\\mathtt{O3}} &=& 31.41 + 2.7 \\ \\mathtt{T12} \\quad\\quad \\mathop{\\mathrm{R^2}}2=0.28 \\quad\n\\hbox{modèle classique,}\\\\\n\\widehat{\\mathtt{O3}} &=& 53.74 + 0.075 \\ \\mathtt{T12}^2 \\quad \\mathop{\\mathrm{R^2}}2=0.35 \\quad\n\\hbox{modèle demandé}.\n\\end{eqnarray*}\\] Les deux modèles ont le même nombre de paramètres, nous préférons le modèle quadratique car le \\(\\mathop{\\mathrm{R^2}}\\) est plus élevé."
  },
  {
    "objectID": "correction/chap2.html",
    "href": "correction/chap2.html",
    "title": "2 La régression linéaire multiple",
    "section": "",
    "text": "Exercice 1 (Question de cours) A, A, B, B, B, C.\n\n\nExercice 2 (Covariance de \\(\\hat\\varepsilon\\) et \\(\\hat Y\\)) Nous allons montrer que, pour tout autre estimateur \\(\\tilde{\\beta}\\) de \\(\\beta\\) linéaire et sans biais, \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta)\\). Décomposons la variance de \\(\\tilde{\\beta}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta})  = \\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta)+\\mathop{\\mathrm{V}}(\\hat \\beta) -\n2 \\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\\] Les variances étant définies positives, si nous montrons que \\(\\mathop{\\mathrm{Cov}}(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0\\), nous aurons fini la démonstration.\\ Puisque \\(\\tilde{\\beta}\\) est linéaire, \\(\\tilde{\\beta} = A Y\\). De plus, nous savons qu’il est sans biais, c’est-à-dire \\(\\mathbf E(\\tilde{\\beta}) = \\beta\\) pour tout \\(\\beta\\), donc \\(A X = I\\). La covariance devient : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=&\n\\mathop{\\mathrm{Cov}}(A Y,(X'X)^{-1}X'Y) - \\mathop{\\mathrm{V}}(\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\\]\n\n\nExercice 3 (Théorème de Gauss Markov) Nous devons montrer que, parmi tous les estimateurs linéaires sans biais, l’estimateur de MC est celui qui a la plus petite variance. La linéarité de \\(\\hat \\beta\\) est évidente. Calculons sa variance : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta) = \\mathop{\\mathrm{V}}((X'X)^{-1}X'Y) =\n(X'X)^{-1}X'\\mathop{\\mathrm{V}}(Y)X(X'X)^{-1}=\\sigma^2 (X'X)^{-1}.\n\\end{eqnarray*}\\] Nous allons montrer que, pour tout autre estimateur \\(\\tilde{\\beta}\\) de \\(\\beta\\) linéaire et sans biais, \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta)\\). Décomposons la variance de \\(\\tilde{\\beta}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta})  = \\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta+\\hat \\beta)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta} - \\hat \\beta)+\\mathop{\\mathrm{V}}(\\hat \\beta) -\n2 \\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta).\n\\end{eqnarray*}\\] Les variances étant définies positives, si nous montrons que \\(\\mathop{\\mathrm{Cov}}(\\tilde{\\beta}- \\hat \\beta,\\hat \\beta)=0\\), nous aurons fini la démonstration.\\ Puisque \\(\\tilde{\\beta}\\) est linéaire, \\(\\tilde{\\beta} = A Y\\). De plus, nous savons qu’il est sans biais, c’est-à-dire \\(\\mathbf E(\\tilde{\\beta}) = \\beta\\) pour tout \\(\\beta\\), donc \\(A X = I\\). La covariance devient : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta} - \\hat \\beta,\\hat \\beta) &=&\n\\mathop{\\mathrm{Cov}}(A Y,(X'X)^{-1}X'Y) - \\mathop{\\mathrm{V}}(\\hat \\beta)\\\\\n&=& \\sigma^2 A X (X'X)^{-1} - \\sigma^2 (X'X)^{-1}=0.\n\\end{eqnarray*}\\]\n\n\nExercice 4 (Représentation des variables) Nous représentons les données dans \\(\\mathbb R^2\\) pour le premier jeu et dans \\(\\mathbb R^3\\) pour le second.\n\n\n\n\n\nDans le premier modèle, nous projetons \\(Y\\) sur l’espace engendré par \\(X\\), soit la droite de vecteur directeur \\(\\overrightarrow{OX}\\). Nous trouvons par le calcul \\(\\hat \\beta = 1.47\\), résultat que nous aurions pu trouver graphiquement car \\(\\overrightarrow{O \\hat Y}= \\hat \\beta . \\overrightarrow{OX}\\).\nConsidérons \\(\\mathbb R^3\\) muni de la base orthonormée \\((\\vec{i},\\vec{j},\\vec{k})\\). Les vecteurs \\(\\overrightarrow{OX}\\) et \\(\\overrightarrow{OZ}\\) engendrent le même plan que celui engendré par \\((\\vec{i},\\vec{j})\\). La projection de \\(Y\\) sur ce plan donne \\(\\overrightarrow{O \\hat Y}\\). Il est quasiment impossible de trouver \\(\\hat \\beta\\) et \\(\\hat \\gamma\\) graphiquement mais nous trouvons par le calcul \\(\\hat \\beta = -3.33\\) et \\(\\hat \\gamma =5\\).\n\n\nExercice 5 (Modèles emboîtés) Nous obtenons \\[\\begin{eqnarray*}\n\\hat Y_p = X \\hat \\beta \\quad  \\hbox{et} \\quad \\hat Y_q= X_q \\hat \\gamma.\n\\end{eqnarray*}\\] Par définition du \\(\\mathbb R2\\), il faut comparer la norme au carré des vecteurs \\(\\hat Y_p\\) et \\(\\hat Y_q\\). Notons les espaces engendrés par les colonnes de \\(X_q\\) et \\(X\\), \\(\\mathcal M_{X_q}\\) et \\(\\mathcal M_{X}\\), nous avons \\(\\mathcal M_{X_q} \\subset \\mathcal M_{X}\\). Nous obtenons alors \\[\\begin{eqnarray*}\n\\hat Y_p = P_{X_p}Y\n= (P_{X_q} + P_{X^{\\perp}_q})P_{X_p}Y &=& P_{X_q}P_{X_p}Y + P_{X^{\\perp}_q}P_{X_p}Y\\\\\n&=& P_{X_q}Y + P_{X^{\\perp}_q \\cap X_p} Y\\\\\n&=& \\hat Y_q + P_{X^{\\perp}_q \\cap X_p} Y.\n\\end{eqnarray*}\\] En utilisant le théorème de Pythagore, nous avons \\[\\begin{eqnarray*}\n\\| \\hat Y_p \\|^2 &=& \\|\\hat Y_q \\|^2 + \\| P_{X^{\\perp}_q \\cap X_p} Y \\|^2\n\\geq \\|\\hat Y_q \\|^2,\n\\end{eqnarray*}\\] d’où \\[\\begin{eqnarray*}\n\\mathbb R2(p)=\\frac{\\| \\hat Y_p \\|^2}{\\| Y \\|^2} \\geq\n\\frac{\\| \\hat Y_q \\|^2}{\\| Y \\|^2} =\\mathbb R2(q).\n\\end{eqnarray*}\\]\nEn conclusion, lorsque les modèles sont emboîtés \\(\\mathcal M_{X_q} \\subset \\mathcal M_{X}\\), le \\(\\mathbb R2\\) du modèle le plus grand (ayant le plus de variables) sera toujours plus grand que le \\(\\mathbb R2\\) du modèle le plus petit.\n\n\nExercice 6 La matrice \\(X'X\\) est symétrique, \\(n\\) vaut 30 et \\(\\bar x= \\bar z=0\\). Le coefficient de corrélation \\[\\begin{equation*}\n\\rho_{x,z} = \\frac{\\sum_{i=1}^{30} (x_i -\\bar x)(z_i - \\bar z)}\n{\\sqrt{\\sum_{i=1}^{30} (x_i -\\bar x)^2\\sum_{i=1}^{30} (z_i - \\bar z)^2}}\n=\\frac{\\sum_{i=1}^{30} x_i z_i}\n{\\sqrt{\\sum_{i=1}^{30} x_i^2 \\sum_{i=1}^{30} z_i^2}}\n=\\frac{7}{\\sqrt{150}}=0.57.\n\\end{equation*}\\] Nous avons \\[\\begin{eqnarray*}\ny_i &=& -2 +x_i+z_i+\\hat \\varepsilon_i\n\\end{eqnarray*}\\] et la moyenne vaut alors \\[\\begin{eqnarray*}\n\\bar y &=& -2 + \\bar x +\\bar z + \\frac{1}{n}\\sum_i \\hat \\varepsilon_i.\n\\end{eqnarray*}\\] La constante étant dans le modèle, la somme des résidus est nulle car le vecteur \\(\\hat \\varepsilon\\) est orthogonal au vecteur \\(\\mathbf{1}\\). Nous obtenons donc que la moyenne de \\(Y\\) vaut 2 car \\(\\bar x=0\\) et \\(\\bar z=0\\). Nous obtenons en développant \\[\\begin{eqnarray*}\n\\|\\hat Y \\|^2 &=& \\sum_{i=1}^{30}(-2+x_i+2z_i)^2\\\\\n&=& 4+10+60+14=88.\n\\end{eqnarray*}\\] Par le théorème de Pythagore, nous concluons que \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{SCT}}=\\mathop{\\mathrm{SCE}}+\\mathop{\\mathrm{SCR}}=88+12=100.\n\\end{eqnarray*}\\]\n\n\nExercice 7 (Changement d’échelle des variables explicatives) Nous avons l’estimation sur le modèle avec les variables originales qui minimise \\[\\begin{align*}\n\\mathop{\\mathrm{MCO}}(\\beta)&=\\|Y - \\sum_{j=1}^{p} X_j \\beta_j \\|^{2}\n\\end{align*}\\] Cette solution est notée \\(\\hat \\beta\\).\nNous avons l’estimation sur le modèle avec les variables prémultipliées par \\(a_{j}\\) (changement d’échelle) qui minimise\n\\[   \n\\begin{align*}\n\\tilde{\\mathop{\\mathrm{MCO}}}(\\beta)&=\\|Y - \\sum_{j=1}^{p} \\tilde X_j \\beta_j \\|^{2} = \\|Y - \\sum_{j=1}^{p} a_{j} X_j \\beta_j \\|^{2}\\\\\n&= \\|Y - \\sum_{j=1}^{p} X_j \\gamma_j \\|^{2}=\\mathop{\\mathrm{MCO}}(\\gamma),\n\\end{align*}\n\\]\nen posant en dernière ligne \\(\\gamma_{j}=a_{j} \\beta_j\\). La solution de de \\(\\mathop{\\mathrm{MCO}}(\\gamma)\\) (ou encore \\(\\mathop{\\mathrm{MCO}}(\\beta)\\)) est \\(\\hat \\beta\\). La solution de \\(\\tilde{\\mathop{\\mathrm{MCO}}}(\\beta)\\) est alors donnée par \\(\\hat \\beta_{j}=a_{j} \\tilde \\beta_j\\).\n\n\nExercice 8 (Différence entre régression multiple et régressions simples)  \n\nCalculons l’estimateur des MCO noté traditionnellement \\((X'X)^{-1}X'Y\\) avec la matrice \\(X\\) qui possède ici deux colonnes (notées ici \\(X\\) et \\(Z\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  (X'X)&=\n         \\begin{pmatrix}\n           \\|X\\|^{2} & &lt;X,Z&gt;\\\\\n           &lt;X,Z&gt; & \\|Z\\|^{2} \\\\\n         \\end{pmatrix}\n\\end{align*}\\] Son déterminant est \\(\\Delta= \\|X\\|^{2}\\|Z\\|^{2} - 2 &lt;X,Z&gt;\\) et son inverse est \\[\\begin{align*}\n  \\frac{1}{\\Delta}\n  \\begin{pmatrix}\n           \\|Z\\|^{2} & -&lt;X,Z&gt;\\\\\n           -&lt;X,Z&gt; & \\|X\\|^{2} \\\\\n  \\end{pmatrix}\n\\end{align*}\\] Ensuite \\(X'Y\\) est simplement le vecteur colonne de coordonnées \\(&lt;X,Y&gt;\\) et \\(&lt;Z,Y&gt;\\). En rassemblant le tout nous avons \\[\\begin{align*}\n  \\hat \\beta_{1}&=\\frac{1}{\\Delta}(\\|Z\\|^{2} &lt;X,Y&gt; - &lt;X,Z&gt;&lt;Z,Y&gt;),\\\\\n  \\hat \\beta_{2}&=\\frac{1}{\\Delta}(\\|X\\|^{2} &lt;Z,Y&gt; - &lt;X,Z&gt;&lt;X,Y&gt;).\n\\end{align*}\\] Si \\(&lt;X,Z&gt;=0\\) (les deux vecteurs sont orthogonaux) alors cette écriture se simplifie en \\[\n  \\hat \\beta_{1}=\\frac{&lt;X,Y&gt;}{\\|X\\|^{2}},\\quad\n  \\hat \\beta_{2}=\\frac{&lt;Z,Y&gt;}{\\|Z\\|^{2}}.\n\\]\nCalculons l’estimateur des MCO noté traditionnellement \\((X'X)^{-1}X'Y\\) avec la matrice \\(X\\) qui possède ici une colonne (notée ici \\(X\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  \\hat \\beta_{X} = \\frac{&lt;X,Y&gt;}{\\|X\\|^{2}}.\n\\end{align*}\\] Passons maintenant à la matrice qui possède ici une colonne (notée ici \\(Z\\)) et \\(n\\) lignes. On a donc \\[\\begin{align*}\n  \\hat \\beta_{Z} = \\frac{&lt;Z,Y&gt;}{\\|Z\\|^{2}}.\n\\end{align*}\\]\nEn général les coefficients des régressions simples ne sont pas ceux obtenus par régression multiple sauf si les variables sont orthogonales.\nNous avons ici les résidus de la première régression qui sont \\[\\begin{align}\n  \\hat \\varepsilon = Y - \\hat \\beta_{X} X.\n\\end{align}\\] La deuxième régression (sur les résidus) donne le coefficient \\[\\begin{align*}\n  \\hat \\beta_{Z} = \\frac{&lt;Z,\\hat \\varepsilon&gt;}{\\|Z\\|^{2}} = \\hat \\beta_{Z} - \\hat \\beta_{X}\\frac{&lt;Z,X&gt;}{\\|Z\\|^{2}}\n\\end{align*}\\] La régression séquentielle donne des coefficients différents des régressions univariées ou bivariées sauf si les variables sont orthogonales. \\end{enumerate}\n\n\n\nExercice 9 (TP : différence entre régression multiple et régressions simples)  \n\nSimulons 2 variables explicatives avec GNU-R pour \\(n=100\\) individus selon selon deux loi uniforme \\([0,1]\\) :\n\nn &lt;- 100\nset.seed(4321) # pour fixer les simulations\nX1 &lt;- runif(n)\nX2 &lt;- runif(n)\n\nEnsuite simulons \\(Y\\) selon le modèle avec \\(\\sigma=0.5\\)\n\nsigma &lt;- 0.5\nset.seed(321) # pour fixer les simulations\nY &lt;- 2 -3*X1 + 4*X2  + rnorm(n, sd=sigma)\ndon &lt;- cbind.data.frame(Y,X1,X2)\nhead(don)\n\n         Y         X1        X2\n1 4.213477 0.33477802 0.5913398\n2 1.602748 0.90913948 0.6715464\n3 2.958646 0.41152969 0.5830570\n4 3.215113 0.04384097 0.3516151\n5 3.367004 0.76350011 0.9298713\n6 3.555247 0.75043889 0.9181180\n\n\nLe graphique est obtenu avec\n\nrgl::plot3d(X1,X2,Y)\n\nOn observe grâce au code ci-dessus que les points sont répartis autour d’un plan (d’équation \\(z=2-3x + 4y\\)).\nEffectuons la régression multiple et stockons \\(\\hat Y\\) dans Yhat :\n\n(regmult &lt;- lm(Y~1+X1+X2, data=don))\n\n\nCall:\nlm(formula = Y ~ 1 + X1 + X2, data = don)\n\nCoefficients:\n(Intercept)           X1           X2  \n      2.080       -2.972        3.827  \n\n\nNous sommes assez proches des coefficients recherchés (2, -3 et 4)\nEffectuons les régression simples\n\n(regX1 &lt;- lm(Y~1+X1, data=don))\n\n\nCall:\nlm(formula = Y ~ 1 + X1, data = don)\n\nCoefficients:\n(Intercept)           X1  \n      3.803       -2.441  \n\n\nNous voyons que le paramètre ne correspond pas à celui de \\(X_{1}\\) dans la régression multiple.\n\n(regX2 &lt;- lm(Y~1+X2, data=don))\n\n\nCall:\nlm(formula = Y ~ 1 + X2, data = don)\n\nCoefficients:\n(Intercept)           X2  \n     0.7221       3.4424  \n\n\nNous voyons que le paramètre ne correspond pas à celui de \\(X_{2}\\) dans la régression multiple.\nEnchainons après la régression simple sur \\(X_{1}\\) la régression des résidus (ce que \\(X_{1}\\) n’a pas réussi à modéliser) sur \\(X_{2}\\):\n\nresidX1 &lt;- residuals(regX1)\ndon &lt;- cbind(don, residX1)\nregX2residX1 &lt;- lm(residX1~1+X2, data=don)\nregX2residX1\n\n\nCall:\nlm(formula = residX1 ~ 1 + X2, data = don)\n\nCoefficients:\n(Intercept)           X2  \n     -1.965        3.758  \n\n\nLà encore le coefficient sur \\(X_{2}\\) ne correspond pas à celui de \\(X_{2}\\) dans la régression multiple. :La prévision est différente\n\n(predict(regX2residX1) + predict(regX1))[1:5]\n\n       1        2        3        4        5 \n3.242781 2.142450 3.024335 3.051894 3.468733 \n\n\nChangeons l’ordre\n\nresidX2 &lt;- residuals(regX2)\ndon &lt;- cbind(don, residX2)\nregX1residX2 &lt;- lm(residX2~1+X1, data=don)\nregX1residX2\n\n\nCall:\nlm(formula = residX2 ~ 1 + X1, data = don)\n\nCoefficients:\n(Intercept)           X1  \n      1.531       -2.918  \n\n\nOn a là encore des différences\n\n(predict(regX1residX2) + predict(regX2))[1:5]\n\n       1        2        3        4        5 \n3.311895 1.911850 3.059400 3.335703 3.226125 \n\n\nEnvisageons la covariance empirique\n\ncov(X1,X2)    \n\n[1] 0.01050429\n\n\net leur produit scalaire\n\nsum(X1*X2)\n\n[1] 28.47696\n\n\nLes deux variables sont loin d’être orthogonales. Centrons les\n\nX1c &lt;- X1 - mean(X1)\nX2c &lt;- X2 - mean(X2)\nsum(X1c*X2c) \n\n[1] 1.039925\n\n\nRefaisons les régressions: la multiple\n\ndonc &lt;- cbind.data.frame(Y,X1c,X2c)\n(regmultc &lt;- lm(Y~1+X1c+X2c, data=donc))\n\n\nCall:\nlm(formula = Y ~ 1 + X1c + X2c, data = donc)\n\nCoefficients:\n(Intercept)          X1c          X2c  \n      2.522       -2.972        3.827  \n\n\nNous retrouvons que seul la constante change puis l’enchainement\n\n(regX1c &lt;- lm(Y~1+X1c, data=donc))\n\n\nCall:\nlm(formula = Y ~ 1 + X1c, data = donc)\n\nCoefficients:\n(Intercept)          X1c  \n      2.522       -2.441  \n\n\nIci les variables \\(X_{1c}\\) et \\(X_{2c}\\) étant centrées elles sont orthogonales à la constante. Nous avons donc \\[\\begin{align}\n      \\Im(\\mathbf{1}, X_{1c}, X_{2c}) = \\Im(\\mathbf{1}) \\stackrel{\\perp}{\\oplus} \\Im(X_{1c}, X_{2c})\n    \\end{align}\\] Le modèle précédent nous donne grâce à l’orthogonalité ci-dessus \\(\\hat Y_{1c} = P_{\\mathbf{1}, X_{1c}}Y=P_{\\mathbf{1}}Y + P_{X_{1c}}Y\\). Le modèle complet donne de son côté \\(\\hat Y = P_{\\mathbf{1}, X_{1c},X_{2c}}Y=P_{\\mathbf{1}}Y + P_{X_{1c},X_{2c}}Y\\). Nous retrouvons donc le coefficient constant qui est la coordonnée sur \\(\\mathbf{1}\\) de \\(P_{\\mathbf{1}}Y\\) dans les deux cas.\nEnchainons sur la régression sur résidus\n\nresidX1c &lt;- residuals(regX1c)\ndonc &lt;- cbind(donc, residX1c)\n(regX2residX1c &lt;- lm(residX1c~1+X2c, data=don))\n\n\nCall:\nlm(formula = residX1c ~ 1 + X2c, data = don)\n\nCoefficients:\n(Intercept)          X2c  \n  1.669e-16    3.758e+00  \n\n\nPuisque le vecteur est \\(Y- \\hat Y_{1}=Y-P_{\\mathbf{1}}Y-P_{X_{1c}}Y\\) et que \\(X_{2c}\\perp\\mathbf{1}\\) on a la projection des résidus sur \\(\\Im(\\mathbf{1}, X_{2c})\\) qui vaut \\[\\begin{align*}\n  P_{\\mathbf{1}, X_{2c}}(Y- \\hat Y_{1c})&= P_{\\mathbf{1}}(Y- \\hat Y_{1c}) + P_{X_{2c}}(Y- \\hat Y_{1c})\\\\\n  &=P_{\\mathbf{1}}Y - P_{\\mathbf{1}}P_{\\mathbf{1}, X_{1c}}Y + P_{X_{2c}}Y -  P_{X_{2c}}P_{\\mathbf{1}, X_{1c}}Y\\\\\n  &= P_{\\mathbf{1}}Y - P_{\\mathbf{1}}Y - 0 + P_{X_{2c}}Y - 0 - P_{X_{2c}}P_{X_{1c}}Y\\\\\n  &= P_{X_{2c}} Y - P_{X_{2c}}P_{X_{1c}}Y\n\\end{align*}\\] Cette dernière somme de projection est dans \\(\\Im(X_{1c}, X_{2c})\\) qui est un sous-espace orthogonal à \\(\\Im(\\mathbf{1})\\), d’où le coefficient 0 pour la constante.\nPour les prévisions rien ne change :\n\npredict(regmultc)[1:5]\n\n       1        2        3        4        5 \n3.348336 1.948483 3.088559 3.295485 3.369866 \n\n\n\n\n\nExercice 10 (TP : régression multiple et code R)  \n\n\nozone &lt;- read.table(\"../donnees/ozone_complet.txt\",sep=\";\",header=TRUE)\nnomvar &lt;- names(ozone)\n\n\n(ch &lt;- paste(nomvar[-1],collapse = \"+\"))\n\n[1] \"T6+T9+T12+T15+T18+Ne6+Ne9+Ne12+Ne15+Ne18+Vdir6+Vvit6+Vdir9+Vvit9+Vdir12+Vvit12+Vdir15+Vvit15+Vdir18+Vvit18+Vx+maxO3v\"\n\n\n\n(ch2 &lt;- paste(\"maxO3~\",ch,sep=\"\"))\n\n[1] \"maxO3~T6+T9+T12+T15+T18+Ne6+Ne9+Ne12+Ne15+Ne18+Vdir6+Vvit6+Vdir9+Vvit9+Vdir12+Vvit12+Vdir15+Vvit15+Vdir18+Vvit18+Vx+maxO3v\"\n\n\n\nform &lt;- formula(ch2)\nlm(form,data=ozone)\n\n\nCall:\nlm(formula = form, data = ozone)\n\nCoefficients:\n(Intercept)           T6           T9          T12          T15          T18  \n  31.628418    -1.937194     0.121834     1.522143     0.609047     0.091871  \n        Ne6          Ne9         Ne12         Ne15         Ne18        Vdir6  \n   0.091969    -0.692891    -0.941925    -0.033847    -0.107068    -0.001344  \n      Vvit6        Vdir9        Vvit9       Vdir12       Vvit12       Vdir15  \n   1.586427    -0.009728    -1.163728    -0.007144     0.182232    -0.003468  \n     Vvit15       Vdir18       Vvit18           Vx       maxO3v  \n   0.246885     0.006251     0.560660     0.255474     0.465541  \n\n\n\n\n\nExercice 11 (Régression orthogonale) Les vecteurs étant orthogonaux, nous avons \\(\\mathcal M_X = \\mathcal M_U \\stackrel{\\perp}{\\oplus} \\mathcal M_V\\). Nous pouvons alors écrire \\[\\begin{eqnarray*}\n\\hat Y_X = P_X Y &=& (P_U + P_{U^{\\perp}})P_X Y \\\\\n&=& P_U P_X Y + P_{U^{\\perp}}P_X Y =  P_U Y + P_{U^{\\perp}\\cap X} Y \\\\\n&=& \\hat Y_U + \\hat Y_V.\n\\end{eqnarray*}\\] La suite de l’exercice est identique. En conclusion, effectuer une régression multiple sur des variables orthogonales revient à effectuer \\(p\\) régressions simples.\n\n\nExercice 12 (Centrage, centrage-réduction et coefficient constant)  \n\nComme la dernière colonne de \\(X\\), notée \\(X_p\\) vaut \\(\\mathbf{1}_n\\) sa moyenne empirique vaut \\(1\\) et la variable centrée issue de \\(X_p\\) est donc \\(X_p -1\\times\\mathbf{1}_n=\\boldsymbol{0}_n\\).\nNous avons le modèle sur variable centrée \\[\n\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\varepsilon\\\\\nY-\\bar Y\\mathbf{1}_n&=&\\sum_{j=1}^{p-1}{(X_j -\\bar X_j\\mathbf{1}_n)\\tilde\\beta_j}+\\varepsilon\\\\\nY&=&\\sum_{j=1}^{p-1}{\\tilde\\beta_j X_j}+ \\Bigl(\\bar Y -\\sum_{j=1}^{p-1}{\\bar X_j\\tilde\\beta_j}\\Bigr)\\mathbf{1}_n+\\varepsilon.\n\\end{eqnarray*}\n\\] En identifiant cela donne \\[\n\\begin{eqnarray}\n\\beta_j&=&\\tilde\\beta_j,\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\nonumber\\\\\n\\beta_p&=&\\bar Y\\mathbf{1}_n-\\sum_{j=1}^{p-1}{\\bar X_j\\tilde\\beta_j}.\n\\end{eqnarray}\n\\tag{1}\\] Si l’on utilise des variables centrées dans le modèle de régression, on ne met pas de colonne \\(\\mathbf{1}\\) (pas de coefficient constant - intercept). Les coefficients du modèle sur les variables originales sont égaux à ceux sur les variables centrées et le coefficient constant est donné par la formule (équation 1).\nMaintenant les variables explicatives sont centrées et réduites : \\[\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\varepsilon\\\\\nY-\\bar Y\\mathbf{1}_n&=&\\sum_{j=1}^{p-1}{\\frac{(X_j -\\bar X_j\\mathbf{1}_n)}{\\hat\\sigma_{X_j}}\\tilde\\beta_j}+\\varepsilon\\\\\nY&=&\\sum_{j=1}^{p-1}{\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}X_j}+ \\Bigl(\\bar Y-\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}\\Bigr)\\mathbf{1}_n+\\varepsilon.\n\\end{eqnarray*}\\] En identifiant cela donne \\[\\begin{eqnarray*}\n\\beta_j&=&\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}},\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\\\\n\\beta_p&=&\\bar Y\\mathbf{1}_n-\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}.\n\\end{eqnarray*}\\] Nous obtenons ici que les coefficients du modèle sur les variables originales sont égaux à ceux sur les variables centrées-réduites divisés par l’écart-type empirique des variables explicatives. Plus la variable explicative \\(X_j\\) est dispersée, plus son coefficient \\(\\beta_j\\) sera réduit par rapport à \\(\\tilde\\beta_j\\). Le coefficient constant est donné par la formule ci-dessus.\nLa variable à expliquer \\(Y\\) est elle aussi centrée-réduite~: \\[\\begin{eqnarray*}\n\\tilde Y&=&\\tilde X\\tilde \\beta+ \\tilde\\varepsilon\\\\\n\\frac{Y-\\bar Y\\mathbf{1}_n}{\\hat\\sigma_Y}&=&\\sum_{j=1}^{p-1}{\\frac{(X_j -\\bar X_j\\mathbf{1}_n)}{\\hat\\sigma_{X_j}}\\tilde\\beta_j}+\\tilde\\varepsilon\\\\\nY&=&\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}X_j}+ \\Bigl(\\bar Y-\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}}\\Bigr)\\mathbf{1}_n+\\hat\\sigma_Y\\tilde\\varepsilon.\n\\end{eqnarray*}\\] En identifiant cela donne \\[\\begin{eqnarray*}\n\\beta_j&=&\\hat\\sigma_Y\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}},\\ \\forall j\\in\\{1,\\dotsc,p-1\\},\\\\\n\\beta_p&=&\\bar Y\\mathbf{1}_n-\\hat\\sigma_Y\\sum_{j=1}^{p-1}{\\bar X_j\\frac{\\tilde\\beta_j}{\\hat\\sigma_{X_j}}},\\\\\n\\varepsilon&=&\\hat\\sigma_Y\\tilde\\varepsilon.\n\\end{eqnarray*}\\] L’écart-type empirique de \\(Y\\) entre en jeu et nous constatons que les résidus du modèle “centré-réduit” sont égaux à ceux initiaux divisés par l’écart-type empirique de \\(Y\\).\nSimulons 3 variables explicatives avec GNU-R pour \\(n=100\\) individus selon une loi uniforme \\([0,1]\\) et équirépartie sur \\([1;10]\\):\n\nn &lt;- 100\nset.seed(1234) # pour fixer les simulations\nX1 &lt;- runif(n)\nX2 &lt;- seq(1, 10, length=100)\nX3 &lt;- rep(1,n)\nX &lt;- cbind(X1, X2, X3)\nhead(X)\n\n            X1       X2 X3\n[1,] 0.1137034 1.000000  1\n[2,] 0.6222994 1.090909  1\n[3,] 0.6092747 1.181818  1\n[4,] 0.6233794 1.272727  1\n[5,] 0.8609154 1.363636  1\n[6,] 0.6403106 1.454545  1\n\n\nSimulons maintenant \\(Y\\) selon le modèle en fixant par exemple \\(\\beta=(1, 2, 0.1, -4)'\\) et \\(\\sigma=0.1\\)\n\nbeta &lt;- c(1, 0.1, -4)\nsigma &lt;- 0.1\nset.seed(12345) # pour fixer les simulations\nY &lt;- X%*%beta + rnorm(n, sd=sigma)\nY[1:5]\n\n[1] -3.727744 -3.197663 -3.283474 -3.294698 -2.942132\n\n\nCentrons maintenant les variables avec \n\nYtilde &lt;- scale(Y, scale = FALSE)\nYtilde[1:5]\n\n[1] -0.73976069 -0.20968007 -0.29549076 -0.30671453  0.04585078\n\nXtilde &lt;- scale(X, scale = FALSE)\nhead(Xtilde)\n\n             X1        X2 X3\n[1,] -0.3237939 -4.500000  0\n[2,]  0.1848021 -4.409091  0\n[3,]  0.1717775 -4.318182  0\n[4,]  0.1858822 -4.227273  0\n[5,]  0.4234181 -4.136364  0\n[6,]  0.2028133 -4.045455  0\n\n\nEt éliminons la dernière colonne de \\(\\tilde X\\)\n\nXtilde &lt;- Xtilde[, -3]\n\nEffectuons les deux régressions en constituant deux data-frames. Nous n’ajoutons pas la constante dans les modèles (-1) car elle est déjà dans la dernière colonne de \\(X\\) pour le premier modèle et on ne la souhaite pas pour le second.\n\ndon &lt;- cbind.data.frame(Y,X)\ndontilde &lt;- cbind.data.frame(Ytilde,Xtilde)\nnames(dontilde) &lt;- c(\"Ytilde\", \"X1tilde\", \"X2tilde\")\nmod &lt;- lm(Y~-1+X1+X2+X3, data=don)\nmodtilde &lt;- lm(Ytilde~-1+X1tilde+X2tilde, data=dontilde)\n\nOn retrouve bien que les coefficients des variables centrées ou non sont identiques\n\ncoef(mod)\n\n        X1         X2         X3 \n 1.0225207  0.1030809 -4.0022782 \n\n\net que l’intercept est simplement est bien celui donné dans la formule (équation 1)\n\nmean(don$Y)- sum( coef(modtilde)*colMeans(X[,1:2]))\n\n[1] -4.002278\n\n\nCentrons et réduisons les variables explicatives (les 2 premières colonnes de \\(X\\))\n\nXtilde &lt;- scale(X[, 1:2], scale = TRUE)\nhead(Xtilde)\n\n             X1        X2\n[1,] -1.1617874 -1.706220\n[2,]  0.6630787 -1.671751\n[3,]  0.6163456 -1.637282\n[4,]  0.6669539 -1.602813\n[5,]  1.5192439 -1.568344\n[6,]  0.7277037 -1.533875\n\n\nReconstuisons le data-frame qui a changé et le modèle correspondant\n\ndontilde &lt;- cbind.data.frame(Ytilde,Xtilde)\nnames(dontilde) &lt;- c(\"Ytilde\", \"X1tilde\", \"X2tilde\")\nmodtilde &lt;- lm(Ytilde~-1+X1tilde+X2tilde, data=dontilde)\n\nRetrouvons les coefficients de départ pour les variables explicatives et pour la constante\n\nmean(don$Y)- sum( coef(modtilde)/apply(X[,-3], 2, sd) *\n                  colMeans(X[,1:2]) )\n\n[1] -4.002278\n\n\nCentrons et réduisons les variables explicatives et la variable à expliquer\n\nYtilde &lt;- scale(Y, scale = TRUE)\nYtilde[1:5]\n\n[1] -1.8943177 -0.5369313 -0.7566682 -0.7854091  0.1174109\n\n\nReconstuisons le data-frame qui a changé et le modèle correspondant\n\ndontilde &lt;- cbind.data.frame(Ytilde, Xtilde)\nnames(dontilde) &lt;- c(\"Ytilde\", \"X1tilde\", \"X2tilde\")\nmodtilde &lt;- lm(Ytilde~-1+X1tilde+X2tilde, data=dontilde)\n\nRetrouvons les coefficients de départ pour les variables explicatives\n\ncoef(modtilde)/apply(X[,-3],2,sd)*sd(Y)\n\n  X1tilde   X2tilde \n1.0225207 0.1030809 \n\n\net pour la constante\n\nmean(don$Y)- sum( coef(modtilde)/apply(X[,-3], 2, sd) *\n                  sd(Y)*colMeans(X[, 1:2]) )\n\n[1] -4.002278\n\n\n\n\n\nExercice 13 (Moindres carrés contraints)  \n\nL’estimateur des MC vaut \\[\\begin{eqnarray*}\n\\hat \\beta = (X'X)^{-1}X'Y,\n\\end{eqnarray*}\\]\nCalculons maintenant l’estimateur contraint. Nous pouvons procéder de deux manières différentes.\nLa première consiste à écrire le lagrangien \\[\\begin{eqnarray*}\n\\mathcal{L} = S(\\beta) - \\lambda'(R\\beta-r).\n\\end{eqnarray*}\\] Les conditions de Lagrange permettent d’obtenir un minimum \\[\\begin{eqnarray*}\n\\left\\{\n\\begin{array}{l}\n\\displaystyle\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = -2X'Y+2X'X\\hat{\\beta}_c-\nR'\\hat{\\lambda}=0,\\\\\n\\displaystyle\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = R\\hat{\\beta}_c-r=0,\n\\end{array}\n\\right.\n\\end{eqnarray*}\\] Multiplions à gauche la première égalité par \\(R(X'X)^{-1}\\), nous obtenons \\[\\begin{eqnarray*}\n-2 R(X'X)^{-1}X'Y+2R(X'X)^{-1}X'X \\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2R\\hat{\\beta}_c-R(X'X)^{-1}R'\\hat{\\lambda}&=&0\\\\\n-2 R(X'X)^{-1}X'Y+2r-R(X'X)^{-1}R'\\hat{\\lambda}&=&0.\n\\end{eqnarray*}\\] Nous obtenons alors pour \\(\\hat \\lambda\\) \\[\\begin{eqnarray*}\n\\hat \\lambda = 2 \\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right].\n\\end{eqnarray*}\\] Remplaçons ensuite \\(\\hat \\lambda\\) \\[\\begin{eqnarray*}\n-2X'Y+2X'X\\hat{\\beta}_c-R'\\hat{\\lambda}&=&0\\\\\n-2X'Y+2X'X\\hat{\\beta}_c-2R'\\left[R(X'X)^{-1}R'\\right]^{-1}\\left[r-R(X'X)^{-1}X'Y\\right]&=& 0,\n\\end{eqnarray*}\\] d’où nous calculons \\(\\hat \\beta_c\\) \\[\\begin{eqnarray*}\n\\hat \\beta_c &=& (X'X)^{-1}X'Y+(X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}\n(r-R\\hat \\beta)\\\\\n&=& \\hat \\beta + (X'X)^{-1}R'\\left[R(X'X)^{-1}R'\\right]^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\\] La fonction \\(S(\\beta)\\) à minimiser est une fonction convexe sur un ensemble convexe (contraintes linéaires), le minimum est donc unique.\nUne autre façon de procéder consiste à utiliser les projecteurs. Supposons pour commencer que \\(r=0\\), la contrainte vaut donc \\(R\\beta=0\\). Calculons analytiquement le projecteur orthogonal sur \\(\\mathcal M_0\\). Rappelons que \\(\\dim(\\mathcal M_0)=p-q\\), nous avons de plus \\[\\begin{eqnarray*}\nR \\beta &=& 0 \\quad \\quad\n\\Leftrightarrow \\quad \\beta \\in Ker(R)\\\\\nR (X'X)^{-1}X'X \\beta &=& 0\\\\\nU' X \\beta &=& 0\\quad \\quad \\hbox{où} \\quad \\quad U = X (X'X)^{-1}R'.\n\\end{eqnarray*}\\] Nous avons donc que \\(\\forall \\beta \\in \\ker(R)\\), \\(U' X \\beta = 0\\), c’est-à-dire que \\(\\mathcal M_U\\), l’espace engendré par les colonnes de \\(U\\), est orthogonal à l’espace engendré par \\(X\\beta\\), \\(\\forall \\beta \\in \\ker(R)\\). Nous avons donc que \\(\\mathcal M_U \\perp \\mathcal M_0\\). Comme \\(U=X[(X'X)^{-1}R']\\), \\(\\mathcal M_U \\subset \\mathcal M_X\\). En résumé, nous avons \\[\\begin{eqnarray*}\n\\mathcal M_U \\subset \\mathcal M_X \\quad  \\hbox{et}\n\\quad  \\mathcal M_U \\perp \\mathcal M_0 \\quad \\hbox{donc}\n\\quad \\mathcal M_U \\subset  (\\mathcal M_X \\cap \\mathcal M_0^{\\perp}).\n\\end{eqnarray*}\\] Afin de montrer que les colonnes de \\(U\\) engendrent \\(\\mathcal M_X \\cap \\mathcal M_0^{\\perp}\\), il faut démontrer que la dimension des deux sous-espaces est égale. Or le rang de \\(U\\) vaut \\(q\\) (\\(R'\\) est de rang \\(q\\), \\((X'X)^{-1}\\) est de rang \\(p\\) et \\(X\\) est de rang \\(p\\)) donc la dimension de \\(\\mathcal M_U\\) vaut \\(q\\). De plus, nous avons vu que \\[\\begin{eqnarray*}\n\\mathcal M_X = \\mathcal M_0 \\stackrel{\\perp}{\\oplus}\\left(\\mathcal M_0^{\\perp} \\cap \\mathcal M_X \\right)\n\\end{eqnarray*}\\] et donc, en passant aux dimensions des sous-espaces, nous en déduisons que \\(\\dim(\\mathcal M_0^{\\perp} \\cap \\mathcal M_X )=q\\). Nous venons de démontrer que \\[\\begin{eqnarray*}\n\\mathcal M_U = \\mathcal M_X \\cap \\mathcal M_0^{\\perp}.\n\\end{eqnarray*}\\] Le projecteur orthogonal sur \\(\\mathcal M_U=\\mathcal M_X \\cap \\mathcal M_0^{\\perp}\\) s’écrit \\[\\begin{eqnarray*}\nP_{U} = U (U'U)^{-1} U'= X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'.\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\hat Y - \\hat Y_0 &=& P_U Y\\\\\nX \\hat \\beta - X \\hat \\beta_0 &=& X (X'X)^{-1} R' [R(X'X)^{-1}R']^{-1}R(X'X)^{-1}X'Y\\\\\n&=& X (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\\] Cela donne \\[\\begin{eqnarray*}\n\\hat \\beta_0 = \\hat \\beta - (X'X)^{-1} R [R(X'X)^{-1}R']^{-1}R \\hat \\beta.\n\\end{eqnarray*}\\] Si maintenant \\(r\\neq 0\\), nous avons alors un sous-espace affine défini par \\(\\{\\beta\\in \\mathbb R^p : R\\beta=r\\}\\) dans lequel nous cherchons une solution qui minimise les moindres carrés. Un sous-espace affine peut être défini de manière équivalente par un point particulier \\(\\beta_p \\in \\mathbb R^p\\) tel que \\(R\\beta_p=r\\) et le sous-espace vectoriel associé \\(\\mathcal M_0^v=\\{\\beta\\in \\mathbb R^p : R\\beta=0\\}\\). Les points du sous-espace affine sont alors \\(\\{\\beta_0 \\in \\mathbb R^p : \\beta_0=\\beta_p+\\beta_0^v, \\beta_0^v \\in \\mathcal M_0^v \\quad et \\quad \\beta_p : R\\beta_p=r\\}\\). La solution qui minimise les moindres carrés, notée \\(\\hat \\beta_0\\), est élément de ce sous-espace affine et est définie par \\(\\hat \\beta_0=\\beta_p+\\hat \\beta_0^v\\) où \\[\\begin{eqnarray*}\n\\hat \\beta_0^v = \\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta.\n\\end{eqnarray*}\\] Nous savons que \\(R\\beta_p=r\\) donc \\[\\begin{eqnarray*}\nR\\beta_p = [R(X'X)^{-1}R'][R(X'X)^{-1}R']^{-1}r\n\\end{eqnarray*}\\] donc une solution particulière est \\(\\beta_p = (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r\\). La solution \\(\\hat \\beta_0\\) qui minimise les moindres carrés sous la contrainte \\(R\\beta=r\\) est alors \\[\n\\begin{align}\n\\hat \\beta_0 &= \\beta_p+\\hat \\beta_0^v\\\\\\nonumber\n&=(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}r +\n\\hat \\beta - (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R\\hat \\beta\\\\\\nonumber\n&=\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{align}\n\\tag{2}\\]\nCalculons l’EQM de \\(\\hat \\beta\\) qui vaut selon la formule classique: \\[\\begin{align*}\n      \\mathop{\\mathrm{EQM}}&= (\\mathbf E(\\hat \\beta) - \\beta) (\\mathbf E(\\hat \\beta) - \\beta)' + \\mathop{\\mathrm{V}}(\\hat\\beta)\n    \\end{align*}\\] avec \\(\\mathbf E(\\hat \\beta)=\\beta\\) (sous l’hypothèse que \\(Y\\) est généré par le modèle de régression) et\\(\\mathop{\\mathrm{V}}(\\hat\\beta) =\\sigma^{2} (X'X)^{-1}\\).\nPour l’EQM de \\(\\hat \\beta_0\\) qui vaut selon la formule classique: \\[\\begin{align*}\n      \\mathop{\\mathrm{EQM}}&= (\\mathbf E(\\hat \\beta_0) - \\beta_0) (\\mathbf E(\\hat \\beta_0) - \\beta_0)' + \\mathop{\\mathrm{V}}(\\hat\\beta_0)\n    \\end{align*}\\] calculons d’abord \\(\\mathbf E(\\hat \\beta_0)\\) en utilisant l’équation (équation 2): \\[\\begin{align*}\n      \\mathbf E(\\hat \\beta_0)&=\\mathbf E(\\hat \\beta) + \\mathbf E[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n      &=\\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\beta)\n    \\end{align*}\\] puisque \\(\\hat \\beta\\) est sans biais. Si nous supposons que le modèle satisfait la contrainte (\\(R\\beta=r\\)) alors là encore le biais est nul.\nCalculons maintemant la variance: \\[\\begin{align*}\n      \\mathop{\\mathrm{V}}(\\hat\\beta_0) &=  \\mathop{\\mathrm{V}}[\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n                      &=\\mathop{\\mathrm{V}}(\\hat\\beta) + \\mathop{\\mathrm{V}}[(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)] +\\\\\n                      &\\quad \\quad 2\\mathop{\\mathrm{Cov}}[\\hat\\beta ; (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta)]\\\\\n      &=   \\sigma^{2}V_{X}^{-1} + (\\mathrm{II}) +  (\\mathrm{III})\n    \\end{align*}\\] Intéressons nous à la seconde partie \\((\\mathrm{II})\\). Comme \\(X\\) est déterministe ainsi que \\(R\\) et \\(r\\) on a en posant pour alléger \\(V_{X}=(X'X)\\) (matrice symétrique) \\[\\begin{align*}\n      (\\mathrm{II}) &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\mathop{\\mathrm{V}}(r-R\\hat \\beta)\n                        [RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                    &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} \\mathop{\\mathrm{V}}(R\\hat \\beta)[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n                    &= V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R\\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\\\\\n       &= \\sigma^{2}V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1} R V_{X}^{-1}\n    \\end{align*}\\] La troisième partie est \\[\\begin{align*}\n      (\\mathrm{III}) &= 2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}\\mathop{\\mathrm{Cov}}[\\hat\\beta ; (r-R\\hat \\beta)]\\\\\n                     &=-2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\mathop{\\mathrm{Cov}}[\\hat\\beta ;\\hat\\beta]\\\\\n      &= -2V_{X}^{-1}R'[RV_{X}^{-1}R']^{-1}R\\sigma^{2}V_{X}^{-1}\n    \\end{align*}\\] Ce qui donne au final \\[\\begin{align*}\n      \\mathop{\\mathrm{V}}(\\hat\\beta_0) &=\\sigma^{2}(X'X)^{-1} - \\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}\n    \\end{align*}\\] L’écart entre les 2 variances est donc de \\(\\sigma^{2}(X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}R(X'X)^{-1}\\) qui est une matrice de la forme \\(A'A\\) donc semi-définie positive."
  },
  {
    "objectID": "correction/chap3.html",
    "href": "correction/chap3.html",
    "title": "3 Validation du modèle",
    "section": "",
    "text": "Exercice 1 (Questions de cours) C si \\(\\mathbf{1}\\) fait partie des variables ou si \\(\\mathbf{1} \\in \\Im(X)\\), A, C, C, A.\n\n\nExercice 2 (Propriétés d’une matrice de projection) La trace d’un projecteur vaut la dimension de l’espace sur lequel s’effectue la projection, donc \\(\\mathop{\\mathrm{tr}}(P_X)=p\\). Le second point découle de la propriété \\(P^2=P\\).\nLes matrices \\(P_X\\) et \\(P_XP_X\\) sont égales, nous avons que \\((P_X)_{ii}\\) vaut \\((P_XP_X)_{ii}\\). Cela s’écrit \\[\\begin{eqnarray*}\nh_{ii} &=&  \\sum_{k=1}^n h_{ik} h_{ki}\\\\\n&=& h_{ii}^2 + \\sum_{k=1, k \\neq i}^n h_{ik}^2\\\\\nh_{ii}(1-h_{ii}) &=& \\sum_{k=1, k \\neq i}^n h_{ik}^2.\n\\end{eqnarray*}\\] La dernière quantité de droite de l’égalité est positive et donc le troisième point est démontré. En nous servant de cet écriture les deux derniers points sont aussi démontrés.\nNous pouvons écrire \\[\\begin{eqnarray*}\nh_{ii}(1-h_{ii}) &=& h_{ij}^2 + \\sum_{k=1, k \\neq i ,j }^n h_{ik}^2.\n\\end{eqnarray*}\\] La quantité de gauche est maximum lorsque \\(h_{ii}=0.5\\) et vaut alors \\(0.25\\). Le quatrième point est démontré.\n\n\nExercice 3 (Lemme d’invertion matricielle) Commençons par effectuer les calculs en notant que la quantité \\(u'M^{-1}v\\) est un scalaire que nous noterons \\(k\\). Nous avons \\[\\begin{eqnarray*}\n\\left(M+uv'\\right)\\left(M^{-1}-\\frac{M^{-1}uv'M^{-1}}{1+u'M^{-1}v}\\right)\n&=&MM^{-1}-\\frac{MM^{-1}uv'M^{-1}}{1+k}+uv'M^{-1}\n-\\frac{uv'M^{-1}uv'M^{-1}}{1+k}\\\\\n&=&I+\\frac{-uv'M^{-1}+uv'M^{-1}+kuv'M^{-1}-ukv'M^{-1}}{1+k}.\n\\end{eqnarray*}\\] Le résultat est démontré.\n\n\nExercice 4 (Résidus studentisés)  \n\nIl suffit d’utiliser la définition du produit matriciel et de la somme matricielle et d’identifier les 2 membres des égalités.\nEn utilisant maintenant l’égalité de l’exercice précédent sur les inverses, avec \\(u=-x_i\\) et \\(v=x_i'\\), nous avons \\[\\begin{eqnarray*}\n(X'_{(i)}X_{(i)})^{-1}=(X'X-x_{i}x_{i}')^{-1}=(X'X)^{-1}+\n\\frac{(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}}\n{1-x_{i}'(X'X)^{-1}x_{i}}.%\\label{eq:hiieta}\n\\end{eqnarray*}\\] La définition de \\(h_{ii}=x_{i}'(X'X)^{-1}x_{i}\\) donne le résultat.\nCalculons la prévision où \\(\\hat \\beta_{(i)}\\) est l’estimateur de \\(\\beta\\) obtenu sans la \\(i^e\\) observation\n\n\\[\\begin{eqnarray*}\n\\hat y_i^p\n= x_{i}'\\hat \\beta_{(i)}\n&=& x_{i}' (X'_{(i)}X_{(i)})^{-1}X'_{(i)}Y_{(i)}\\\\\n&=& x_{i}'\\left[(X'X)^{-1}\n+ \\frac{(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}}{1-h_{ii}}\n\\right]\\left(X'Y-x_{i}'y_i\\right)\\\\\n&=& x_{i}' \\hat \\beta + \\frac{h_{ii}}{1-h_{ii}}x_{i}' \\hat \\beta\n- h_{ii}y_i -\\frac{h_{ii}^2}{1-h_{ii}}y_i\\\\\n&=& \\frac{1}{1-h_{ii}}\\hat y_i - \\frac{h_{ii}}{1-h_{ii}}y_i.\n\\end{eqnarray*}\\]\n\nCe dernier résultat donne \\[\\begin{eqnarray*}\n\\hat \\varepsilon_i = (1-h_{ii})(y_i-\\hat y^p_i).\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\nt^*_i &=& \\frac{\\hat \\varepsilon_i}{\\hat \\sigma_{(i)}\\sqrt{1-h_{ii}}}\\\\\n&=&\\frac{\\sqrt{(1-h_{ii})}(y_i - \\hat y_i^p)}{\\hat \\sigma_{(i)}}.\n\\end{eqnarray*}\\] Pour terminer, remarquons qu’en multipliant l’égalité de la question 3 à gauche par \\(x_{i}'\\) et à droite par \\(x_{i}\\) \\[\\begin{eqnarray*}\nx_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}\n&=& h_{ii}+ \\frac{h_{ii}^2}{1-h_{ii}}.\\\\\n1+x_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}\n&=& 1 +\\frac{h_{ii}}{1-h_{ii}}=\\frac{h_{ii}}{1-h_{ii}}.\n\\end{eqnarray*}\\]\nUtilisons l’expression \\[\\begin{eqnarray*}\nt^*_i=\\frac{y_i-\\hat y_i^p }\n{\\hat \\sigma_{(i)}\\sqrt{1+x_{i}'(X'_{(i)}X_{(i)})^{-1}x_{i}}}.\n\\end{eqnarray*}\\] Nous pouvons alors appliquer la preuve de la proposition 5.4 page 97, en constatant que la \\(i^e\\) observation est une nouvelle observation. Nous avons donc \\(n-1\\) observations pour estimer les paramètres, cela donne donc un Student à \\(n-1-p\\) paramètres.\n\n\n\nExercice 5 (Distance de Cook)  \n\nNous reprenons une partie des calculs de l’exercice précédent : \\[\\begin{eqnarray*}\n\\hat \\beta_{(i)} &=& (X'_{(i)}X_{(i)})^{-1}X'_{(i)}Y_{(i)}\\\\\n&=& (X'X)^{-1}[X'Y-x_{i}y_i]+\\frac{1}{1-h_{ii}}\n(X'X)^{-1}x_{i}x_{i}'(X'X)^{-1}[X'Y-x_{i}y_i]\\\\\n&=& \\hat \\beta - (X'X)^{-1}x_{i}y_i + \\frac{1}{1-h_{ii}}\n(X'X)^{-1}x_{i}x_{i}'\\hat \\beta - \\frac{h_{ii}}{1-h_{ii}}\n(X'X)^{-1}x_{i}y_i,\n\\end{eqnarray*}\\] d’où le résultat.\nPour obtenir la seconde écriture de la distance de Cook, nous écrivons d’abord que \\[\\begin{eqnarray*}\n\\hat \\beta_{(i)} - \\hat \\beta = \\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\n(X'X)^{-1}x_{i}.\n\\end{eqnarray*}\\] Puis nous développons \\[\\begin{eqnarray*}\nC_i &=& \\frac{1}{p \\hat \\sigma^2}(\\hat \\beta_{[i]}-\\hat \\beta)'\nX'X(\\hat \\beta_{(i)}-\\hat \\beta)\\\\\n&=& \\frac{1}{p \\hat \\sigma^2} \\left(\\frac{- \\hat \\varepsilon_i}{1-h_{ii}}\\right)^2 x_{i}' (X'X)^{-1}(X'X)(X'X)^{-1}x_{i}.\n\\end{eqnarray*}\\] Le résultat est démontré.\n\n\n\nExercice 6 (Régression partielle) Nous avons le modèle suivant : \\[\\begin{eqnarray*}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\beta_jP_{X_{\\bar{j}}^\\perp} X_j + \\eta.\n\\end{eqnarray*}\\] L’estimateur des moindres carrés \\(\\tilde\\beta_j\\) issu de ce modèle vaut \\[\\begin{eqnarray*}\n\\tilde \\beta_j = (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y.\n\\end{eqnarray*}\\] La projection de \\(Y\\) sur \\(\\Im(X_{\\bar{j}})\\) (i.e. la prévision par le modèle sans la variable \\(X_j\\)) peut s’écrire comme la projection \\(Y\\) sur \\(\\Im(X)\\) qui est ensuite projetée sur \\(\\Im(X_{\\bar{j}})\\), puisque \\(\\Im(X_{\\bar{j}})\\subset \\Im(X)\\). Ceci s’écrit \\[\\begin{eqnarray*}\nP_{X_{\\bar{j}}}Y&=&P_{X_{\\bar{j}}}P_{X}Y=P_{X_{\\bar{j}}}X\\hat{\\beta}\n=P_{X_{\\bar{j}}}(X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jX_j)\n=X_{\\bar{j}}\\hat\\beta_{\\bar{j}}+\\hat\\beta_jP_{X_{\\bar{j}}}X_j,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\nX_{\\bar{j}}\\hat\\beta_{\\bar{j}} = P_{X_{\\bar{j}}} Y -\n\\hat\\beta_jP_{X_{\\bar{j}}}X_j.\n\\end{eqnarray*}\\] Récrivons les résidus \\[\\begin{eqnarray*}\n\\hat{\\varepsilon}&=&P_{X^\\perp} Y=Y-X\\hat\\beta\n=Y-X_{\\bar{j}}\\hat\\beta_{\\bar{j}}-\\hat\\beta_jX_j\\nonumber\\\\\n&=&Y-P_{X_{\\bar{j}}}Y + \\hat\\beta_jP_{X_{\\bar{j}}}X_j  -\\hat\\beta_j X_j\\nonumber\\\\\n&=&(I-P_{X_{\\bar{j}}})Y - \\hat\\beta_j(I-P_{X_{\\bar{j}}})X_j\\nonumber\\\\\n&=&P_{X_{\\bar{j}}^\\perp} Y-\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j.%\\label{eq:origine:residpartiel}\n\\end{eqnarray*}\\] En réordonnant cette dernière égalité, nous pouvons écrire \\[\\begin{eqnarray}\nP_{X_{\\bar{j}}^\\perp} Y&=&\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon}.\\nonumber\n\\end{eqnarray}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\tilde\\beta_j &=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j P_{X_{\\bar{j}}^\\perp} Y\\\\\n&=& (X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1}\nX'_j(\\hat\\beta_jP_{X_{\\bar{j}}^\\perp} X_j+\\hat{\\varepsilon})\\\\\n&=& \\hat\\beta_j +(X'_j P_{X_{\\bar{j}}^\\perp} X_j)^{-1} X'_j\\hat{\\varepsilon}).\n\\end{eqnarray*}\\] Le produit scalaire \\(X'_j\\hat{\\varepsilon} = \\langle X_j,\\hat{\\varepsilon} \\rangle\\) est nul car les deux vecteurs appartiennent à des sous-espaces orthogonaux, d’où le résultat.\n\n\nExercice 7 (TP : Résidus partiels)  \n\nImportation\n\ndon &lt;- read.table(\"../donnees/tprespartiel.dta\", header=TRUE, sep=\";\")\n\nEstimation du modèle\n\nmod &lt;- lm(Y~X1+X2+X3+X4, data=don)\n\nAnalyse des résidus partiels. Commençons par calculer les résidus partiels (matrice \\(n\\times p\\))\n\nrpartiel &lt;- residuals(mod, type=\"partial\")\n\nCréons un data-frame à 3 colonnes et \\(np\\) lignes: la colonne de nom de la variable (X1 répété \\(n\\) fois, X2 répété \\(n\\) fois, X3 répété \\(n\\) fois et X4 répété \\(n\\) fois), les variables X1, X2, X3 et X4 et les résidus partiels.\n\nnoms &lt;- rep(names(don)[1:4], each=nrow(don))\nX &lt;-  as.vector(data.matrix(don[,1:4]))\ndonlong &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\n\nLa représentation avec est donnée par\n\nlibrary(ggplot2)\nggplot(donlong, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\nLes 3 premières variables montrent des tendances linéaires (ou aucune pour la troisième) alors que la troisième semble montrer plutôt une tendance quadratique.\nRefaisons le modèle avec X5 :\n\ndon &lt;- cbind.data.frame(don, X5=don$X4^{2})\nmod2 &lt;- lm(Y~X1+X2+X3+X5, data=don)\nrpartiel &lt;- residuals(mod2, type=\"partial\")\nnoms &lt;- rep(names(don)[c(1:3,6)], each=nrow(don))\nX &lt;-  as.vector(data.matrix(don[,c(1:3,6)]))\ndonlong2 &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\nggplot(donlong2, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\net nous constatons que les résidus partiels sont tous à tendance linéaire. Les 2 modèles ayant le même nombre de variables nous pouvons les comparer via leur \\(\\mathop{\\mathrm{R^2}}\\) qui valent 0.986 et 0.9966. La seconde modélisation est la meilleure tant pour la qualité globale que pour l’analyse des résidus.\nAvec le second jeu de données\n\ndonbis &lt;- read.table(\"../donnees/tpbisrespartiel.dta\", header=TRUE, sep=\";\")\nmodbis &lt;- lm(Y~X1+X2+X3+X4, data=donbis)\nrpartiel &lt;- residuals(modbis, type=\"partial\")\nnoms &lt;- rep(names(donbis)[1:4], each=nrow(donbis))\nX &lt;-  as.vector(data.matrix(donbis[,1:4]))\ndonlongbis &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\nggplot(donlongbis, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\nNous voyons clairement une sinusoïde de type \\(\\sin(-2\\pi X_4)\\) sur le dernier graphique. Changeons X4\n\ndonbis &lt;- cbind.data.frame(donbis, X5=sin(-2*pi*donbis$X4))\nmodbis2 &lt;- lm(Y~X1+X2+X3+X5, data=donbis)\nrpartiel &lt;- residuals(modbis2, type=\"partial\")\nnoms &lt;- rep(names(donbis)[c(1:3,6)], each=nrow(donbis))\nX &lt;-  as.vector(data.matrix(donbis[,c(1:3,6)]))\ndonlongbis2 &lt;- cbind.data.frame(noms, X, rpartiel=as.vector(rpartiel))\nggplot(donlongbis2, aes(X, rpartiel)) + geom_point() +\n  facet_wrap(vars(noms), scale=\"free\")\n\n\n\n\nLes résidus partiels sont tous à tendance linéaire et donc corrects. La qualité globale analysée par \\(\\mathop{\\mathrm{R^2}}\\) augmente elle aussi de 0.8106 à 0.9985."
  },
  {
    "objectID": "correction/chap4.html",
    "href": "correction/chap4.html",
    "title": "4 Extensions : non-inversibilité et (ou) erreurs corrélées",
    "section": "",
    "text": "Exercice 1 (Questions de cours) B, B, B, A\n\n\nExercice 2 (Corrélation multiple et hypothèse \\(\\mathcal{H}_1\\))  \n\nMontrons que la moyenne empirique de \\(X\\hat \\beta\\) vaut \\(\\bar Y\\). Le vecteur moyenne est obtenu en projetant sur \\(\\mathbf{1}_n\\). En effet, comme \\[\\begin{eqnarray*}\nP_{\\mathbf{1}}&=&\\mathbf{1}_n(\\mathbf{1}_n'\\mathbf{1}_n)^{-1}\\mathbf{1}_n'=\\frac{1}{n}\\mathbf{1}_n\\mathbf{1}_n',\n\\end{eqnarray*}\\] nous avons, pour une variable \\(Z=(Z_1,\\dotsc,Z_n)'\\), \\[\\begin{eqnarray*}\nP_{\\mathbf{1}}Z&=&=\\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n'Z= \\frac{1}{n}\\mathbf{1}_n\\sum_{i=1}^{n}{Z_i}=\\bar Z\\mathbf{1}_n.\n\\end{eqnarray*}\\] Comme \\(\\mathbf{1}_n\\in\\Im(X)\\), nous avons \\[\\begin{eqnarray*}\n\\bar Y&=&P_\\mathbf{1}Y=P_\\mathbf{1}P_XY=P_\\mathbf{1}X\\hat \\beta,\n\\end{eqnarray*}\\] c’est-à-dire que la moyenne empirique de \\(X\\hat \\beta\\) vaut \\(\\bar Y\\).\nLe coefficient de corrélation entre \\(\\hat Y\\) et \\(Y\\) élevé au carré s’écrit donc \\[\\begin{eqnarray*}\n\\rho^2(\\hat Y,Y)&=&\\frac{\\langle \\hat Y-\\bar Y,Y-\\bar Y\\rangle^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}\\\\\n&=&\\frac{\\langle \\hat Y-\\bar Y,Y-\\hat Y+\\hat Y-\\bar Y\\rangle^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}\\\\\n&=&\\Bigl\\{\\frac{\\langle \\hat Y-\\bar Y,Y-\\hat Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|}+\\frac{\\langle \\hat Y-\\bar Y,\\hat Y-\\bar Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|}\\Bigl\\}^2.\n\\end{eqnarray*}\\] Comme \\((Y-\\hat Y)\\in\\Im(X)^\\perp\\) et que \\((\\hat Y-\\bar Y)\\in\\Im(X)\\), nous avons \\(\\langle \\hat Y-\\bar Y,Y-\\hat Y\\rangle=0\\) et donc \\[\\begin{eqnarray*}\n\\rho^2(\\hat Y,Y)&=&\\frac{\\|\\hat Y-\\bar Y\\|^2 \\|\\hat Y-\\bar Y\\|^2}{\\|\\hat Y-\\bar Y\\|^2\\|Y-\\bar Y\\|^2}=\\mathop{\\mathrm{R^2}}2.\n\\end{eqnarray*}\\]\n\nEn effectuant le calcul nous trouvons que \\(Y-2X_1+2X_2=3\\eta\\).\nEn calculant les normes carrées, nous avons \\[\\begin{eqnarray*}\n\\|X_1\\|^2&=&1^2+1^2+1^2=3,\\\\\n\\|X_2\\|^2&=&1/2+1/2+2=3,\\\\\n\\|X_3\\|^2&=&3/2+3/2=3.\n\\end{eqnarray*}\\] En calculant les produits scalaires, nous avons \\[\\begin{eqnarray*}\n\\langle X_1,X_2\\rangle&=&1\\times 1/\\sqrt{2}+ 1\\times 1/\\sqrt{2} +1\\times (-\\sqrt{2})\n=\\sqrt{2}-\\sqrt{2}=0,\\\\\n\\langle X_1,\\eta\\rangle&=&\\sqrt{3}/\\sqrt{2}-\\sqrt{3}/\\sqrt{2}=0,\\\\\n\\langle X_2,\\eta\\rangle&=&1/\\sqrt{2}\\times\\sqrt{3}/\\sqrt{2}-1/\\sqrt{2}\\times\\sqrt{3}/\\sqrt{2}=0.\n\\end{eqnarray*}\\]\nLa représentation graphique est :\n\n\n\n\n\nNous avons ici \\(X_1\\in\\Im(X)\\), \\(X_2\\in\\Im(X)\\) et \\(\\eta\\in\\Im(X)^\\perp\\), ce qui permet de trouver \\(\\hat Y\\)~: \\[\\begin{eqnarray*}\nP_XY&=&P_X(2X_1-2X_2+3\\eta)=2P_XX_1 -2P_XX_2+3P_X\\eta=\\\\\n&=&2X_1-2X_2=(2-\\sqrt{2},2-\\sqrt{2},2-2\\sqrt{2})'.\n\\end{eqnarray*}\\]\nPuisque \\(\\mathbf{1}\\) fait partie des variables explicatives, nous avons \\[\\begin{eqnarray*}\n\\rho(Y,\\hat Y)&=&\\frac{\\langle Y-\\bar Y,\\hat Y-\\bar Y\\rangle}{\\|\\hat Y-\\bar Y\\|\\|Y-\\bar Y\\|},\n\\end{eqnarray*}\\] ce qui est la définition du cosinus de l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y\\hat Y}\\).\nNotons par \\(Y_\\alpha\\) le vecteur \\(X\\alpha\\). Sa moyenne vaut \\(\\bar Y_\\alpha\\). Nous avons maintenant le cosinus de l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\). Graphiquement, la moyenne de \\(Y_\\alpha\\) est la projection sur \\(X_1=\\mathbf{1}_3\\).\nLa représentation graphique nous permet de voir que l’angle entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\) est le même que celui entre \\(\\overrightarrow{\\bar YY}\\) et \\(\\overrightarrow{\\bar Y\\hat Y}\\). L’angle est minimum (et le cosinus maximum) quand \\(\\alpha=\\hat\\beta\\) ou pour tout \\(\\alpha\\) tel que \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}=k\\overrightarrow{\\bar Y\\hat Y}\\) avec \\(k&gt;0\\).\nDu fait de l’orthogonalité entre \\(X_1\\) et \\(X_2\\), \\(\\overrightarrow{\\bar Y_\\alpha Y_\\alpha}\\) est toujours colinéaire à \\(\\overrightarrow{\\bar Y\\hat Y}\\), seul le signe change en fonction de l’orientation des vecteurs (même sens ou sens opposé).\n\nComme \\(\\rho(X_j;X_k)=1\\) alors \\(R(X_j;(\\mathbf{1},X_j))=1\\) et donc puisque la constante fait partie du modèle \\(R(X_j;X_{(j)})=1\\). L’hypothèse \\({\\mathcal{H}}_1\\) n’est donc pas vérifiée.\n\n\n\nExercice 3 (EQM de la régression Ridge)  \n\nLes démonstrations figurent en page 77 : \\[\\begin{eqnarray*}\nB(\\hat \\beta_{ridge}) &=& -\\kappa (X'X + \\kappa I)^{-1} \\beta,\\\\\nV(\\hat \\beta_{\\mathrm{ridge}})&=&\\sigma^2(X'X + \\kappa I)^{-1}X'X(X'X + \\kappa I)^{-1}\\\\\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})&=&(X'X + \\kappa I)^{-1}\\left[\\kappa^2\\beta \\beta'+\\sigma^2(X'X) \\right](X'X + \\kappa I)^{-1}.\n\\end{eqnarray*}\\]\nPuisque \\(X'X=P\\mathop{\\mathrm{diag}}(\\lambda_i) P'\\), nous avons \\[\\begin{eqnarray*}\n(X'X + \\kappa I)&=&P\\mathop{\\mathrm{diag}}(\\lambda_i) P'+ \\kappa PP'=P\\mathop{\\mathrm{diag}}(\\lambda_i+\\kappa)P'.\n\\end{eqnarray*}\\] En se rappelant que \\(P^{-1}=P'\\), son inverse vaut \\[\\begin{eqnarray*}\n(X'X + \\kappa I)^{-1}&=&P\\mathop{\\mathrm{diag}}(1/(\\lambda_i+\\kappa))P'.\n\\end{eqnarray*}\\] Nous avons donc \\[\\begin{equation*}\n\\begin{split}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})&=P\\mathop{\\mathrm{diag}}(\\frac{1}{\\lambda_i+\\kappa})P'\\left[\\kappa^2\\beta \\beta'+\\sigma^2(X'X) \\right]P\\mathop{\\mathrm{diag}}(\\frac{1}{\\lambda_i+\\kappa})P'\\\\\n&=P\\mathop{\\mathrm{diag}}(\\frac{1}{\\lambda_i+\\kappa})\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\mathop{\\mathrm{diag}}(\\frac{1}{\\lambda_i+\\kappa})P'.\n\\end{split}\n\\end{equation*}\\] Nous en déduisons que sa trace vaut \\[\\begin{equation*}\n\\begin{split}\n\\mathop{\\mathrm{tr}}\\left\\{EQM(\\hat \\beta_{\\mathrm{ridge}})\\right\\}&=\\mathop{\\mathrm{tr}}\\left\\{\\mathop{\\mathrm{diag}}(\\frac{1}{\\lambda_i+\\kappa})\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\right.\\\\\n&\\quad \\left.\\mathop{\\mathrm{diag}}(\\frac{1}{\\lambda_i+\\kappa})P'P\\right\\},\n\\end{split}\n\\end{equation*}\\] et, comme \\(P'P=I_p\\), nous avons alors \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{tr}}\\left\\{EQM(\\hat \\beta_{\\mathrm{ridge}})\\right\\}\n&=&\\mathop{\\mathrm{tr}}\\left\\{\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2 I_p\\right]\\mathop{\\mathrm{diag}}(\\frac{1}{(\\lambda_i+\\kappa)^2})\\right\\}.\n\\end{eqnarray*}\\] Le \\(i^e\\) élément de la diagonale de la matrice \\(P'\\beta\\beta'P\\) vaut \\([P'\\beta]_i^2\\). Celui de \\(\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2  I_p\\right]\\) vaut \\(\\kappa^2[P'\\beta]_i^2+\\sigma^2\\) et celui de \\[\\left[\\kappa^2(P'\\beta\\beta'P)+\\sigma^2\n  I_p\\right]\\mathop{\\mathrm{diag}}(\\frac{1}{(\\lambda_i+\\kappa)^2})\\] vaut donc \\[\\kappa^2[P'\\beta]_i^2+\\sigma^2/(\\lambda_i+\\kappa)^2.\\] On en déduit le résultat annoncé car la trace est la somme des éléments diagonaux d’une matrice.\nL’estimateur des MC est non biaisé et son \\(\\mathop{\\mathrm{EQM}}\\) vaut sa variance : \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})&=&\\sigma^2(X'X)^{-1}.\n\\end{eqnarray*}\\] Nous avons alors \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X + \\kappa I)(X'X)^{-1}\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X(X'X)^{-1} + \\kappa I(X'X)^{-1})\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(I\\!+\\!\\kappa (X'X)^{-1})(X'X \\!+\\! \\kappa I)(X'X \\!+\\! \\kappa I)^{-1}\\\\\n\\!&=&\\!\\sigma^2(X'X \\!+\\! \\kappa I)^{-1}(X'X\\!+\\!2 \\kappa I \\!+\\! \\kappa^2 (X'X)^{-1})(X'X \\!+\\! \\kappa I)^{-1}.\n\\end{eqnarray*}\\]\nLe calcul de \\(\\Delta=\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{ridge}})- \\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\\) est immédiat en utilisant l’expression précédente de \\(\\mathop{\\mathrm{EQM}}(\\hat \\beta_{\\mathrm{MC}})\\) et celle rappelée en question 1.\nEn utilisant le théorème proposé avec \\(A=(X'X + \\kappa I)^{-1}\\) et \\(B=(\\sigma^2(2I_p+\\kappa^2(X'X)^{-1})-\\kappa\\beta\\beta')\\) nous obtenons le résultat demandé. Cette condition dépend de \\(\\beta\\) qui est inconnu, mais aussi de \\(X\\), c’est-à-dire des mesures obtenues.\nIntéressons-nous à la matrice \\(\\gamma\\gamma'\\). Cette matrice est symétrique donc diagonalisable, de valeurs propres positives ou nulles. La somme de ses valeurs propres est égale à la trace de cette matrice \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{tr}}(\\gamma\\gamma')=\\mathop{\\mathrm{tr}}(\\gamma'\\gamma)=\\gamma'\\gamma.\n\\end{eqnarray*}\\] Montrons que cette matrice n’a qu’une seule valeur propre non nulle \\(\\gamma'\\gamma\\). Pour cela, considérons le vecteur \\(\\gamma\\in\\mathbb R^p\\) et montrons qu’il est vecteur propre de \\(\\gamma\\gamma'\\) associé à la valeur propre \\(\\gamma'\\gamma\\)~: \\[\\begin{eqnarray*}\n(\\gamma\\gamma')\\gamma&=&\\gamma(\\gamma'\\gamma)=(\\gamma'\\gamma)\\gamma.\n\\end{eqnarray*}\\] Nous avons donc un vecteur propre de \\(\\gamma\\gamma'\\) qui est \\(\\gamma\\) associé à la valeur propre \\(\\gamma'\\gamma\\). De plus, nous savons que la somme des valeurs propres positives ou nulles de \\(\\gamma\\gamma'\\) vaut \\(\\gamma'\\gamma\\). Nous en déduisons que les \\(p-1\\) valeurs propres restantes sont toutes nulles. Nous pouvons donc dire que la matrice \\(\\gamma\\gamma'\\) se décompose comme \\[\\begin{eqnarray*}\n\\gamma\\gamma'&=&UDU',\n\\end{eqnarray*}\\] où \\(U\\) est la matrice orthogonale des vecteurs propres normés à l’unité de \\(\\gamma\\gamma'\\) et \\(D=\\mathop{\\mathrm{diag}}(\\gamma'\\gamma,0,\\dotsc,0)\\). Nous avons donc \\[\\begin{eqnarray*}\nI_p-\\gamma\\gamma'&=&UU' - UDU'=U(\\mathop{\\mathrm{diag}}(1-\\gamma'\\gamma,1,\\dotsc,1)U'.\n\\end{eqnarray*}\\] Les valeurs propres de \\(I_p-\\gamma\\gamma'\\) sont donc \\(1-\\gamma'\\gamma,1,\\dotsc,1\\), qui sont toutes positives ou nulles dès que \\(\\gamma'\\gamma\\le 1\\).\nUne condition pour que \\(\\sigma^2(2I_p-\\kappa\\beta\\beta')\\) soit semi-définie positive est que \\((\\kappa\\beta\\beta')\\le \\sigma^2\\) (cf. question précédente) et donc \\((\\sigma^2(2I_p+\\kappa^2(X'X)^{-1})-\\kappa\\beta\\beta')\\) est alors la somme de 2 matrices semi-définies positives donc semi-définie positive. Cela implique qu’il s’agit d’une condition suffisante pour que \\(\\Delta\\) soit semi-définie positive.\nNous venons de montrer 2 conditions, l’une nécessaire et suffisante, l’autre suffisante, afin que \\(\\Delta\\) soit semi-définie positive. Cette assertion signifie que, quelle que soit la combinaison linéaire du vecteur de paramètre (par exemple une coordonnée), l’estimateur ridge est meilleur que celui des MC au sens de l’EQM. Cela signifie aussi que, si une de ces conditions est vérifiée, globalement au sens de la trace de l’EQM, l’estimateur ridge est meilleur que celui des MC. Au niveau des conditions, cela permet de trouver la valeur optimale de \\(\\kappa\\). Malheureusement chacune des 2 conditions dépend de la valeur \\(\\beta\\) inconnue et donc n’est pas réellement utilisable en pratique. La condition suffisante procure une amélioration, dans le sens où elle ne dépend pas de \\(X\\) donc de l’expérience. Le prix à payer est bien sûr qu’il s’agit seulement d’une condition suffisante et donc plus restrictive.\n\n\n\nExercice 4 (Régression pondérée)  \n\nNous souhaitons minimiser \\[\\begin{eqnarray*}\n\\sum_{i=1}^n \\left(y_i-\\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 p_i,\n\\end{eqnarray*}\\] où \\(p_i\\) est un réel positif. Nous pouvons écrire ce critère sous la forme suivante : \\[\\begin{eqnarray*}\n\\sum_{i=1}^n \\left(\\sqrt{p_i}y_i-\\sum_{j=1}^p \\beta_j\\sqrt{p_i} x_{ij}\\right)^2\n= \\sum_{i=1}^n \\left(y^\\star_i-\\sum_{j=1}^p \\beta_jx_{ij}^\\star\\right)^2,\n\\end{eqnarray*}\\] où \\(y^\\star_i=\\sqrt{p_i}y_i\\) et \\(x_{ij}^\\star = \\sqrt{p_i} x_{ij}\\).\nNotons \\(P^{1/2}\\) la matrice des poids qui vaut \\(P^{1/2}=\\mathop{\\mathrm{diag}}(\\sqrt{p_i})\\). Ce dernier critère est un critère des MC avec comme observations \\(Y^\\star\\) et \\(X^\\star\\) où \\(Y^\\star = P^{1/2} Y\\) et \\(X^\\star = P^{1/2} X\\). L’estimateur vaut alors \\[\\begin{eqnarray*}\n\\hat \\beta_{pond} &=& (X^{\\star\\prime}X^\\star)^{-1}X^{\\star\\prime}Y^\\star\\\\\n&=& (X'PX)^{-1}X'PY.\n\\end{eqnarray*}\\]\nLorsque nous avons la constante comme seule variable explicative, \\(X=\\mathbf{1}_n\\), et nous avons alors \\[\\begin{eqnarray*}\n\\hat \\beta_{pond} &=&\\frac{\\sum p_i y_i}{\\sum p_i}.\n\\end{eqnarray*}\\]\nLorsque les poids sont constants, nous retrouvons, non plus une moyenne pondérée, mais la moyenne usuelle.\n\n\n\nExercice 5 (Gauss-Markov) L’estimateur des MC s’écrit \\(\\hat \\beta_2 = \\sum_{i=1}^n p_i y_i,\\) avec \\(p_i=(x_i-\\bar x)/\\sum(x_i -\\bar x)^2\\). Considérons un autre estimateur \\(\\tilde{\\beta_2}\\) linéaire en \\(y_i\\) et sans biais, c’est-à-dire \\[\\tilde{\\beta_2} =\\sum_{i=1}^n \\lambda_i y_i.\\] Montrons que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\). L’égalité \\(\\mathbf E(\\tilde{\\beta_2}) = \\beta_1 \\sum \\lambda_i + \\beta_2 \\sum \\lambda_i x_i + \\sum \\lambda_i \\mathbf E(\\varepsilon_i)\\) est vraie pour tout \\(\\beta_2\\) et \\(\\tilde \\beta_2\\) est sans biais donc \\(\\mathbf E(\\tilde \\beta_2)=\\beta_2\\) pour tout \\(\\beta_2\\), c’est-à-dire que \\(\\sum \\lambda_i=0\\) et \\(\\sum \\lambda_i x_i=1\\).\nMontrons que \\(\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2)\\). \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) = \\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2 + \\hat \\beta_2)\n=\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2)+\n2\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2).\n\\end{eqnarray*}\\] \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2}- \\hat \\beta_2,\\hat \\beta_2)\n\\!=\\!\\mathop{\\mathrm{Cov}}(\\tilde{\\beta_2},\\hat \\beta_2) -\\mathop{\\mathrm{V}}(-\\hat \\beta_2)\n\\!=\\!\\frac{\\sigma^2\\sum \\lambda_i(x_i-\\bar x)}{\\sum (x_i-\\bar x)^2} -\n\\frac{\\sigma^2}{\\sum (x_i-\\bar x)^2}\n\\!=\\!0,\n\\end{eqnarray*}\\] et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) =\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}- \\hat \\beta_2)+\\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Une variance est toujours positive et donc \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\tilde{\\beta_2}) \\geq \\mathop{\\mathrm{V}}(\\hat \\beta_2).\n\\end{eqnarray*}\\] Le résultat est démontré. On obtiendrait la même chose pour \\(\\hat \\beta_1\\).\n\n\nExercice 6 (Corrélation spatiale)  \n\nÉcrivons pour la ligne/site \\(i\\): \\[\\begin{align*}\nY_{i}&=X_{i.}'\\beta + \\varepsilon_{i}\n\\end{align*}\\] et nous savons que \\[\\begin{align*}\n\\varepsilon_{i}= \\rho \\sum_{i=1}^n M_{ij} \\varepsilon_{j} + \\eta_{i}\n\\end{align*}\\] Comme quand \\(i=j\\) on a \\(M_{ij}=0\\) on en déduit \\[\\begin{eqnarray*}\n\\varepsilon_i=\\rho\\sum_{j\\ne i,j=1}^{n}{M_{ij} \\varepsilon_j} + \\eta_i,\n\\end{eqnarray*}\\]\nLe site \\(i\\) est expliqué par un modèle de type auto-régression par les autres sites. Le site est très dépendant des sites proches (fort \\(M_{ij}\\)) et peu ou pas dépendant des autres sites (faible \\(M_{ij}\\) ou valeur nulle).\nOn repart de \\[\\begin{align*}\n\\varepsilon&=\\rho M \\varepsilon +\\eta\\\\\n(I - \\rho M)\\varepsilon &=\\eta\\\\\n\\varepsilon&=(I - \\rho M)^{-1}\\eta=A^{-1}\\eta\n\\end{align*}\\]\nComme \\(\\eta\\) est gaussien de moyenne nulle et de variance \\(\\sigma^{2}I\\) on a que \\(\\varepsilon\\) est gaussien de moyenne nulle et de variance \\[\\begin{align*}\n\\mathop{\\mathrm{V}}(\\varepsilon) &= \\mathop{\\mathrm{V}}(A^{-1}\\eta) = A^{-1} \\mathop{\\mathrm{V}}(\\eta) {A'}^{-1} =\\sigma^{2}  A^{-1} {A'}^{-1}\\\\\n&=\\sigma^{2}\\Omega\n\\end{align*}\\]\n\nPour la vraisemblance du modèle trouvons la loi de \\(Y=X\\beta + \\varepsilon\\). Comme \\(\\varepsilon\\) est gaussien de moyenne nulle et de variance \\(\\sigma^{2}\\Omega\\) on a que \\(Y\\) gaussien de moyenne \\(X\\beta\\) et de variance \\(\\sigma^{2}\\Omega\\). On a donc \\[\\begin{align*}\nL(Y,\\beta,\\sigma^2,\\rho)&=  \\frac{1}{(2\\pi)^{n/2}}\\frac{1}{|\\sigma^{2}\\Omega|^{1/2}}\n\\exp\\Bigl\\{-\\frac{1}{2\\sigma^2}(Y-X\\beta)'\\Omega^{-1}(Y-X\\beta)\\Bigr\\}\n\\end{align*}\\]\nLa log-vraisemblance est \\[\\begin{align*}\n\\mathcal{L}&=-\\frac{n}{2}\\log (2\\pi\\sigma^{2}) - \\frac{1}{2}\\log|\\Omega| - \\frac{1}{2\\sigma^2}(Y-X\\beta)'\\Omega^{-1}(Y-X\\beta)\n\\end{align*}\\] La dérivée par rapport à \\(\\beta\\) est \\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial\\beta}&=-\\frac{1}{2\\sigma^2}X'\\Omega^{-1}(Y-X\\beta)\\\\\n\\end{align*}\\] et en l’annulant on a \\[\\begin{align*}\nX'\\Omega^{-1}Y=X'\\Omega^{-1}X\\hat\\beta\n\\end{align*}\\] d’où \\[\n\\begin{eqnarray}\n\\hat \\beta&=&(X'\\hat \\Omega^{-1}X)^{-1}X'\\hat \\Omega^{-1}Y\\nonumber\\\\\n&=&(X'\\hat A'\\hat AX)^{-1}X'\\hat A'\\hat AY.\n\\end{eqnarray}\n\\tag{1}\\]\nLa dérivée par rapport à \\(\\sigma^2\\) est \\[\\begin{align*}\n\\frac{\\partial \\mathcal{L}}{\\partial\\sigma^2}&=-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}(Y-X\\beta)'\\Omega^{-1}(Y-X\\beta)\n\\end{align*}\\] et en l’annulant on a \\[\n\\begin{align}\n\\hat \\sigma^2&=\\frac{1}{n}(Y-X\\hat \\beta)'\\hat \\Omega^{-1} (Y-X\\hat \\beta)\\nonumber\\\\\n&=\\frac{1}{n}(Y-X\\hat \\beta)\\hat A'\\hat A (Y-X\\hat \\beta).\n\\end{align}\n\\tag{2}\\]\n\nComme nous savons les valeurs de \\(\\hat \\beta\\) comme fonction de \\(\\rho\\) (équation 1) et de \\(\\hat \\sigma^2\\) comme fonction de \\(\\rho\\) (équation 2) il suffit de les remplacer dans \\(-\\mathcal{L}\\) et on obtient \\[\\begin{eqnarray*}\n    h(\\hat\\rho)\\!&=\\!&\\!\\frac{n}{2}\\log Y'(I\\!-\\!X(X'\\hat A'\\hat AX)^{-1}X'\\hat A'\\hat A)'\n\\hat A'\\hat A(I\\!-\\!X(X'\\hat A'\\hat AX)^{-1}X'\\hat A'\\hat A)Y \\\\\n    && \\quad - \\frac{1}{2}\\log|\\hat A'\\hat A|^2\n    \\end{eqnarray*}\\]"
  },
  {
    "objectID": "correction/chap5.html",
    "href": "correction/chap5.html",
    "title": "5 Inférence dans le modèle gaussien",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, C, A, B, B.\n\n\nExercice 2 (Théorème 5.1) L’IC (i) découle de la propriété (i) de la proposition 5.3. La propriété (ii) donnant un IC pour \\(\\sigma^2\\) découle de la loi de \\(\\hat \\sigma^2\\). Enfin, la propriété (iii) est une conséquence de la loi obtenue propriété (ii) de la proposition 5.3.\n\n\nExercice 3 (Test et \\(R^2\\)) En utilisant l’orthogonalité des sous-espaces (figure 5.3 page 99) et le théorème de Pythagore, nous avons \\[\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2.\n\\end{eqnarray*}\\] Nous pouvons le démontrer de la manière suivante : \\[\\begin{eqnarray*}\n\\|\\hat Y_0-\\hat Y\\|^2 &=& \\|\\hat Y_0-Y+Y-\\hat Y\\|^2\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n+2\\langle \\hat Y_0-Y,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle Y-\\hat Y_0,Y-\\hat Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle\\\\\n&=& \\|\\hat \\varepsilon_0\\|^2+ \\| \\hat \\varepsilon\\|^2\n-2\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle.\n\\end{eqnarray*}\\] Or \\(\\Im(X_0) \\subset \\Im(X)\\), nous avons donc \\(P_{X^\\perp}P_{X_0^\\perp}=P_{X^\\perp}\\). De plus, \\(\\hat \\varepsilon=P_{X^\\perp}Y\\), cela donne \\[\\begin{eqnarray*}\n\\langle (P_{X^\\perp}+P_{X})P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle &=&\n\\langle P_{X^\\perp}Y,P_{X^\\perp}Y\\rangle\n+\\langle P_{X}P_{X_0^\\perp}Y,P_{X^\\perp}Y\\rangle \\\\\n&=& \\|\\hat \\varepsilon\\|^2 + 0.\n\\end{eqnarray*}\\] Le résultat est démontré, revenons à la statistique de test. Introduisons les différentes écritures du \\(\\mathop{\\mathrm{R^2}}\\) \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{R^2}}2 = \\frac{\\|\\hat Y - \\bar Y\\|^2}{\\|Y - \\bar Y\\|^2}=1 -\n\\frac{\\|\\hat \\varepsilon\\|^2}{\\|Y - \\bar Y\\|^2}.\n\\end{eqnarray*}\\] La statistique de test vaut \\[\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat \\varepsilon_0\\|^2- \\| \\hat \\varepsilon\\|^2}\n{\\| Y-\\hat Y\\|^2}\\frac{n-p}{p-p_0}\\\\\n&=&\\frac{\\|\\hat \\varepsilon_0\\|^2/\\|Y-\\bar Y\\|^2-\n\\| \\hat \\varepsilon\\|^2/\\|Y-\\bar Y\\|^2}\n{\\| Y-\\hat Y\\|^2/\\|Y-\\bar Y\\|^2}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\\] nous obtenons \\[\\begin{eqnarray*}\nF&=&\\frac{\\mathop{\\mathrm{R^2}}-\\mathop{\\mathrm{R^2_0}}}{1-\\mathop{\\mathrm{R^2}}}\\frac{n-p}{p-p_0},\n\\end{eqnarray*}\\] soit le résultat annoncé. Cette dernière quantité est toujours positive car \\(\\mathop{\\mathrm{R^2_0}}\\leq \\mathop{\\mathrm{R^2}}\\) et nous avons là un moyen de tester des modèles emboîtés via le coefficient de détermination.\n\n\nExercice 4 (Test et \\(R^2\\) et constante dans le modèle) à corriger ;).\n\n\nExercice 5 (Ozone)  \n\nLes résultats sont dans l’ordre \\[\\begin{eqnarray*}\n6.2, 0.8, 6.66, -1.5, -1, 50, 5, 124.\n\\end{eqnarray*}\\]\nLa statistique de test de nullité du paramètre se trouve dans la troisième colonne, nous conservons \\({\\mathrm{H_0}}\\) pour les paramètres associés à Ne9 et Ne12, et la rejetons pour les autres.\nLa statistique de test de nullité simultanée des paramètres autres que la constante vaut 50. Nous rejetons \\({\\mathrm{H_0}}\\).\nNous connaissons \\[\\begin{align*}\n\\hat y^{p}_{n+1} &= x'_{n+1}\\hat \\beta,\\\\\nx'_{n+1}&=(1, 10, 20, 0, 0, 1)\\\\\n\\hat \\beta&=(62, -4, 5, -1.5, -0.5, 0.8)'\n\\end{align*}\\] et donc la prévision est \\(\\hat y^{p}_{n+1} =122.8\\). Pour l’intervalle de confiance il nous faut \\(\\hat\\sigma=16\\) mais aussi la matrice \\(X'X\\) (donc toutes les données) ce que nous n’avons pas ici. On ne peut donc faire d’intervalle de confiance.\nNous sommes en présence de modèles emboîtés, nous pouvons appliquer la formule adaptée (voir l’exercice précédent) : \\[\\begin{eqnarray*}\nF&=& \\frac{\\mathop{\\mathrm{R^2}}2-\\mathop{\\mathrm{R^2_0}}2}{1-\\mathop{\\mathrm{R^2}}2}\\frac{n-p}{p-p_0}\\\\\n&=& \\frac{0.66-0.5}{1-0.66}\\frac{124}{2}= 29.\n\\end{eqnarray*}\\] Nous conservons \\({\\mathrm{H_0}}\\), c’est-à-dire le modèle le plus simple.\n\n\n\nExercice 6 (Équivalence du test T et du test F) Récrivons la statistique de test \\(F\\), en se rappelant que \\(X_0\\) est la matrice \\(X\\) privée de sa \\(j^e\\) colonne, celle correspondant au coefficient que l’on teste : \\[\\begin{eqnarray*}\nF&=&\\frac{\\|X\\hat \\beta-P_{X_0}X\\hat \\beta\\|^2}{\\hat \\sigma^2}\n  =\\frac{\\|X_j\\hat \\beta_j-\\hat \\beta_jP_{X_0}X_j\\|^2}{\\hat \\sigma^2}\n=\\frac{\\hat\\beta_j^2}{\\hat \\sigma^2}X_j'(I-P_{X_0})X_j.\n\\end{eqnarray*}\\] Récrivons maintenant le carré de la statistique \\(T\\) en explicitant \\(\\hat  \\sigma^2_{\\hat \\beta_j}\\) : \\[\\begin{eqnarray*}\nT^2&=&\\frac{\\hat \\beta_j^2}{\\hat \\sigma^2 [(X'X)^{-1}]_{jj}},\n\\end{eqnarray*}\\] où \\([(X'X)^{-1}]_{jj}\\) est le \\(j^e\\) élément diagonal de la matrice \\((X'X)^{-1}\\). Afin de calculer ce terme, nous utilisons la formule permettant d’obtenir l’inverse d’une matrice bloc, formule donnée en annexe A.2 page 416. Pour appliquer facilement cette formule, en changeant l’ordre des variables, la matrice \\(X\\) devient \\((X_0|X_j)\\) et \\(X'X\\) s’écrit alors \\[\\begin{eqnarray*}\nX'X&=&\\left(\n\\begin{array}{c|c}\nX'_0X_0&X'_0X_j\\\\\\hline\nX'_jX_0&X'_jX_j\n\\end{array}\\right).\n\\end{eqnarray*}\\] Son inverse, en utilisant la formule d’inverse de matrice bloc, est \\[\\begin{eqnarray*}\n[(X'X)^{-1}]_{jj}&=&\\left(X'_jX_j-X'_jX_0(X'_0X_0)^{-1}X'_0X_j\\right)^{-1}\n=\\left(X_j'(I-P_{X_0})X_j\\right)^{-1}.\n\\end{eqnarray*}\\] Nous avons donc \\(T^2=F\\). Au niveau des lois, l’égalité est aussi valable et nous avons que le carré d’un Student à \\((n-p)\\) ddl est une loi de Fisher à \\((1,n-p)\\) ddl. Bien entendu, le quantile \\((1-\\alpha)\\) d’une loi de Fisher correspond au quantile \\(1-\\alpha/2\\) d’une loi de Student. La loi \\(\\mathcal{T}\\) est symétrique autour de 0 et donc, lorsqu’elle est élevée au carré, les valeurs plus faibles que \\(t_{n-p}(\\alpha/2)\\), qui ont une probabilité sous \\({\\mathrm{H_0}}\\) de \\(\\alpha/2\\) d’apparaître, et celles plus fortes que \\(t_{n-p}(1-\\alpha/2)\\), qui ont une probabilité sous \\({\\mathrm{H_0}}\\) de \\(\\alpha/2\\) d’apparaître, deviennent toutes plus grandes que \\(t^2_{n-p}(1-\\alpha/2)\\). La probabilité que ces valeurs dépassent ce seuil sous \\({\\mathrm{H_0}}\\) est de \\(\\alpha\\) et correspond donc bien par définition à \\(f_{1,n-p}(1-\\alpha)\\).\n\n\nExercice 7 (Équivalence du test F et du test de VM) Nous avons noté la vraisemblance en début du chapitre par \\[\\begin{eqnarray*}\n\\mathcal{L}(Y,\\beta,\\sigma^2) &=& \\prod_{i=1}^n f_{Y}(y_i)\n= \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\sum_{i=1}^n \\left(y_i- \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2\\right]}\\\\\n&=& \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2}\\exp{\\left[-\\frac{1}{2 \\sigma^2}\n\\|Y-X\\beta\\|^2\\right]}.\n\\end{eqnarray*}\\] Cette vraisemblance est maximale lorsque \\(\\hat \\beta\\) est l’estimateur des MC et que \\(\\hat \\sigma^2 = \\|Y-X\\hat \\beta\\|^2/n\\). Nous avons alors \\[\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}(Y,\\beta,\\sigma^2)&=&\n\\left(\\frac{n}{2\\pi\\|Y-X\\hat \\beta\\|^2 }\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\\\\\n&=&\\left(\\frac{n}{2\\pi \\mathop{\\mathrm{SCR}}}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2),\n\\end{eqnarray*}\\] où \\(\\mathop{\\mathrm{SCR}}=\\|Y-X\\hat \\beta\\|^2\\).\nSous l’hypothèse \\({\\mathrm{H_0}}\\) nous obtenons de façon évidente le résultat suivant : \\[\\begin{eqnarray*}\n\\max_{\\beta,\\sigma^2} \\mathcal{L}_0(Y,\\beta_0,\\sigma^2)\n=\\left(\\frac{n}{2\\pi \\mathop{\\mathrm{SCR}}_0}\\right)^{n/2}\\exp{\\left(-\\frac{n}{2}\\right)}\n=\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2_0),\n\\end{eqnarray*}\\] où \\(\\mathop{\\mathrm{SCR}}_0\\) correspond à la somme des carrés résiduels sous \\({\\mathrm{H_0}}\\), c’est-à-dire \\(\\mathop{\\mathrm{SCR}}_0=\\|Y-X_0\\hat \\beta_0\\|^2\\). On définit le test du rapport de vraisemblance maximale (VM) par la région critique suivante : \\[\\begin{eqnarray*}\n\\mathcal{D}_\\alpha = \\left\\{\nY \\in \\mathbb R^n : \\lambda=\\frac{\\mathcal{L}_0(Y,\\hat \\beta_0,\\hat \\sigma^2)}\n{\\mathcal{L}(Y,\\hat \\beta,\\hat \\sigma^2)} &lt; \\lambda_0\n\\right\\}.\n\\end{eqnarray*}\\] La statistique du rapport de vraisemblance maximale vaut ici \\[\\begin{eqnarray*}\n\\lambda = \\left(\\frac{\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}_0}\\right)^{n/2} =\n\\left(\\frac{\\mathop{\\mathrm{SCR}}_0}{\\mathop{\\mathrm{SCR}}}\\right)^{-n/2}.\n\\end{eqnarray*}\\] Le test du rapport de VM rejette \\({\\mathrm{H_0}}\\) lorsque la statistique \\(\\lambda\\) est inférieure à une valeur \\(\\lambda_0\\) définie de façon à avoir le niveau du test égal à \\(\\alpha\\). Le problème qui reste à étudier est de connaître la distribution (au moins sous \\({\\mathrm{H_0}}\\)) de \\(\\lambda\\). Définissons, pour \\(\\lambda\\) positif, la fonction bijective \\(g\\) suivante : \\[\\begin{eqnarray*}\ng(\\lambda) = \\lambda^{-2/n}-1.\n\\end{eqnarray*}\\] La fonction \\(g\\) est décroissante (sa dérivée est toujours négative), donc \\(\\lambda&lt;\\lambda_0\\) si et seulement si \\(g(\\lambda)&gt;g(\\lambda_0)\\). Cette fonction \\(g\\) va nous permettre de nous ramener à des statistiques dont la loi est connue. Nous avons alors \\[\\begin{eqnarray*}\ng(\\lambda)&&gt;&g(\\lambda_0)\\\\\n\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&&gt;&g(\\lambda_0)\\\\\n\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&&gt;&f_0\n\\end{eqnarray*}\\] où \\(f_0\\) est déterminée par \\[\\begin{eqnarray*}\nP_{{\\mathrm{H_0}}}\\left(\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}&gt;f_0\n\\right)=\\alpha,\n\\end{eqnarray*}\\] avec la loi de cette statistique qui est une loi \\(\\mathcal{F}_{p-p_0,n-p}\\) (cf.~section précédente). Le test du rapport de VM est donc équivalent au test qui rejette \\({\\mathrm{H_0}}\\) lorsque la statistique \\[\\begin{eqnarray*}\nF=\\frac{n-p}{p-p_0}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}\n\\end{eqnarray*}\\] est supérieure à \\(f_0\\), où \\(f_0\\) est la valeur du fractile \\(\\alpha\\) de la loi de Fisher à \\((p-p_0,n-p)\\) degrés de liberté.\n\n\nExercice 8 (Test de Fisher pour une hypothèse linéaire quelconque) Nous pouvons toujours traduire l’hypothèse \\({\\mathrm{H_0}}\\) : \\(R\\beta=r\\) en terme de sous-espace de \\(\\mathcal M_X\\). Lorsque \\(r=0\\), nous avons un sous-espace vectoriel de \\(\\mathcal M_X\\) et lorsque \\(r\\neq 0\\) nous avons un sous-espace affine de \\(\\mathcal M_X\\). Dans les deux cas, nous noterons ce sous-espace \\(\\mathcal M_0\\) et \\(\\mathcal M_0 \\subset \\mathcal M_X\\). Cependant nous ne pourrons plus le visualiser facilement comme nous l’avons fait précédemment avec \\(\\mathcal M_{X_0}\\) où nous avions enlevé des colonnes à la matrice \\(X\\). Nous allons décomposer l’espace \\(\\mathcal M_X\\) en deux sous-espaces orthogonaux \\[\\begin{eqnarray*}\n\\mathcal M_X = \\mathcal M_0 \\stackrel{\\perp}{\\oplus} ( \\mathcal M_0^\\perp \\cap \\mathcal M_X ).\n\\end{eqnarray*}\\] Sous \\({\\mathrm{H_0}}\\), l’estimation des moindres carrés donne \\(\\hat Y_0\\) projection orthogonale de \\(Y\\) sur \\(\\mathcal M_0\\) et nous appliquons la même démarche pour construire la statistique de test. La démonstration est donc la même que celle du théorème 5.2. C’est-à-dire que nous regardons si \\(\\hat Y_0\\) est proche de \\(\\hat Y\\) et nous avons donc \\[\\begin{eqnarray*}\nF&=&\\frac{\\|\\hat Y -\\hat Y_0\\|^2/\\dim(\\mathcal M_0^{\\perp}  \\cap \\mathcal M_X)}{\\|Y - \\hat Y\\|^2/\n\\dim(\\mathcal M_{X^{\\perp}})}\\\\\n&=&\\frac{n-p}{q} \\frac{\\|Y-\\hat Y_0\\|^2-\\|Y-\\hat Y\\|^2}{ \\|Y-\\hat Y\\|^2}\\\\\n&=& \\frac{n-p}{q}\\frac{\\mathop{\\mathrm{SCR}}_0-\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCR}}}\\sim \\mathcal{F}_{q,n-p}.\n\\end{eqnarray*}\\] Le problème du test réside dans le calcul de \\(\\hat Y_0\\). Dans la partie précédente, il était facile de calculer \\(\\hat Y_0\\) car nous avions la forme explicite du projecteur sur \\(\\mathcal M_0\\). Une première façon de procéder revient à trouver la forme du projecteur sur \\(\\mathcal M_0\\). Une autre façon de faire est de récrire le problème de minimisation sous la contrainte \\(R\\beta=r\\). Ces deux manières d’opérer sont présentées en détail dans la correction de l’exercice 2.13. Dans tous les cas l’estimateur des MC contraints par \\(R\\beta=r\\) est défini par \\[\\begin{eqnarray*}\n\\hat \\beta_0&=&\\hat \\beta + (X'X)^{-1}R'[R(X'X)^{-1}R']^{-1}(r-R\\hat \\beta).\n\\end{eqnarray*}\\]\n\n\nExercice 9 (Généralisation de la régression ridge) Soit la fonction à minimiser \\[\\begin{align*}\n    R(\\beta)&=\\|Y-X\\beta\\|^2 -\\sum_{j=1}^{p}\\delta_j(\\beta_j^2) \\\\\n    &= (Y-X\\beta)'(Y-X\\beta) - \\beta' \\Delta \\beta\n  \\end{align*}\\] avec \\(\\delta_{1}, \\dotsc, \\delta_{p}\\) des réels positifs ou nuls.\nSachant que \\(\\frac{\\partial \\beta' A\\beta}{\\partial \\beta}=2A\\beta\\) (avec \\(A\\) symétrique) et que \\(\\frac{\\partial X\\beta}{\\partial \\beta}=X'\\) nous avons la dérivée partielle suivante \\[\\begin{align*}\n    \\frac{\\partial R}{\\partial \\beta}&=-2X'(Y-X\\beta)  + 2\\Delta \\beta\n  \\end{align*}\\] En annulant cette dérivée nous avons \\[\\begin{align*}\n    -2X'(Y-X\\hat\\beta_{\\mathrm{RG}}) + 2\\Delta \\hat\\beta_{\\mathrm{RG}}&=0\\\\\n         (X'X + \\Delta) \\hat\\beta_{\\mathrm{RG}} &=  X'Y\n  \\end{align*}\\] donc en prémultipliant par \\((X'X-\\Delta)^{-1}\\) nous obtenons \\[\\begin{align*}\n    \\hat\\beta_{\\mathrm{RG}}=(X'X-\\Delta)^{-1}X'Y.\n  \\end{align*}\\] En régression multiple le nombre de paramètres est \\(p=\\mathop{\\mathrm{tr}}(P_{X})\\) avec \\(P_{X}\\) la matrice de l’endomorphisme qui permet d’obtenir \\(\\hat Y\\) à partir de \\(Y\\). Dans cette régression ridge, nous avons que \\[\\begin{align*}\n    \\hat Y_{\\mathrm{RG}}&=X\\hat\\beta_{\\mathrm{RG}}=X(X'X-\\Delta)^{-1}X'Y\n  \\end{align*}\\] donc la matrice de l’endomorphisme est ici \\(X(X'X-\\Delta)^{-1}X'\\) et le nombre équivalent de paramètres est \\(\\mathop{\\mathrm{tr}}(X(X'X-\\Delta)^{-1}X')\\).\n\n\nExercice 10 (IC pour la régression ridge)  \n\nLoi de \\(\\hat \\beta\\) : \\({\\mathcal{N}}(\\beta, \\sigma^{2}(X'X)^{-1})\\) grâce au modèle et à \\({\\mathcal{H}}_3\\).\nLoi de $_{}() $. Comme \\(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)= (X'X-\\tilde\\kappa I)^{-1}X'Y\\) avec \\(A=(X'X-\\tilde\\kappa I)^{-1}X'\\) qui est une matrice fixe. Avec \\({\\mathcal{H}}_{3}\\) et le modèle de régression multiple on a que \\(Y\\sim{\\mathcal{N}}(X\\beta, \\sigma^{2}I)\\).\nPuisque \\(Y\\) est un vecteur gaussien, il en est de même de \\(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa)=AY\\). Calculons son espérance \\[\\begin{align*}\n\\mathbf E(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\mathbf E(AY)=A\\mathbf E(Y)=AX\\beta\\\\\n&=(X'X-\\tilde\\kappa I)^{-1}X'X\\beta\n\\end{align*}\\] et sa variance \\[\\begin{align*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_{\\mathrm{ridge}}(\\tilde\\kappa))&=\\mathop{\\mathrm{V}}(AY)=A\\mathop{\\mathrm{V}}(Y)A'=A\\sigma^{2}I A' = \\sigma^{2} A A'\\\\\n&=\\sigma^{2}(X'X-\\tilde\\kappa I)^{-1}X'X(X'X-\\tilde\\kappa I)^{-1}.\n\\end{align*}\\]\nCalculons le produit scalaire de \\(Y-\\hat Y_{\\mathrm{ridge}}\\) et \\(\\hat Y_{MC}:\\) \\[\\begin{align*}\n&lt;Y-\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}&gt;&=&lt;Y-\\hat Y_{MC} + \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}};\\hat Y_{MC}&gt;  \\\\\n& =  &lt;Y-\\hat Y_{MC}; \\hat Y_{MC}&gt; + &lt;   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}&gt;\\\\\n&= 0 + &lt;   \\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}} ;  \\hat Y_{MC}&gt;\n\\end{align*}\\] Or \\(\\hat Y_{\\mathrm{ridge}} = X\\beta_{\\mathrm{ridge}}(\\tilde\\kappa)\\) donc il appartient au sous espace vectoriel \\(\\Im(X)\\), de même que \\(\\hat Y_{MC}=P_{X}Y\\). Sauf si \\(\\tilde\\kappa=0\\) on a que \\(\\hat Y_{\\mathrm{ridge}}\\neq \\hat Y_{MC}\\) donc \\(\\hat Y_{MC} -\\hat Y_{\\mathrm{ridge}}\\) est un vecteur non nul de \\(\\Im(X)\\) et donc son produit scalaire avec \\(\\hat Y_{MC}\\in \\Im(X)\\) est non nul.\nIl faut pouvoir démontrer l’indépendance de \\(\\hat\\sigma_{\\mathrm{ridge}}\\) et \\(\\hat \\beta_{\\mathrm{ridge}}\\). Pour le théorème 5.1, on montre l’indépendance entre\\(\\hat \\beta\\) et \\(\\hat \\sigma\\) en considérant les 2 vecteurs \\(\\hat\\beta\\) et \\(\\hat \\varepsilon=(Y-\\hat Y)\\). Comme nous pouvons écrire \\(\\hat \\beta=(X'X)^{-1}X'P_XY\\), \\(\\hat \\beta\\) est donc une fonction fixe (dépendante uniquement des \\(X\\)) de \\(P_XY\\). De plus, \\(\\hat \\varepsilon=P_{X^\\perp}Y\\) est orthogonal à \\(P_XY\\). Ces 2 vecteurs suivent des lois normales et sont donc indépendants. Il en résulte que \\(\\hat \\beta\\) et \\(Y-\\hat Y\\) sont indépendants et de même pour \\(\\hat \\beta\\) et \\(\\hat \\sigma\\).\nIci, \\(\\hat\\sigma_{\\mathrm{ridge}}\\) est une fonction de \\(Y-\\hat Y_{\\mathrm{ridge}}\\). Le vecteur \\(\\hat\\beta_{\\mathrm{ridge}}=(X'X+\\tilde\\kappa I_p)^{-1}X'Y=(X'X+\\tilde\\kappa I_p)^{-1}X'P_XY\\) est une fonction fixe (\\(\\tilde \\kappa\\) est considéré comme fixé) de \\(P_XY\\). Par contre, \\(P_XY\\) n’est pas orthogonal à \\((Y-\\hat Y_{\\mathrm{ridge}})\\), comme nous l’avons montré, nous ne pouvons donc montrer l’indépendance de \\(\\hat\\beta_{\\mathrm{ridge}}\\) et \\(\\hat\\sigma_{\\mathrm{ridge}}\\).\nUne autre idée serait d’utiliser \\(\\hat\\sigma\\) mais en général si l’on utilise la régression ridge c’est que l’on se doute que \\(\\hat Y\\) n’est pas un bon estimateur de \\(X\\beta\\) et donc \\(\\hat\\sigma\\) qui est une fonction de \\(Y-\\hat Y\\) risque de ne pas être un bon estimateur de \\(\\sigma\\). L’estimateur \\(\\hat\\sigma\\) peut même être nul, ce qui pratiquement peut arriver quand \\(p&gt;n\\).\nEn général quand \\(X\\) est fixe pour un bootstrap en régression on estime \\(\\hat \\beta\\) puis on déduit les \\(\\{\\hat \\epsilon_{i}\\}\\). De cet ensemble sont tirés de manière équiprobable avec remise \\(n\\) résidus \\(\\{\\hat \\epsilon_{i}^{*}\\}\\). Ces nouveaux résidus sont additionnés à \\(X\\beta\\) pour faire un nouveau vecteur \\(Y^{*}\\) et avoir un échantillon bootstap \\(Y^{*}, X\\).\nIci l’estimation de \\(\\hat \\beta\\) sera mauvaise (et c’est pour cela que l’on utilise la régression ridge) et plutôt que d’estimer de mauvais résidus nous allons retirer avec remise parmi les \\(Y_{i}, X_{i.}\\) ce qui est la procédure adaptée au \\(X\\) aléatoire mais ici nous avons peu de choix\nEntrées : \\(\\tilde \\kappa\\) fixé, \\(\\alpha\\) fixé, \\(B\\) choisi.  Sorties : IC, au niveau \\(\\alpha\\), coordonnée par coordonnée de \\(\\beta\\).\n\nEstimer \\(\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)\\) .\nEn déduire \\(\\hat \\varepsilon_{\\mathrm{ridge}}=Y-X\\hat \\beta_{\\mathrm{ridge}}\\).\nPour \\(k=1\\) à \\(B\\)\n\ntirer avec remise \\(n\\) résidus estimés parmi les \\(n\\) coordonnées de \\(\\hat \\varepsilon_{\\mathrm{ridge}}\\) ;\non note ces résidus (réunis dans 1 vecteur) \\(\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}\\) ;\nconstruire 1 échantillon \\(Y^{(k)}=X\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)+\\hat \\varepsilon_{\\mathrm{ridge}}^{(k)}\\) ;\n\\(\\tilde \\kappa^{(k)} \\leftarrow \\tilde \\kappa\\) ;\nestimer le vecteur de paramètre \\(\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})=(X'X+\\tilde\\kappa^{(k)} I_p)^{-1}X'Y^{(k)}\\) ;\n\nPour \\(j=1\\) à \\(p\\)\n\ncalculer les quantiles empiriques de niveau \\(\\alpha/2\\) et \\(1-\\alpha/2\\) pour la coordonnée \\(j\\), sur tous les vecteurs \\(\\{\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa)\\}\\) ;\n\n\nL’algorithme est presque le même. Cependant comme \\(\\tilde \\kappa\\) n’est pas fixé, pour estimer \\(\\beta_{\\mathrm{ridge}}(\\tilde \\kappa)\\) il faut déterminer \\(\\tilde \\kappa\\) par une méthode choisie. Ensuite, à chaque estimation de \\(\\beta_{\\mathrm{ridge}}^{(k)}(\\tilde \\kappa^{(k)})\\), il est nécessaire au préalable de déterminer \\(\\tilde \\kappa^{(k)}\\) par la même méthode que celle utilisée pour déterminer \\(\\tilde \\kappa\\)."
  },
  {
    "objectID": "correction/chap6.html",
    "href": "correction/chap6.html",
    "title": "6 Variables qualitatives : ANCOVA et ANOVA",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, A, C, B.\n\n\nExercice 2 (Analyse de la covariance)  \n\nNous avons pour le modèle complet la matrice suivante : \\[\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}&\\cdots&0\\\\\n\\vdots&\\cdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n\\] et pour les deux sous-modèles, nous avons les matrices suivantes : \\[\nX=\\begin{bmatrix}\n1&\\cdots&0     &x_{11}\\\\\n\\vdots &\\cdots&\\vdots&\\vdots\\\\\n1&\\cdots&      0&x_{1n_1}\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n0&\\cdots&1&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0&\\cdots&1&x_{In_I}\n\\end{bmatrix}\n\\quad\nX=\\begin{bmatrix}\n1&x_{11}&\\cdots&0\\\\\n\\vdots &\\vdots&\\vdots&\\vdots\\\\\n1&x_{1n_1}&\\cdots&0\\\\\n\\cdots&\\cdots&\\cdots&\\cdots\\\\\n1&0&\\cdots&x_{I1}\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&0&\\cdots&x_{In_I}\n\\end{bmatrix}\n\\]\nDans le modèle complet, nous obtenons par le calcul \\[\nX'X = \\begin{bmatrix}\nn_1&0&\\cdots&\\sum x_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&n_I&0&\\cdots&\\sum x_{iI}\\\\\n\\sum x_{i1}&0&\\cdots&\\sum x^2_{i1}&0&\\cdots\\\\\n&\\ddots&&&\\ddots&\\\\\n0&\\cdots&\\sum x_{iI}&0&\\cdots&\\sum x^2_{iI}\\\\\n\\end{bmatrix}\n  \\quad\nX'Y = \\begin{bmatrix}\n\\sum y_{i1}\\\\\n\\vdots\\\\\n\\sum y_{iI}\\\\\n\\sum x_{i1}y_{i1}\\\\\n\\vdots\\\\\n\\sum x_{iI}y_{iI}\\\\\n\\end{bmatrix}\n\\] Une inversion par bloc de \\(X'X\\) et un calcul matriciel donnent le résultat indiqué.\nUne autre façon de voir le problème est de partir du problème de minimisation \\[\\begin{eqnarray*}\n&&\\min \\sum_{i=1}^I\\sum_{j=1}^{n_i}\\left(y_{ij}-\\alpha_{j}-\\beta_{j}x_{ij}\\right)^2\\\\\n&=& \\min \\sum_{j=1}^{n_i}\\left(y_{j1}-\\alpha_1-\\beta_{1}x_{j1}\\right)^2+\\cdots\n+\\sum_{j=1}^{n_I}\\left(y_{jI}-\\alpha_I-\\beta_{I}x_{JI}\\right)^2.\n\\end{eqnarray*}\\] Cela revient donc à calculer les estimateurs des MC pour chaque modalité de la variable qualitative. Attention tout de même, des régressions pour chaque modalité donnent bien les mêmes coefficients \\(\\alpha_{i}, \\beta_{i}\\) mais les écarts-types estimés seront différents: un par modalité dans le cas des régressions pour chaque modalité, un seul écart-type estimé dans le cas de l’ANCOVA.\n\n\n\nExercice 3 (Estimateurs des MC et ANOVA à 1 facteur) La preuve de cette proposition est relativement longue et peu difficile. Nous avons toujours \\(Y\\) un vecteur de \\(\\mathbb R^n\\) à expliquer. Nous projetons \\(Y\\) sur le sous-espace engendré par les colonnes de \\(A_c\\), noté \\(\\mathcal M_{A_c}\\), de dimension I, et obtenons un unique \\(\\hat Y\\). Cependant, en fonction des contraintes utilisées, le repère de \\(\\mathcal M_{A_c}\\) va changer.\nLe cas le plus facile se retrouve lorsque \\(\\mu=0\\). Nous avons alors \\[\\begin{eqnarray*}\n(A_c'A_c) =\n\\begin{bmatrix}\nn_1&0&\\cdots&0 \\\\\n0&n_2&0&\\cdots \\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n0& \\cdots & 0 & n_I\n\\end{bmatrix}\n\\quad\n(A_c'Y)=\\begin{bmatrix}\n\\sum_{j=1}^{n_1} y_{1j}\\\\\n\\sum_{j=1}^{n_2} y_{2j}\\\\\n\\vdots\\\\\n\\sum_{j=1}^{n_I} y_{Ij}\n\\end{bmatrix}\n\\end{eqnarray*}\\] d’où le résultat. La variance de \\(\\hat \\alpha\\) vaut \\(\\sigma^2 (A_c'A_c)^{-1}\\) et cette matrice est bien diagonale.\nPour les autres contraintes, nous utilisons le vecteur \\(\\vec{e}_{ij}\\) de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle repérée par le couple \\((i,j)\\) qui vaut 1 pour repérer un individu. Nous notons \\(\\vec{e}_{i}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celles repérées par les indices \\(i,j\\) pour \\(j=1,\\cdots,n_{i}\\) qui valent 1. En fait, ce vecteur repère donc les individus qui admettent la modalité \\(i\\). La somme des \\(\\vec{e}_{i}\\) vaut le vecteur \\(\\mathbf{1}\\). Les vecteurs colonnes de la matrice \\(A_c\\) valent donc \\(\\vec{e}_{1},\\cdots,\\vec{e}_{I}\\).\nConsidérons le modèle \\[\\begin{eqnarray*}\nY=\\mu \\mathbf{1} + \\alpha_1 \\vec{e_1}+ \\alpha_2 \\vec{e_2} +\n\\cdots + \\alpha_I\\vec{e_I} + \\varepsilon.\n\\end{eqnarray*}\\] Voyons comment nous pouvons récrire ce modèle lorsque les contraintes sont satisfaites.\n\n\\(\\alpha_1=0\\), le modèle devient alors \\[\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1} + 0 \\vec{e_1} + \\alpha_2 \\vec{e_2} + \\cdots +\n\\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_2 \\vec{e_2} + \\cdots + \\alpha_I\\vec{e_I} + \\varepsilon\\\\\n&=& [\\mathbf{1}, \\vec{e_2}, \\cdots, \\vec{e_I}] \\beta + \\varepsilon\\\\\n&=& X_{[\\alpha_1=0]} \\beta_{[\\alpha_1=0]} + \\varepsilon.\n\\end{eqnarray*}\\]\n\\(\\sum n_i \\alpha_i = 0\\) cela veut dire que \\(\\alpha_I= - \\sum_{j=1}^{I-1} n_j\\alpha_j/n_I\\), le modèle devient \\[\n\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1}+ \\alpha_1 \\vec{e_1} + \\cdots +\\alpha_{I-1} \\vec{e_{I-1}}   \n- \\sum_{j=1}^{I-1}  \\frac{n_j\\alpha_j}{n_I}\\vec{e_I} + \\varepsilon\\\\\n&=& \\mu \\mathbf{1} + \\alpha_1(\\vec{e}_{1}-\\frac{n_1}{n_I}\\vec{e}_{I}) + \\cdots\n+ \\alpha_{I-1} (\\vec{e}_{I-1}-\\frac{n_{I-1}}{n_I}\\vec{e}_{I})+\\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_1\\vec{v}_{1}+ \\cdots + \\alpha_{I-1} \\vec{v}_{I-1}\n+ \\varepsilon \\quad \\hbox{où} \\quad \\vec{v}_{i}= (\\vec{e}_{i}-\\frac{n_i}{n_I}\\vec{e}_{I})\\\\\n&=& X_{[\\sum  n_i \\alpha_i=0]} \\beta_{[\\sum  n_i\\alpha_i=0]} + \\varepsilon.\n\\end{eqnarray*}\n\\]\n\\(\\sum \\alpha_i = 0\\) cela veut dire que \\(\\alpha_I= - \\sum_{j=1}^{I-1} \\alpha_j\\), le modèle devient \\[\n\\begin{eqnarray*}\nY &=&\\mu \\mathbf{1} + \\alpha_1 \\vec{e_1} + \\cdots + \\alpha_{I-1} \\vec{e_{I-1}} -\n\\sum_{j=1}^{I-1}  \\alpha_j \\vec{e_I} + \\varepsilon\\\\\n&=& \\mu \\mathbf{1} + \\alpha_1(\\vec{e}_{1}-\\vec{e}_{I}) + \\cdots\n+ \\alpha_{I-1} (\\vec{e}_{I-1}-\\vec{e}_{I})+\\varepsilon\\\\\n&=&\\mu \\mathbf{1} + \\alpha_1\\vec{u}_{1}+ \\cdots + \\alpha_{I-1} \\vec{u}_{I-1}\n+ \\varepsilon \\quad \\hbox{où} \\quad \\vec{u}_{i}= (\\vec{e}_{i}-\\vec{e}_{I})\\\\\n&=& X_{[\\sum  \\alpha_i=0]} \\beta_{[\\sum  \\alpha_i=0]} + \\varepsilon.\n\\end{eqnarray*}\n\\]\n\nDans tous les cas, la matrice \\(X\\) est de taille \\(n \\times I\\), et de rang \\(I\\). La matrice \\(X'X\\) est donc inversible. Nous pouvons calculer l’estimateur \\(\\hat \\beta\\) des MC de \\(\\beta\\) par la formule \\(\\hat \\beta = (X'X)^{-1}X'Y\\) et obtenir les valeurs des estimateurs. Cependant ce calcul n’est pas toujours simple et il est plus facile de démontrer les résultats via les projections.\nLes différentes matrices \\(X\\) et la matrice \\(A\\) engendrent le même sous-espace, donc la projection de \\(Y\\), notée \\(\\hat Y\\) dans ce sous-espace, est toujours la même. La proposition 6.2 indique que \\[\\begin{eqnarray*}\n\\hat Y = \\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I}.\n\\end{eqnarray*}\\] Avec les différentes contraintes, nous avons les 3 cas suivants :\n\n\\(\\alpha_1=0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_2 \\vec{e_2} + \\cdots +\n\\hat \\alpha_I\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum n_i \\alpha_i = 0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1}\n\\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum \\alpha_i = 0\\), la projection s’écrit \\[\n\\begin{eqnarray*}\n\\hat Y &=& \\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots +\n\\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\nIl suffit maintenant d’écrire que la projection est identique dans chaque cas et de remarquer que le vecteur \\(\\mathbf{1}\\) est la somme des vecteurs \\(\\vec{e_i}\\) pour \\(i\\) variant de 1à\\(I\\). Cela donne\n\n\\(\\alpha_1=0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=&\\hat \\mu\\mathbf{1}+\\hat\\alpha_2\\vec{e_2}+\\cdots+\\hat \\alpha_I\\vec{e_I}\\\\\n&=& \\hat \\mu\\vec{e_1}+(\\hat \\mu+\\hat \\alpha_2)\\vec{e_2}\n\\cdots (\\hat \\mu+\\hat \\alpha_I)\\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum n_i \\alpha_i = 0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=& \\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots + \\hat \\alpha_{I-1}\n\\vec{e_{I-1}}   - \\sum_{j=1}^{I-1}  \\frac{n_j \\hat \\alpha_j}{n_I}\\vec{e_I}\\\\\n&=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n(\\hat \\mu  + \\hat \\alpha_{I-1}) \\vec{e_{I-1}} +\n(\\hat \\mu  - \\sum_{i=1}^{I-1} \\frac{n_i}{n_I}\\hat \\alpha_i) \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\\(\\sum \\alpha_i = 0\\) \\[\n\\begin{eqnarray*}\n&&\\bar{y_1} \\vec{e_1} + \\cdots + \\bar{y_I} \\vec{e_I} \\\\\n&=&\\hat \\mu \\mathbf{1} + \\hat \\alpha_1 \\vec{e_1} + \\cdots +\n\\hat \\alpha_{I-1} \\vec{e_{I-1}} - \\sum_{j=1}^{I-1} \\hat  \\alpha_j \\vec{e_I}\\\\\n&=& (\\hat \\mu + \\hat \\alpha_1) \\vec{e_1} + \\cdots +\n(\\hat \\mu  + \\hat \\alpha_{I-1})\\vec{e_{I-1}} +\n(\\hat \\mu - \\sum_{i=1}^{I-1} \\hat \\alpha_i) \\vec{e_I}.\n\\end{eqnarray*}\n\\]\n\nEn identifiant les différents termes, nous obtenons le résultat annoncé.\n\n\nExercice 4 (Estimateurs des MC et ANOVA à deux facteurs) Nous notons \\(\\vec{e}_{ijk}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle indicée par \\(ijk\\) qui vaut 1. Sous les contraintes de type analyse par cellule, le modèle devient \\[\\begin{eqnarray*}\ny_{ijk} &=& \\gamma_{ij} + \\varepsilon_{ijk},\n\\end{eqnarray*}\\] et donc matriciellement \\[\\begin{eqnarray*}\nY= X \\beta +\\varepsilon \\quad \\quad X=(\\vec{e_{11}},\\vec{e_{12}},\\ldots,\\vec{e_{IJ}}),\n\\end{eqnarray*}\\] où le vecteur \\(\\vec{e}_{ij}= \\sum_{k} \\vec{e}_{ijk}\\). Les vecteurs colonnes de la matrice \\(X\\) sont orthogonaux entre eux. Le calcul matriciel \\((X'X)^{-1}X'Y\\) donne alors le résultat annoncé.\n\n\nExercice 5 (Estimateurs des MC et ANOVA à deux facteurs, suite) Nous notons \\(\\vec{e}_{ijk}\\) le vecteur de \\(\\mathbb R^n\\) dont toutes les coordonnées sont nulles sauf celle indicée par \\(ijk\\) qui vaut 1. Nous définissons ensuite les vecteurs suivants~: \\[\\begin{eqnarray*}\n\\vec{e}_{ij} = \\sum_{k} \\vec{e}_{ijk} \\quad\n\\vec{e}_{i.} = \\sum_{j} \\vec{e}_{ij}  \\quad\n\\vec{e}_{.j} = \\sum_{i} \\vec{e}_{ij}  \\quad\n\\vec{e} = \\sum_{i,j,k} \\vec{e}_{ijk}.\n\\end{eqnarray*}\\] Afin d’effectuer cet exercice, nous définissons les sous-espaces suivants~: \\[\\begin{eqnarray*}\nE_1&\\!\\!:=\\!\\!&\\{m \\vec{e},\\ m \\hbox{ quelconque} \\}\\\\\nE_2&\\!\\!:=\\!\\!&\\{\\sum_i a_i \\vec{e}_{i.},\\ \\sum_i a_i=0\\}\\\\\nE_3&\\!\\!:=\\!\\!&\\{\\sum_j b_j \\vec{e}_{.j},\\ \\sum_j b_j=0\\}\\\\\nE_4&\\!\\!:=\\!\\!&\\{\\sum_{ij} c_{ij} \\vec{e}_{ij},\n\\ \\forall j \\sum_{i} c_{ij}=0 \\hbox{ et } \\forall i \\sum_{j} c_{ij}=0\\}.\n\\end{eqnarray*}\\] Ces espaces \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\) sont de dimension respective 1, \\(I-1\\), \\(J-1\\) et \\((I-1)(J-1)\\). Lorsque le plan est équilibré, tous ces sous-espaces sont orthogonaux. Nous avons la décomposition suivante~: \\[\\begin{eqnarray*}\nE = E_1 \\stackrel{\\perp}{\\oplus} E_2 \\stackrel{\\perp}{\\oplus} E_3\n\\stackrel{\\perp}{\\oplus} E_4.\n\\end{eqnarray*}\\]\nLa projection sur \\(E\\) peut se décomposer en une partie sur \\(E_1,\\cdots,E_4\\) et l’estimateur des MC est obtenu par projection de \\(Y\\) sur \\(E\\). Notons \\(P_{E^\\perp}\\), \\(P_{E},\\) \\(P_{E_1},\\) \\(P_{E_2},\\) \\(P_{E_3}\\) et \\(P_{E_4}\\) les projections orthogonales sur les sous-espaces \\(E^\\perp\\), \\(E\\), \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\), nous avons alors \\[\\begin{eqnarray*}\nP_{E_1} Y &=& \\bar{y} \\mathbf{1} ,\n\\end{eqnarray*}\\] puis, en remarquant que projeter sur le sous-espace engendré par les colonnes de \\(A=[\\vec{e}_{1.},\\cdots,\\vec{e}_{I.}]\\) est identique à la projection sur \\(E_1 \\stackrel{\\perp}{\\oplus} E_2\\), nous avons alors avec \\(\\mathbf{1} = \\sum_i \\vec{e}_{i.}\\), \\[\\begin{eqnarray*}\nP_{A} Y = \\sum_i \\bar{y}_{i.} \\vec{e}_{i.} \\quad \\hbox{donc}\n\\quad P_{E_2} Y =\\sum_i (\\bar{y}_{i.} - \\bar{y})\\ \\vec{e}_{i.}.\\\\\n\\end{eqnarray*}\\] De la même façon, nous obtenons \\[\\begin{eqnarray*}\nP_{E_3}(Y)&=&\\sum_j (\\bar{y}_{.j} - \\bar{y})\\ \\vec{e}_{.j},\\\\\nP_{E_4}(Y)&=&\\sum_{ij} (\\bar{y}_{ij}-\\bar{y}_{i.}-\\bar{y}_{.j}+\\bar{y})\\  \\vec{e}_{i.},\\\\\nP_{E^\\perp}(Y)  &=&\\sum_{ijk} (y_{ijk}-\\bar{y}_{ij})\\ \\vec{e}_{ijk},\n\\end{eqnarray*}\\] où \\(\\vec{e}_{ijk}\\) est le vecteur dont toutes les coordonnées sont nulles sauf celle indicée par \\({ijk}\\) qui vaut 1. En identifiant terme à terme, nous retrouvons le résultat énoncé.\n\n\nExercice 6 (Tableau d’ANOVA à 2 facteurs équilibrés) Lorsque le plan est équilibré, nous avons démontré, que les sous-espaces \\(E_1\\), \\(E_2\\), \\(E_3\\) et \\(E_4\\) sont orthogonaux (cf. exercice précédent) deux à deux. Nous avons alors \\[\\begin{eqnarray*}\nY &=& P_{E_1}(Y) + P_{E_2}(Y) + P_{E_3}(Y) + P_{E_4}(Y) + P_{E^\\perp}(Y).\n\\end{eqnarray*}\\] Nous obtenons ensuite par le théorème de Pythagore \\[\\begin{eqnarray*}\n\\begin{array}{ccccccccccc}\n\\|Y - \\bar Y \\|^2 &=&  \\| P_{E_2}(Y)\\|^2 &+&\n\\|P_{E_3}(Y)\\|^2 &+& \\|P_{E_4}(Y)\\|^2 &+& \\|P_{E^\\perp}(Y)\\|^2\\\\\n\\mathop{\\mathrm{SCT}}&=& \\text{SC}_A &+& \\text{SC}_B &+& \\text{SC}_{AB} &+& \\mathop{\\mathrm{SCR}},\n\\end{array}\n\\end{eqnarray*}\\] où \\[\\begin{eqnarray*}\n\\mathop{\\mathrm{SCT}}&=& \\sum_i \\sum_j \\sum k (y_{ijk} - \\bar y)^2\\\\\n\\text{SC}_A &=& Jr \\sum_i (y_{i..}-\\bar y)^2\\\\\n\\text{SC}_B &=& Ir \\sum_j (y_{.j.} - \\bar y)^2\\\\\n\\text{SC}_{AB} &=& r \\sum_i \\sum_j (y_{ij.} - y_{i..} - y_{.j.} +\\bar y)^2\\\\\n\\mathop{\\mathrm{SCR}}&=& \\sum_i \\sum_j \\sum_k (y_{ijk}- \\bar{y_{ij}})^2.\n\\end{eqnarray*}\\]\nAfin de bien visualiser les vecteurs voici un exemple avec \\(I=2\\), \\(J=3\\) et \\(r=2\\) en remplaçant les \\(0\\) par \\(.\\) :\n\\[\n\\begin{array}{*{12}c}\n      \\vec{e}&\\vec{e}_{1.}&\\vec{e}_{2.}&\n      \\vec{e}_{.1}&\\vec{e}_{.2}&\\vec{e}_{.3}&\n      \\vec{e_{11}}&\\vec{e}_{12}&\\vec{e}_{13}&\n      \\vec{e}_{21}&\\vec{e}_{22}&\\vec{e}_{23}&\\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&1&.&.&1&.&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&1&.&.&1&.&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n1&1&.&.&.&1&.&.&1&.&.&.& \\\\\n%%\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&1&.&.&.&.&.&1&.&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&1&.&.&.&.&.&1&.& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n1&.&1&.&.&1&.&.&.&.&.&1& \\\\\n\\end{array}\n\\]\n\nEn écrivant les vecteurs dans le cadre général et en faisant la somme ci-dessous \\[\n\\vec{Y}=\\mu \\vec{e}+\\sum_{i} \\alpha_i \\vec{e}_{i.}+\\sum_{j} \\beta_j \\vec{e}_{.j}\n+\\sum_{ij} (\\alpha\\beta)_{ij} \\vec{e}_{ij} + \\vec{\\varepsilon},\n\\tag{1}\\] on a bien que la ligne \\(ijk\\) vaut \\[\ny_{ijk} = \\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+ \\varepsilon_{ijk}.\n\\]\nMontrons que \\(E_1 \\perp E_2\\). Pour cela prenons deux vecteurs quelconques de \\(E_1\\) et $E_2 $, ils s’écrivent \\(m\\vec{e}\\) et \\(\\sum_{i=1}^I a_{i} \\vec{e}_{i.}\\) (avec \\(\\sum_{i=1}^I a_{i}=0\\)) et leur produit scalaire vaut \\[\n&lt;m\\vec{e};\\sum_{i=1}^I a_{i} \\vec{e}_{i.}&gt;=m\\sum_{i=1}^I a_{i}&lt;\\vec{e};\\vec{e}_{i.}&gt; = m I \\sum_{i=1}^I a_{i} =0\n\\] De même avec \\(E_1 \\perp E_3\\).\nMontrons que \\(E_1 \\perp E_4\\). Pour cela prenons deux vecteurs quelconques de \\(E_1\\) et \\(E_4\\), ils s’écrivent \\(m\\vec{e}\\) et \\(\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}\\) (avec pour tout \\(i\\) \\(\\sum_j (ab)_{ij}=0\\) et pour tout \\(j\\) \\(\\sum_i (ab)_{ij}=0\\)). Leur produit scalaire vaut \\[\\begin{align*}\n  &lt;m\\vec{e};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}&gt;\n  &=m\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}&lt;\\vec{e};\\vec{e}_{ij}&gt; = m r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\\]\nMontrons que \\(E_2 \\perp E_4\\). Pour cela prenons deux vecteurs quelconques de \\(E_2\\) et \\(E_4\\), ils s’écrivent \\(\\sum_{l=1}^I a_{l} \\vec{e}_{l}\\) (avec \\(\\sum_{l=1}^I a_{l}=0\\)) et \\(\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij} \\vec{e}_{ij}\\) (avec \\(i\\) \\(\\sum_j (ab)_{ij}=0\\) et pour tout \\(j\\) \\(\\sum_i (ab)_{ij}=0\\). Leur produit scalaire vaut \\[\\begin{align*}\n  &lt;\\sum_{l=1}^I a_{l} \\vec{e}_{l.};\\sum_{i=1}^I\\sum_{j=1}^J (ab)_{ij}\\vec{e}_{ij}&gt;\n  &=\\sum_{l=1}^I a_{l}\\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij}&lt;\\vec{e}_{l.};\\vec{e}_{ij}&gt; \\\\\n  &=\\sum_{l=1}^I a_{l} r \\sum_{i=1}^I \\sum_{j=1}^J (ab)_{ij} =0\n\\end{align*}\\] De même avec \\(E_3 \\perp E_4\\).\nLa dimension de \\(E_{1}\\) vaut 1 (car \\(\\vec{e}\\) est non nul). Le sous-espace \\(E_{2}\\) est engendré par les \\(I\\) vecteurs non nuls et orthogonaux deux à deux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) donc le sous espace engendré est au moins de dimension \\(I\\). Cependant on ajoute une contrainte linéaire donc \\(dim(E_{2})=I-1\\). De même pour \\(E_{2}\\) dont la dimension est donc \\(J-1\\). Enfin sous-espace \\(E_{4}\\) est engendré par les \\(IJ\\) vecteurs non nuls et orthogonaux deux à deux \\(\\{\\vec{e}_{ij}\\}\\) (donc le sous espace engendré est de dimension \\(IJ\\)) mais auquel on ajoute plusieurs contraintes linéaires. Il faut donc compter le nombre de contrainte linéaires indépendantes.\nMontrons que les \\(I+J\\) contraintes $ i {j} (ab){ij}=0$ et \\(\\forall j \\sum_{i} (ab)_{ij}=0\\) ne sont pas indépendantes. En effet quand \\(I+J-1\\) contraintes sont vérifiées, la dernière restante l’est aussi. \\[\n  \\begin{array}{*{5}c}\n  (ab)_{11}&(ab)_{12}&\\ldots&(ab)_{1J-1}&(ab)_{1J}&=0\\\\\n  (ab)_{21}&(ab)_{22}&\\ldots&(ab)_{2J-1}&(ab)_{2J}&=0\\\\\n  \\vdots&\\vdots& &\\vdots&\\vdots&\\vdots\\\\\n  (ab)_{I1}&(ab)_{I2}&\\ldots&(ab)_{IJ-1}&(ab)_{IJ}&=0\\\\\n  =0&=0&\\ldots&=0&c=?&\n\\end{array}\n\\] Posons que \\(I+J-1\\) contraintes sont vérifiées~: \\(I\\) en ligne et \\(J-1\\) en colonnes (voir ci-dessus). En sommant toute la matrice on sait (somme en ligne) que cela vaut zéro et donc la somme en colonne vaut elle aussi 0 et donc la dernière somme \\(c\\) vaut 0 (voir ci-dessus). Nous avons donc que la dimension de \\(E_{4}\\) est \\(IJ-(I +J -1)=(I-1)(J-1)\\)\n\nCalculons \\(P_{1}Y\\) \\[\\begin{align*}\nP_{1}Y &= \\mathbf{1}(\\mathbf{1}' \\mathbf{1})^{-1} \\mathbf{1}'Y=\\mathbf{1}(IJr)^{-1} \\sum_{ijk}Y_{ijk} = \\frac{1}{IJr} Y_{...} \\mathbf{1}\\\\\n      &=\\bar Y_{...} \\vec{e}\n\\end{align*}\\]\nCalculons \\(P_{2}Y\\). On sait que \\(F_{2}=E_1 \\stackrel{\\perp}{\\bigoplus} E_2\\) et que \\(F_{2}\\) est de dimension \\(I\\). Il est donc engendré par les \\(I\\) vecteurs orthogonaux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) qui en forme une base. Du fait de la décomposition on a \\[\n\\begin{align}\nP_{F_{2}}Y &= P_{1}Y  + P_{2}Y\n\\end{align}\n\\tag{2}\\] Calculons maintenant directement la projection sur \\(F_{2}\\). Ce sous-espace est engendré par les \\(I\\) vecteurs orthogonaux \\(\\{\\vec{e}_{i.}\\}_{i=1}^{I}\\) en posant la matrice concaténant les (coordonnées des) vecteurs: \\[\\begin{align*}\nF_{2}&=(\\vec{e}_{1.}|\\vec{e}_{2}|\\cdots|\\vec{e}_{I.})\n\\end{align*}\\] on a le projecteur \\[\\begin{align*}\nP_{F_{2}}&=F_{2}(F_{2}'F_{2})^{-1}F'_{2}\n\\end{align*}\\] En effectuant le calcul matriciel on a \\[\\begin{align*}\n(F_{2}'F_{2})&=\\mathop{\\mathrm{diag}}(I, I, \\dotsc, I)\n\\end{align*}\\] et par calcul matriciel direct on trouve \\[\n\\begin{align}\nP_{F_{2}}Y&=F_{2}\\mathop{\\mathrm{diag}}(1/I, 1/I, \\dotsc, 1/I)F_{2}'Y\\nonumber\\\\\n&=F_{2}\n\\begin{pmatrix}\n\\bar Y_{1..}\\\\\n\\bar Y_{2..}\\\\\n\\vdots\\\\\n\\bar Y_{I..}\\\\\n\\end{pmatrix}\n= \\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n+ \\dotsc \\bar Y_{I..}\\vec{e}_{I.}\n\\end{align}\n\\tag{3}\\] En utilisant les équations équation 2 et équation 3 on trouve \\[\\begin{align*}\nP_{2}Y&=\\bar Y_{1..} \\vec{e}_{1.} + \\bar Y_{2..}\\vec{e}_{2.}\n+ \\dotsc + \\bar Y_{I..}\\vec{e}_{I.} - \\bar Y_{...} \\vec{e}.\n\\end{align*}\\] Remarquons que \\(\\vec{e}=\\vec{e}_{1.} + \\dotsc +\\vec{e}_{I.}\\) et en remplaçant cela dans l’équation précédente nous avons \\[\n\\begin{align*}\nP_{2}Y&=(\\bar Y_{1..} -\\bar Y_{...})  \\vec{e}_{1.} + (\\bar Y_{2..} - \\bar Y_{...}) \\vec{e}_{2.}\n+ \\dotsc + (\\bar Y_{I..} -\\bar Y_{...}) \\vec{e}_{I.}.\n\\end{align*}\n\\]\nEn calquant ces calculs pour \\(E_{3}\\) on trouve \\[\\begin{align*}\nP_{3}Y&=(\\bar Y_{.1.}-\\bar Y_{...}) \\vec{e}_{.1} + (\\bar Y_{.2.}-\\bar Y_{...})\\vec{e}_{.2}\n+ \\dotsc +(\\bar Y_{.J.}-\\bar Y_{...})\\vec{e}_{.J}.\n\\end{align*}\\]\nEnfin pour \\(E_{4}\\), remarquons que \\(F_{4}=E=\\mathop{\\mathrm{\\mathsf{vect}}}(\\vec{e}_{11}, \\dotsc, \\vec{e}_{IJ})\\). La projection sur \\(E\\) identifié à sa matrice \\((\\vec{e}_{11}| \\dotsc |\\vec{e}_{IJ})\\) peut être calculée de manière directe comme \\[\\begin{align}\nP_{E}Y&=E\\mathop{\\mathrm{diag}}(1/r, 1/r, \\cdots, 1/r)E'Y\\nonumber\\\\\n&=E\n\\begin{pmatrix}\n\\bar Y_{11.}\\\\\n\\bar Y_{21.}\\\\\n\\vdots\\\\\n\\bar Y_{IJ.}\\\\\n\\end{pmatrix}\n= \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\\label{eq:pebis}\n\\end{align}\\] En se servant de la décomposition on a \\[\\begin{align}\nP_{E}Y&=P_{1}Y + P_{2}Y  + P_{3}Y  + P_{4}Y\n\\end{align}\\] Et en identifiant les deux calculs (avec \\(\\vec{e}_{i.}=\\vec{e}_{i1} + \\dotsc +\\vec{e}_{iJ}\\) et \\(\\vec{e}_{.j}=\\vec{e}_{1j} + \\dotsc +\\vec{e}_{Ij}\\) ) \\[\\begin{align*}\nP_{4}Y&= (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ}.\n\\end{align*}\\]\nLa dernière projection s’obtient comme \\[\\begin{align*}\nQY&=Y- P_{E}Y = Y -  \\bar Y_{11.} \\vec{e}_{11} + \\dotsc \\bar Y_{IJ.}\\vec{e}_{IJ}\n\\end{align*}\\]\n\nEn reprenant la décomposition en sous-espace orthogonaux suivante \\[\n\\begin{align}\n\\mathbb R^{n}&= E  \\stackrel{\\perp}{\\bigoplus} E^{\\perp} =E  \\stackrel{\\perp}{\\bigoplus} Q\\\\\n&= E_1 \\stackrel{\\perp}{\\bigoplus} E_2 \\stackrel{\\perp}{\\bigoplus} E_3 \\stackrel{\\perp}{\\bigoplus} E_4 \\stackrel{\\perp}{\\bigoplus} Q\n\\end{align}\n\\tag{4}\\] On a donc que \\[\n\\begin{align*}\nY &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y.\n\\end{align*}\n\\tag{5}\\] En utilisant toutes les définitions de la question précédente on a \\[\n\\begin{split}\nY&=\\bar Y_{...} \\vec{e}\\\\\n& \\ \\  +  (\\bar Y_{1..} - \\bar Y_{...}) \\vec{e}_{1.} + \\dotsc +\\bar (Y_{I..}- \\bar Y_{...})\\vec{e}_{I.}\\\\\n& \\ \\    + (\\bar Y_{.1.}- \\bar Y_{...}) \\vec{e}_{.1} +  \\dotsc +\\bar (Y_{.J.}- \\bar Y_{...})\\vec{e}_{.J} \\\\\n& \\ \\  + (\\bar Y_{11.} - \\bar Y_{1..} - \\bar Y_{.1.} + \\bar Y_{...})\\vec{e}_{11}+ \\dotsc + (\\bar Y_{IJ} - \\bar Y_{I..} - \\bar Y_{.J.} + \\bar Y_{...})\\vec{e}_{IJ} \\\\\n& \\ \\ + P_{Q}Y.\n\\end{split}\n\\tag{6}\\] En utilisant l’équation 1 on identifie terme à terme et nous obtenons les paramètres du modèle : \\[\\begin{align*}\n\\hat \\mu&=\\bar Y_{...}\\\\\n\\hat \\alpha_{i}&=(\\bar Y_{i..} - \\bar Y_{...})\\\\\n\\hat \\beta_{j}&=(\\bar Y_{.j.} - \\bar Y_{...})\\\\\n(\\widehat{\\alpha\\beta})_{ij}&=(\\bar Y_{ij.} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})\\\\\n\\end{align*}\\]\nEn utilisant l’équation 5 et en se rappelant de l’orthogonalité (équation 4) on a \\[\n\\begin{align}\n\\|Y -\\bar Y_{...} \\vec{e}\\|^{2} &=\\| P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y + P_{Q}Y\\|^{2}\\nonumber\\\\\n&=\\|P_{1}Y\\|^{2} + \\|P_{2}Y\\|^{2} + \\|P_{3}Y\\|^{2} + \\|P_{4}Y\\|^{2} + \\|P_{Q}Y\\|^{2}\n\\end{align}\n\\tag{7}\\] et remplaçant les projections par leur expression (voir par exemple équation 6) et calculant les normes on a \\[\n\\begin{split}\n\\sum_{i=1}^I \\sum_{j=1}^J\\sum_{k=1}^r (Y_{ijk} - \\bar Y_{...})^{2}\n&= rJ\\sum_{i=1}^I(\\bar Y_{i..} - \\bar Y_{...})^{2}\n+ rI\\sum_{j=1}^J(\\bar Y_{.j.} - \\bar Y_{...})^{2}\\\\\n& \\ \\  + r\\sum_{i=1}^I\\sum_{j=1}^J\n(\\bar Y_{ij} - \\bar Y_{i..} - \\bar Y_{.j.} + \\bar Y_{...})^{2}\\\\\n& \\ \\  + \\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^r\n(Y_{ijk} -  \\bar Y_{ij})^{2}.\n\\end{split}\n\\tag{8}\\] En multipliant par \\(1/n\\) l’équation ci-dessus nous obtenons la décomposition de la variance.\nLe vecteur \\(Y\\) grâce à \\({\\mathcal{H}}_{3}\\) est un vecteur gaussien de moyenne \\(\\vec{m}=\\mu +\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}\\) et de variance \\(\\sigma^{2}I_{n}\\). On sait que \\[\\begin{align*}\nP_{E}Y &= P_{1}Y + P_{2}Y + P_{3}Y + P_{4}Y\n\\end{align*}\\] Grâce au théorème de Cochran on a que \\[\n\\frac{\\|P_{i}Y -P_{i}\\vec{m} \\|^{2}}{\\sigma^{2}} \\sim \\chi^{2} (dim(E_{i}))\n\\] ou encore que \\(\\frac{\\|P_{i}Y\\|^{2}}{\\sigma^{2}}\\) suit un \\(\\chi^{2} (dim(E_{i}))\\) décentré de paramètre de décentrage \\(\\|P_{i}\\vec{m} \\|^{2}\\). Pour \\(Q=E^{\\perp}\\) on a que \\(P_{Q}\\vec{m}=0\\) et il n’y a pas de décentrage.\nReprenons l’équation 7 qui s’écrit aussi avec des sommes (équation 8) ou encore \\[\\begin{align*}\n\\mathop{\\mathrm{SCT}}\n&= \\mathop{\\mathrm{SCE}}_{a} + \\mathop{\\mathrm{SCE}}_{b} + \\mathop{\\mathrm{SCE}}_{ab}  + \\mathop{\\mathrm{SCR}}\n\\end{align*}\\] On a donc que \\[\\begin{align*}\n\\mathop{\\mathrm{SCE}}_{a}&=\\|P_{2}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (I-1), \\|P_{2}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{b}&=\\|P_{3}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (J-1), \\|P_{3}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{ab}&=\\|P_{4}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} ((I-1)(J-1)), \\|P_{4}\\vec{m} \\|^{2}),\\\\\n\\mathop{\\mathrm{SCE}}_{ab}&=\\|P_{Q}Y\\|^{2}\\sim \\sigma^{2}\\chi^{2} (n - IJ).\n\\end{align*}\\] Nous voyons donc que chaque terme est la norme carrée d’une projection du vecteur gaussien \\(Y\\) dans un sous-espace et que ces sous-espaces sont orthogonaux 2 à 2. Les vecteurs gaussiens projetés sont donc indépendants ainsi que leur norme au carré. Les lois de ces normes carrées sont donc à \\(\\sigma^{2}\\) près des \\(\\chi^{2}\\) décentrés (sauf pour \\(Q\\)) qui sont indépendants.\nNotons \\(\\mathop{\\mathrm{CME}}_{ab} = \\mathop{\\mathrm{SCE}}_{ab}/((I-1)(J-1))\\) et \\(\\mathop{\\mathrm{CMR}}= \\mathop{\\mathrm{SCR}}/(n-IJ)\\) nous avons donc \\[\\begin{align*}\n\\frac{\\mathop{\\mathrm{CME}}_{ab}}{\\mathop{\\mathrm{CMR}}}&=\\frac{\\frac{\\mathop{\\mathrm{SCE}}_{ab}}{\\sigma^{2}(I-1)(J-1)}}{\\frac{\\mathop{\\mathrm{SCR}}}{\\sigma^{2}(n-IJ)}}\n\\end{align*}\\] qui est le rapport de deux \\(\\chi^{2}\\) indépendants ramenés à leur degrés de liberté et dont le numérateur est décentré. Nous avons donc une loi de Fisher de paramètres \\((I-1)(J-1), n -IJ, \\|P_{4}\\vec{m} \\|^{2}\\). Sous \\({\\mathrm{H_0}}:\\) « il n’y a pas d’interaction » (ou \\(P_{4}\\vec{m}=0\\)) alors la loi se simplifie et le paramètre de décentrage inconnu (qui dépend de \\(\\vec{m}\\)) disparaît et la loi est \\(F((I-1)(J-1), n -IJ)\\)."
  },
  {
    "objectID": "correction/chap7.html",
    "href": "correction/chap7.html",
    "title": "7 Choix de variables",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, C, B en général. Un cas particulier de la dernière question est le suivant : si les variables sélectionnées \\(\\xi\\) engendrent un sous-espace orthogonal au sous-espace engendré par les variables non sélectionnées \\(\\bar\\xi\\), alors C est la bonne réponse.\n\n\nExercice 2 (Analyse du biais) La preuve des deux premiers points s’effectue comme l’exemple de la section 7.2.1. Nous ne détaillerons que le premier point. Supposons que \\(|\\xi|\\) soit plus petit que \\(p\\), le “vrai” nombre de variables entrant dans le modèle. Nous avons pour estimateur de \\(\\beta\\) \\[\n\\hat \\beta_{\\xi} = (X_{\\xi}'X_{\\xi})^{-1}X_{\\xi}'Y = P_{X_{\\xi}}Y.\n\\] Le vrai modèle étant obtenu avec \\(p\\) variables, \\(\\mathbf E(Y)=X_p \\beta\\). Nous avons alors \\[\n\\begin{eqnarray*}\n\\mathbf E(\\hat \\beta_{\\xi})&=&P_{X_{\\xi}}X_p \\beta\\\\\n&=& P_{X_{\\xi}}X_{\\xi} \\beta_{\\xi} +P_{X_{\\xi}}X_{\\bar \\xi} \\beta_{\\bar \\xi}.\n\\end{eqnarray*}\n\\] Cette dernière quantité n’est pas nulle sauf si \\(\\Im(X_{\\xi}) \\perp \\Im(X_{\\bar \\xi})\\). Comme \\(\\hat \\beta_{\\xi}\\) est en général biaisé, il en est de même pour la valeur prévue \\(\\hat y_{\\xi}\\) dont l’espérance ne vaudra pas \\(X\\beta\\).\n\n\nExercice 3 (Variance des estimateurs) L’estimateur obtenu avec les \\(|\\xi|\\) variables est noté \\(\\hat \\beta_{\\xi}\\) et l’estimateur obtenu dans le modèle complet \\(\\hat \\beta\\). Ces vecteurs ne sont pas de même taille, le premier est de longueur \\(|\\xi|\\), le second de longueur \\(p\\). Nous comparons les \\(|\\xi|\\) composantes communes, c’est-à-dire que nous comparons \\(\\hat \\beta_{\\xi}\\) et \\([\\hat \\beta]_{\\xi}\\). Partitionnons la matrice \\(X\\) en \\(X_{\\xi}\\) et \\(X_{\\bar \\xi}\\). Nous avons alors \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta) &=& \\sigma^2 \\left(\n\\begin{array}{cc}\nX'_{\\xi}X_{\\xi} &X'_{\\xi}X_{\\bar \\xi}\\\\\nX'_{\\bar \\xi}X_{\\xi} &X'_{\\bar \\xi}X_{\\bar \\xi}\n\\end{array}\n\\right)^{-1}.\n\\end{eqnarray*}\n\\] En utilisant la formule d’inverse par bloc, donnée en annexe A, nous obtenons \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}\\right]^{-1},\n\\end{eqnarray*}\n\\] alors que la variance de \\(\\hat \\beta_{\\xi}\\) vaut \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi}) &=& \\sigma^2 \\left[X_{\\xi}'X_{\\xi}\\right]^{-1}.\n\\end{eqnarray*}\n\\] Nous devons comparer \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})\\) et \\(\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi})\\). Nous avons \\[\n\\begin{eqnarray*}\nX_{\\xi}'X_{\\xi}-X_{\\xi}'X_{\\bar \\xi}(X_{\\bar \\xi}'\nX_{\\bar \\xi})^{-1}X_{\\bar \\xi}'X_{\\xi}=X_{\\xi}'(I-P_{X_{\\bar \\xi}})X_{\\xi}\n=X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}.\n\\end{eqnarray*}\n\\] La matrice \\(P_{X_{\\bar \\xi}^\\perp}\\) est la matrice d’un projecteur, alors elle est semi-définie positive (SDP) (cf. annexe A), donc \\(X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}\\) est également SDP. La matrice \\(X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}^\\perp}X_{\\xi}\\) est définie positive (DP) puisque c’est \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})/ \\sigma^2\\). Utilisons le changement de notation suivant : \\[\n\\begin{eqnarray*}\nA=X_{\\xi}'X_{\\xi}-X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi} \\quad \\hbox{et} \\quad\nB=X_{\\xi}'P_{X_{\\bar \\xi}}X_{\\xi}.\n\\end{eqnarray*}\n\\] La matrice \\(A\\) est DP et la matrice \\(B\\) SDP. La propriété donnée en annexe A indique que \\(A^{-1}-(A+B)^{-1}\\) est SDP, or \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})-\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi}) = \\sigma^2 (A^{-1}-(A+B)^{-1}).\n\\end{eqnarray*}\n\\] Donc la quantité \\(\\mathop{\\mathrm{V}}([\\hat \\beta]_{\\xi})-\\mathop{\\mathrm{V}}(\\hat \\beta_{\\xi})\\) est SDP. Le résultat est démontré. L’estimation, en terme de variance, de \\(\\xi\\) composantes est plus précise que les mêmes \\(\\xi\\) composantes extraites d’une estimation obtenue avec \\(p\\) composantes.\nLa variance des valeurs ajustées dépend de la variance de \\(\\hat \\beta\\), le point 2 de la proposition se démontre de façon similaire.\nRemarque : nous venons de comparer deux estimateurs de même taille via leur matrice de variance. Pour cela, nous montrons que la différence de ces deux matrices est une matrice SDP. Que pouvons-nous dire alors sur la variance de chacune des coordonnées ? Plus précisément, pour simplifier les notations, notons le premier estimateur (de taille \\(p\\)) \\(\\tilde \\beta\\) de variance \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)\\) et le second estimateur \\(\\hat \\beta\\) de variance \\(\\mathop{\\mathrm{V}}(\\hat \\beta)\\). Si \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta)\\) est SDP, pouvons-nous dire que \\(\\mathop{\\mathrm{V}}(\\tilde \\beta_i)-\\mathop{\\mathrm{V}}(\\hat \\beta_i)\\) est un nombre positif pour \\(i\\) variant de \\(1\\) à \\(p\\) ? Considérons par exemple le vecteur \\(u_1'=(1,0,\\dotsc,0)\\) de \\(\\mathbb R^p\\). Nous avons alors \\[\nu_1' \\hat \\beta = \\hat \\beta_1 \\quad \\hbox{et}\n\\quad u_1' \\tilde \\beta = \\tilde \\beta_1.\n\\] Comme \\(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta)\\) est SDP, nous avons pour tout vecteur \\(u\\) de \\(\\mathbb R^p\\) que \\(u'(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta))u\\geq 0\\), c’est donc vrai en particulier pour \\(u_1\\). Nous avons donc \\[\n\\begin{eqnarray*}\nu_1'(\\mathop{\\mathrm{V}}(\\tilde \\beta)-\\mathop{\\mathrm{V}}(\\hat \\beta))u_1&\\geq& 0\\\\\nu_1'\\mathop{\\mathrm{V}}(\\tilde \\beta)u_1-u_1'\\mathop{\\mathrm{V}}(\\hat \\beta)u_1&\\geq& 0\\\\\n\\mathop{\\mathrm{V}}(u_1'\\tilde \\beta)-\\mathop{\\mathrm{V}}(u_1'\\hat \\beta)&\\geq& 0\\\\\n\\mathop{\\mathrm{V}}(\\tilde \\beta_1) &\\geq & \\mathop{\\mathrm{V}}(\\hat \\beta_1).\n\\end{eqnarray*}\n\\] Nous pouvons retrouver ce résultat pour les autres coordonnées des vecteurs estimés ou encore pour des combinaisons linéaires quelconques de ces coordonnées.\n\n\nExercice 4 (Choix de variables) Tous les modèles possibles ont été étudiés, la recherche est donc exhaustive. En prenant comme critère l’AIC ou le BIC, le modèle retenu est le modèle M134. Comme prévu, le \\(\\mathop{\\mathrm{R^2}}\\) indique le modèle conservant toutes les variables. Cependant le \\(\\mathop{\\mathrm{R^2}}\\) peut être utilisé pour tester des modèles emboîtés. Dans ce cas, le modèle retenu est également le M134.\nPour une procédure avec test nous devons démarrer d’un modèle. Démarrons par exemple du modèle avec uniquement la constante. Nous ajoutons une variable après l’autre. Nous ajoutons celle avec la statistique de test la plus élevée et cette valeur doit être plus grande que 2.3 (sinon aucune variable n’est ajoutée et le modèle courant est conservé). Dans les modèles à une variable, c’est le modèle M1 qui est choisi (statistique la plus élevée de 41.9 et supérieure à 2.3). Ensuite nous ajoutons à M1 une variable (2 ou 3 ou 4) et la meilleure est la 4 mais la statistique est de 0.9 (&lt; 2.3) on n’ajoute pas de variable et on choisit M1.\nDémarrons du modèle complet 1234, on enlève la moins significative (la valeur de la statistique de test la plus faible en dehors de la constante et qui doit être plus faible que 2.3). Ici nous enlevons la variable 2 et nous avons donc le modèle M134. Dans ce modèle la variable la moins significative est la 3 mais sa statistique est plus grande que 2.3, on conserve donc le modèle M134.\n\n\nExercice 5 (Utilisation du \\(R^2\\)) Plaçons nous dans le cas pratique où la constante fait partie des modèles, elle est donc dans une des colonnes de \\(Z\\) (par exemple la première). Le \\(\\mathop{\\mathrm{R^2}}\\) est par définition \\[\n\\begin{align*}\n\\mathop{\\mathrm{R^2}}(Z)&=\\frac{\\|P_{Z}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\\\\\n\\mathop{\\mathrm{R^2}}(X)&=\\frac{\\|P_{X}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\n\\end{align*}\n\\] Comme \\(\\Im(Z)\\subset \\Im(X)\\) on peut décomposer en deux \\(\\Im(X)\\): la partie \\(\\Im(Z)\\) puis le reste ce qui se note \\[\n\\Im(X)=\\Im(Z) \\stackrel{\\perp}{\\oplus} (\\Im(X)\\cap \\Im(Z)^{\\perp})\n\\] Au niveau des projecteurs on a donc \\[\nP_{X}=P_{Z} + P_{X\\cap Z^{\\perp}}\n\\] ce qui permet d’écrire le \\(\\mathop{\\mathrm{R^2}}\\) du modèle avec \\(X\\) comme suit puis par pythagore (\\((P_{Z}Y - P_{\\mathbf{1}}Y)\\in \\Im(Z)\\) et \\(P_{X\\cap Z^{\\perp}}Y\\in \\Im(Z)^{\\perp}\\)) \\[\n\\begin{align*}\n\\mathop{\\mathrm{R^2}}(X)&=\\frac{\\|P_{Z}Y + P_{X\\cap Z^{\\perp}}Y - P_{\\mathbf{1}}Y\\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}} \\\\\n&= \\frac{\\|(P_{Z}Y - P_{\\mathbf{1}}Y) + P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\\\\\n&=\\frac{\\|(P_{Z}Y - P_{\\mathbf{1}}Y) \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}} + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}= \\mathop{\\mathrm{R^2}}2(Z) + \\frac{\\|P_{X\\cap Z^{\\perp}}Y \\|^{2}}{\\|Y - P_{\\mathbf{1}}Y\\|^{2}}\n\\end{align*}\n\\] Comme la seconde partie est positive ou nulle on a que \\(\\mathop{\\mathrm{R^2}}(X)\\) est au moins aussi grand que \\(\\mathop{\\mathrm{R^2}}(Z)\\). Il y a égalité dans le cas rare où \\(Y\\perp (\\Im(X)\\cap \\Im(Z)^{\\perp})\\) c’est à dire que l’on a ajouté des variables qui ne sont pas entièrement reliées aux variables de \\(Z\\) mais dont la partie non redondante avec celle de \\(Z\\) (qui existe car le rang de \\(X\\) est \\(p\\)) a une corrélation empirique avec \\(Y\\) de 0 exactement. Comme le \\(\\mathop{\\mathrm{R^2}}\\) augmente en ajoutant des variables le modèle sélectionné sera celui avec toutes les variables.\nDonc si nous cherchons à expliquer la concentration d’ozone à Rennes et si nous avons en même temps la consommation de nouille au Viêt Nam les mêmes jours, le \\(\\mathop{\\mathrm{R^2}}\\) nous conduira à sélectionner aussi la consommation de nouille au Viet-Nam comme variable explicative, variable dont on sent le peu d’influence sur la concentration d’ozone.\n\n\nExercice 6 (Cas orthogonal)  \n\nLes variables sont orthogonales donc on a \\(X'X=I_{p}\\) et l’estimateur des MCO s’écrit \\[\n\\hat \\beta=(X'X)^{-1}X'Y=X'Y\n\\] En remplaçant \\(Y\\) par le modèle (\\(Y=X\\beta + \\varepsilon\\)) on a \\[\n\\hat \\beta=X'X\\beta + X'\\varepsilon = \\beta + X'\\varepsilon.\n\\]\nLa somme des résidus vaut ici \\[\n\\begin{align*}\n\\mathop{\\mathrm{SCR}}&=\\|Y-X\\hat\\beta\\|^{2}=(Y-X\\hat\\beta)'(Y-X\\hat\\beta)= Y'Y - 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta\\\\\n&= \\sum_{i=1}^n y_{i}^2- 2Y'X\\hat\\beta + \\hat\\beta'X'X\\hat\\beta.\n\\end{align*}\n\\] Évaluons le second terme du membre de droite: comme \\(X\\hat\\beta=P_{X}Y\\) et en utilisant le fait que \\(Y=P_{X}Y + P_{X^{\\perp}}Y\\) on obtient \\[\nY'X\\hat\\beta= (P_{X}Y + P_{X^{\\perp}}Y)'P_{X}Y= Y' P_{X}' P_{X}Y = \\hat\\beta'X'X\\hat \\beta\n\\] car \\(P_{X}Y\\) et \\(P_{X^{\\perp}}Y\\) sont orthogonaux et leur produit scalaire \\(Y'P_{X^{\\perp}}'P_{X}Y\\) vaut 0. On a donc \\[\n\\mathop{\\mathrm{SCR}}=\\sum_{i=1}^n y_{i}^2 - \\hat\\beta'X'X\\hat \\beta\n\\] Comme ici la matrice \\(X\\) est orthogonale on a \\(X'X=I_{p}\\) et l’expression devient \\[\n\\mathop{\\mathrm{SCR}}=\\sum_{i=1}^n y_{i}^2 - \\sum_{j=1}^p \\hat \\beta_{j}\n\\tag{1}\\] La somme du carré des résidus est d’autant plus faible que les coefficients estimés sont grands en valeur absolue.\nPrenons un modèle \\(\\xi\\) qui regroupe les \\(k\\) premières variables (ce qui est pratique pour les notations). Comme la matrice \\(X\\) est orthogonale on a au niveau des sous espaces que toutes les variables engendrent des sous-espaces vectoriels orthogonaux que l’on peut regrouper: \\[\n\\begin{align*}\n\\Im(X) &= \\Im(X_{1})\\stackrel{\\perp}{\\oplus} \\Im(X_{2})\\stackrel{\\perp}{\\oplus} \\cdots \\stackrel{\\perp}{\\oplus}\\Im(X_{p})\\\\\n&= \\Im(X_{\\xi})\\stackrel{\\perp}{\\oplus}\\Im(X_{\\bar\\xi})\n\\end{align*}\n\\] Ce qui donne au niveau des projecteurs \\[\nP_{X}= P_{X_{\\xi}} + P_{X_{\\bar\\xi}}\n\\] Écrivons par ailleurs l’ajustement: \\[\n\\begin{align*}\nP_{X}Y&= X\\hat \\beta= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\\\\\nP_{X_{\\xi}}Y + P_{X_{\\bar\\xi}}Y&= X_{1}\\hat \\beta_{1} + X_{2}\\hat \\beta_{2} + \\cdots +X_{p}\\hat \\beta_{p}\n\\end{align*}\n\\] Si je projette sur \\(\\Im(X_{\\xi})\\) l’équation ci-dessus cela nous donne (\\(\\Im(X_{\\bar\\xi})\\subset \\Im(X_{\\xi})^{\\perp}\\)) \\[\nP_{X_{\\xi}}Y= X_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}\n\\] Or nous savons que les MCO dans le modèle \\(\\xi\\) donne l’ajustement \\(P_{X_{\\xi}}Y=X_{\\xi}\\hat\\beta_{\\xi}\\) qui est unique, ce qui donne en identifiant les deux écritures que \\[\nX_{1}\\hat \\beta_{1}  + \\cdots +X_{k}\\hat \\beta_{k}= X_{\\xi}\n\\] Comme une base de \\(\\Im(X_{\\bar\\xi})\\) est donnée par les colonnes orthonormées de \\(X_{\\xi}\\) on en déduit que les coefficients dans la base sont uniques d’où \\[\n[\\hat \\beta]_{\\xi}= \\hat\\beta_{\\xi}.\n\\]\nLes critères AIC et BIC valent pour le modèle \\(\\xi\\) \\[\n-2\\mathcal{L} + 2 |\\xi + 1| f(n)\n\\] et quand on compare le modèle \\(\\xi\\) et le modèle \\(\\{\\xi, l\\}\\) on compare donc \\[\n-2\\mathcal{L}(\\xi) + 2 |\\xi + 1| f(n) \\ \\ \\mathrm{et} \\ \\ -2\\mathcal{L}(\\{\\xi, l\\}) + 2 |\\xi + 2| f(n)\n\\] Comme \\(\\mathcal{L}(\\xi)\\) vaut \\(-{n}/{2}\\log\\sigma^{2} -{n}/{2}-{1}/{2\\sigma^{2}}.\\mathop{\\mathrm{SCR}}(\\xi)\\) lorsque l’on compare on peut éliminer les termes identiques et il reste donc \\[\n\\frac{1}{\\sigma^{2}}\\mathop{\\mathrm{SCR}}(\\xi) \\ \\ \\mathrm{et} \\ \\  \\frac{1}{\\sigma^{2}}\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\}) + 2  f(n)\n\\] Effectuons la différence: \\[\n\\Delta =  \\frac{1}{\\sigma^{2}}(\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\}) - \\mathop{\\mathrm{SCR}}(\\xi)) + 2  f(n)\n\\] et en utilisant équation 1 et la question 3 on obtient \\[\n\\begin{align*}\n\\Delta &= \\frac{1}{\\sigma^{2}}(\\sum_{i=1}^n y_{i}^2 - \\sum_{\\xi} \\hat \\beta_{j}^2 - \\hat \\beta_{l}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2)+ 2  f(n)\\\\\n&= -\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n)\n\\end{align*}\n\\] Quand \\(\\Delta\\) est négatif l’AIC (ou le BIC) du modèle \\(\\{\\xi, l\\}\\) est plus faible que l’AIC (ou le BIC) que celui du modèle \\(\\xi\\), c’est à dire que l’AIC (ou le BIC) du modèle \\(\\{\\xi, l\\}\\) est meilleur: on ajoute la variable \\(l\\). Cela donne donc \\[\n\\begin{align*}\n\\Delta & &lt; 0\\\\\n-\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}} + 2f(n) & &lt; 0\\\\\n\\frac{\\hat \\beta_{l}^2}{\\sigma^{2}}&&gt; 2f(n)\n\\end{align*}\n\\]\nEn écrivant que \\[\nN=\\frac{\\mathop{\\mathrm{SCR}}(\\xi) - \\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{\\sigma^{2}}\n\\] nous voyons qu’il faut évaluer la différence des SCR. En utilisant équation 1 et la question 3 on a \\[\n\\begin{align*}\n\\mathop{\\mathrm{SCR}}(\\xi) - \\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})&=\\sum_{i=1}^n y_{i}^2  -  \\sum_{\\xi} \\hat \\beta_{j}^2 - \\sum_{i=1}^n y_{i}^2 + \\sum_{\\xi} \\hat \\beta_{j}^2 + \\hat \\beta_{l}^2  \\\\\n&= \\hat \\beta_{l}^2\n\\end{align*}\n\\] Nous en déduisons la valeur de \\(N\\) (la dernière égalité découle du calcul de \\(\\hat \\beta\\) en question 1) \\[\nN=\\frac{\\hat\\beta_{l}^{2}}{\\sigma^{2}}= \\frac{(\\beta_{l} + [X' \\varepsilon]_{l})^{2}}{\\sigma^{2}}\n\\] Le dernier membre nous indique que la partie aléatoire est \\([X' \\varepsilon]_{l}\\). D’après \\({\\mathcal{H}}_{3}\\) on a que \\(X' \\varepsilon\\) est gaussien d’espérance \\(\\mathbf E(X'\\varepsilon)=X'\\mathbf E(\\varepsilon)=0\\) et de variance \\(\\mathop{\\mathrm{V}}(X'\\varepsilon)=X'\\mathop{\\mathrm{V}}(\\varepsilon)X=\\sigma^{2}I_{p}\\). Sa coordonnée \\(l\\) notée \\([X' \\varepsilon]_{l}\\) est donc une loi normale \\({\\mathcal{N}}(0, \\sigma^{2})\\) et on en déduit que \\[\n\\frac{\\hat \\beta_{l}}{\\sigma} \\sim {\\mathcal{N}}(\\frac{\\beta_{l}}{\\sigma}, 1).\n\\] Le carré de cette loi normale (noté \\(N\\)) est donc un \\(\\chi^{2}(1)\\) décentré de paramètre de décentrage \\({\\beta_{l}^{2}}/{\\sigma^{2}}\\). On connait donc la loi de la statistique de test \\(N\\) (un \\(\\chi^{2}(1)\\) décentré de paramètre de décentrage \\(\\beta_{l}^{2}\\)). Quand \\({\\mathrm{H_0}}\\) est vrai, c’est-à-dire le modèle \\(\\xi\\) est valide, il n’y a pas la variable \\(l\\) dans le modèle, donc \\(\\beta_{l}=0\\) et le paramètre de décentrage vaut \\(0\\). Le seuil de rejet basé sur la statistique \\(N\\) est son quantile de niveau 0.95 sous \\({\\mathrm{H_0}}\\)’ c’est à dire celui d’un \\(\\chi^{2}(1)\\) à 0.95 :\n\nqchisq(0.95, df=1)\n\n[1] 3.841459\n\n\nPar définition le \\(\\mathop{\\mathrm{R^2_a}}\\) vaut \\[\n\\mathop{\\mathrm{R^2_a}}(\\xi) =1-\\frac{n-1}{\\mathop{\\mathrm{SCT}}}\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}\n\\] Calculons la différence des \\(\\mathop{\\mathrm{R^2_a}}\\) \\[\n\\Delta\\mathop{\\mathrm{R^2_a}}= \\frac{n-1}{\\mathop{\\mathrm{SCT}}}(\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{n-|\\xi| -1})\n\\] Cette différence est positive (on ajoute la variable \\(l\\)) si \\[\n\\begin{align*}\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})}{n-|\\xi| -1}&&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}- \\frac{\\mathop{\\mathrm{SCR}}(\\xi) -\\hat\\beta^{2}_{l} }{n-|\\xi| -1}&&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|} - \\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}.\\frac{n-|\\xi|}{n-|\\xi|-1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1} &&gt;0\\\\\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{n-|\\xi|}\\frac{-1}{n-|\\xi| -1}+ \\frac{\\hat\\beta^{2}_{l}}{n-|\\xi| -1}&&gt;0\n\\end{align*}\n\\] En utilisant l’approximation on a donc approximativement \\[\n\\begin{align*}\n   \\frac{1}{n-|\\xi| -1}(\\hat\\beta^{2}_{l}- \\sigma^{2})  &&gt;0\\\\\n  \\frac{\\sigma^{2}}{n-|\\xi| -1} (\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1)&&gt;0\\\\\n\\end{align*}\n\\] Le premier terme étant positif on retrouve que \\[\n\\begin{align*}\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}- 1&&gt;0\\\\\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^{2}}&&gt;1.\n\\end{align*}\n\\]\nComme \\[\n\\mathop{\\mathrm{C_p}}(\\xi)=\\frac{\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2}-n+2|\\xi|.\n\\] on fait là encore la différence des \\(\\mathop{\\mathrm{C_p}}\\) \\[\n\\begin{align*}\n\\Delta\\mathop{\\mathrm{C_p}}&= \\frac{\\mathop{\\mathrm{SCR}}(\\{\\xi, l\\})-\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2}+2 =\n\\frac{\\mathop{\\mathrm{SCR}}(\\xi) -\\hat\\beta^{2}_{l} -\\mathop{\\mathrm{SCR}}(\\xi)}{\\sigma^2} +2\\\\\n&=-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2\n\\end{align*}\n\\] Cette différence est négative (on ajoute la variable \\(l\\)) si \\[\n-\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2} +2&lt;0 \\quad\\Longleftrightarrow\\quad\n\\frac{\\hat\\beta^{2}_{l}}{\\sigma^2}&gt;2.\n\\]\nD’après la question 3 nous n’avons besoin d’estimer qu’une fois le modèle complet et les coordonnées donnent les estimateurs dans les modèles restreints.\nAlgorithme :\n\nEstimer \\(\\beta\\) dans le modèle complet \\(\\hat \\beta = X'Y\\) .\nDéduire \\(\\hat\\sigma^{2}\\) dans le modèle complet \\(\\hat\\sigma^{2}=\\|Y - X\\hat \\beta\\|^{2}/(n-p)\\) .\nOrdonner les coordonnées dans l’ordre décroissant: \\[\n\\hat \\beta_{{(1)}} \\geq \\hat \\beta_{{(2)}}\\geq \\cdots \\geq \\hat \\beta_{{(p)}}.\n\\] Les colonnes correspondantes seront notées \\((k)\\).\nPour \\(k=1\\) à \\(p\\)\n\nSi \\(\\frac{\\hat\\beta^{2}_{(k)}}{\\hat\\sigma^2}\\) &gt; Seuil alors ajout la variable \\((k)\\)\nSinon Sortie de la boucle, plus de variable à ajouter\n\nLe seuil vaut \\(2f(n)\\) pour l’AIC et le BIC, il vaut 3.84 pour un test, 1 pour le \\(\\mathop{\\mathrm{R^2_a}}\\) et 2 pour le \\(\\mathop{\\mathrm{C_p}}\\)."
  },
  {
    "objectID": "correction/chap8.html",
    "href": "correction/chap8.html",
    "title": "8 Régularisation des moindre carrés : Ridge, Lasso et elastic net",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, B, B, B, A (pour un bon choix de \\(\\lambda\\)) et B, A, C et D.\n\n\nExercice 2 (Projection et régression ridge)  \n\n\nExercice 3 (Variance des valeurs ajustées avec une régression ridge)  \n\n\nExercice 4 (Nombre effectif de paramètres de la régression ridge)  \n\nRappelons que pour une valeur \\(\\kappa\\) donnée, le vecteur de coefficients de la régression ridge s’écrit \\[\n\\hat \\beta_{\\mathrm{ridge}}(\\kappa) = (X'X + \\kappa I)^{-1}X'Y.\n\\] et donc l’ajustement par la régression ridge est \\[\n\\hat Y_{\\mathrm{ridge}}(\\kappa)=X(X'X + \\kappa I)^{-1}X'Y=H^*(\\kappa)Y\n\\]\nSoit \\(U_i\\) le vecteur propre de \\(A\\) associé à la valeur propre \\(d^2_i\\). Nous avons donc par définition que \\[\n\\begin{eqnarray*}\nAU_i&=&d^2_iU_i\\\\\nAU_i+\\lambda U_i&=&d^2_iU_i+\\lambda U_i=(d^2_i+\\lambda) U_i\\\\\n(A+\\lambda I_p)U_i&=&(d^2_i+\\lambda) U_i,\n\\end{eqnarray*}\n\\] c’est-à-dire que \\(U_i\\) est aussi vecteur propre de \\(A+\\lambda I_p\\) associé à la valeur propre \\(\\lambda+d^2_i\\).\nNous savons que \\(X=QD P'\\) avec \\(Q\\) et \\(P\\) matrices orthogonales et \\(D=\\mathop{\\mathrm{diag}}(d_1,\\dotsc,d_p)\\). Puisque \\(Q\\) est orthogonale, nous avons, par définition, \\(Q'Q=I\\). Nous avons donc que \\(X'X=(QD P')'QD P'=PDQ'QDP'=PD^2P'\\). Puisque \\(P\\) est orthogonale \\(P'P=I_p\\) et \\(P^{-1}=P\\). \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{tr}}(X(X'X+\\lambda I_p)^{-1}X')&=&\\mathop{\\mathrm{tr}}((X'X+\\lambda I_p)^{-1}X'X)\\\\\n&=&\\mathop{\\mathrm{tr}}((PD^2P'+\\lambda PP')^{-1}PD^2P')\\\\\n&=&\\mathop{\\mathrm{tr}}((P(D+\\lambda I_p )P')^{-1}PD^2P').\n\\end{eqnarray*}\n\\] Nous avons donc \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{tr}}(X(X'X+\\lambda I_p)^{-1}X')&=&\\mathop{\\mathrm{tr}}( (P')^{-1}(D+\\lambda I_p )^{-1} P^{-1} PD^2P')\\\\\n&=&\\mathop{\\mathrm{tr}}( (P')^{-1}(D+\\lambda I_p )^{-1} D^2P')\\\\\n&=&\\mathop{\\mathrm{tr}}( (D+\\lambda I_p )^{-1} D^2).\n\\end{eqnarray*}\n\\] Selon la définition de \\(H^*(\\kappa)\\), nous savons que sa trace vaut donc \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{tr}}( (D+\\kappa I_p )^{-1} D^2).\n\\end{eqnarray*}\n\\] Comme \\(D\\) et \\(I_p\\) sont des matrices diagonales, leur somme et produit sont simplement leur somme et produit terme à terme des éléments de la diagonale, et donc cette trace (somme des éléments de la diagonale) vaut \\[\n\\sum_{i=1}^{p}{\\frac{d_j^2}{d_j^2+\\kappa}}.\n\\]\n\n\n\nExercice 5 (Estimateurs à rétrecissement - shrinkage)  \n\nSoit le modèle de régression \\[\nY=X\\beta+\\varepsilon.\n\\] En le pré-multipliant par \\(P\\), nous avons \\[\nZ=PY=PX\\beta+P\\varepsilon=DQ\\beta+\\eta=D\\gamma+\\eta.\n\\] Puisque \\(\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2 I_n)\\) et \\(P\\) fixé, nous avons que \\(\\eta=P\\varepsilon\\) suit une loi normale de moyenne \\(\\mathbf E(\\eta)=P\\mathbf E(\\varepsilon)=0\\) et de variance \\(\\mathop{\\mathrm{V}}(\\eta)=P\\mathop{\\mathrm{V}}(\\varepsilon)P'=\\sigma^2PP'=\\sigma^2I_n\\).\nPar définition, \\(Z\\) vaut \\(PY\\) et nous savons que \\(Y\\sim\\mathcal{N}(X\\beta,\\sigma^2 I_n)\\), donc \\(Z\\sim\\mathcal{N}(PX\\beta,\\sigma^2 PP')\\), c’est-à-dire \\(Z\\sim\\mathcal{N}(DQ\\beta,\\sigma^2 I_n)\\) ou encore \\(Z\\sim\\mathcal{N}(D\\gamma,\\sigma^2 I_n)\\). En utilisant la valeur de \\(D\\) nous avons \\[    \n\\begin{eqnarray*}\nD\\gamma&=&\n\\begin{pmatrix}\n  \\Delta \\gamma\\\\\n0\n\\end{pmatrix}.\n\\end{eqnarray*}\n\\] Donc \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2I_p)\\).\nSoit un estimateur de \\(\\beta\\) linéaire en \\(Y\\)~: \\(\\hat \\beta=AY\\). Soit l’estimateur de \\(\\gamma=Q\\beta\\) linéaire en \\(Y\\)~: \\(\\hat\\gamma=Q AY\\). Pour calculer leur matrice de l’EQM, nous devons calculer leur biais et leur variance. Le biais de \\(\\hat \\beta\\) est \\[\nB(\\hat \\beta)=\\mathbf E(\\hat \\beta)-\\beta=\\mathbf E(AY)-\\beta=A\\mathbf E(Y)-\\beta=AX\\beta-\\beta.\n\\] Le biais de \\(\\hat\\gamma\\) s’écrit \\[\nB(\\hat\\gamma)=\\mathbf E(\\hat \\gamma)-\\gamma=\\mathbf E(Q\\hat \\beta)-\\gamma=Q\\mathbf E(\\hat \\beta)-\\gamma=QAX\\beta-\\gamma.\n\\] Comme \\(\\gamma=Q\\beta\\) et \\(Q'Q=I_p\\) nous avons \\[\nB(\\hat\\gamma)=QAXQ'\\gamma-\\gamma.\n\\] La variance de \\(\\hat \\beta\\) s’écrit \\[\n\\mathop{\\mathrm{V}}(\\hat \\beta)=\\mathop{\\mathrm{V}}(AY)=A\\mathop{\\mathrm{V}}(Y)A'=\\sigma^2 AA',\n\\] et celle de \\(\\hat \\gamma\\) est \\[\n\\mathop{\\mathrm{V}}(\\hat\\gamma)=\\mathop{\\mathrm{V}}(Q\\hat \\beta)=Q\\mathop{\\mathrm{V}}(\\hat \\beta)Q'=\\sigma^2 QAA'Q'.\n\\] Nous en déduisons que les matrices des EQM sont respectivement \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\beta)&=&(AX\\beta-\\beta)(AX\\beta-\\beta)'+\\sigma^2 AA',\\\\\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma)&=&(QAXQ'\\gamma-\\gamma)(QAXQ'\\gamma-\\gamma)' + \\sigma^2 QAA'Q',\n\\end{eqnarray*}\n\\] et enfin les traces de ces matrices s’écrivent \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{tr}}(\\mathop{\\mathrm{EQM}}(\\hat \\beta))&=&(AX\\beta-\\beta)'(AX\\beta-\\beta)+\\sigma^2\\mathop{\\mathrm{tr}}(AA'),\\\\\n\\mathop{\\mathrm{tr}}(\\mathop{\\mathrm{EQM}}(\\hat \\gamma))&=&(QAXQ'\\gamma-\\gamma)'(QAXQ'\\gamma-\\gamma)+ \\sigma^2\\mathop{\\mathrm{tr}}(AA').\\\\\n\\end{eqnarray*}\n\\] Rappelons que \\(\\gamma=Q\\beta\\) et que \\(Q'Q=I_p\\), nous avons donc \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{tr}}(\\mathop{\\mathrm{EQM}}(\\hat \\gamma))&=&\\gamma'(QAXQ'-I_p)'(QAXQ'-I_p)\\gamma+ \\sigma^2\\mathop{\\mathrm{tr}}(AA')\\\\\n&=&\\beta'(QAX - Q)'(QAX - Q)\\beta+ \\sigma^2\\mathop{\\mathrm{tr}}(AA')\\\\\n&=&\\beta'(AX-I_p)Q'Q(AX-I_p)\\beta+ \\sigma^2\\mathop{\\mathrm{tr}}(AA')\\\\\n&=&\\beta'(AX-I_p)(AX-I_p)\\beta+ \\sigma^2\\mathop{\\mathrm{tr}}(AA')=\\mathop{\\mathrm{tr}}(\\mathop{\\mathrm{EQM}}(\\hat \\beta)).\n\\end{eqnarray*}\n\\] En conclusion, que l’on s’intéresse à un estimateur linéaire de \\(\\beta\\) ou à un estimateur linéaire de \\(\\gamma\\), dès que l’on passe de l’un à l’autre en multipliant par \\(Q\\) ou \\(Q'\\), matrice orthogonale, la trace de l’EQM est identique, c’est-à-dire que les performances globales des 2 estimateurs sont identiques.\nNous avons le modèle de régression suivant~: \\[\nZ_{1:p}=\\Delta\\gamma+\\eta_{1:p},\n\\] et donc, par définition de l’estimateur des MC, nous avons \\[\n\\hat \\gamma_{\\mathrm{MC}}=(\\Delta'\\Delta)^{-1}\\Delta'Z_{1:p}.\n\\] Comme \\(\\Delta\\) est une matrice diagonale, nous avons \\[\n\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-2}\\Delta'Z_{1:p}=\\Delta^{-1}Z_{1:p}.\n\\] Cet estimateur est d’expression très simple et il est toujours défini de manière unique, ce qui n’est pas forcément le cas de \\(\\hat \\beta_{\\mathrm{MC}}\\).\nComme \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p)\\) nous avons que \\(\\hat \\gamma_{\\mathrm{MC}}=\\Delta^{-1}Z_{1:p}\\) suit une loi normale d’espérance \\(\\mathbf E(\\Delta^{-1}Z_{1:p})=\\Delta^{-1}\\mathbf E(Z_{1:p})=\\gamma\\) et de variance \\(\\mathop{\\mathrm{V}}(\\hat \\gamma_{\\mathrm{MC}})=\\sigma^2\\Delta^{-2}\\). Puisque \\(\\hat \\gamma_{\\mathrm{MC}}\\) est un estimateur des MC, il est sans biais, ce qui est habituel.\nL’EQM de \\(\\hat \\gamma_{\\mathrm{MC}}\\), estimateur sans biais, est simplement sa variance. Pour la \\(i^e\\) coordonnée de \\(\\hat \\gamma_{\\mathrm{MC}}\\), l’EQM est égal à l’élément \\(i,i\\) de la matrice de variance \\(\\mathop{\\mathrm{V}}(\\hat \\gamma_{\\mathrm{MC}})\\), c’est-à-dire \\(\\sigma^2/\\delta_i^2\\). La trace de l’EQM est alors simplement la somme, sur toutes les coordonnées \\(i\\), de cet EQM obtenu.\nPar définition \\(\\hat \\gamma(c)=\\mathop{\\mathrm{diag}}(c_i)Z_{1:p}\\) et nous savons que \\(Z_{1:p}\\sim\\mathcal{N}(\\Delta\\gamma,\\sigma^2 I_p).\\) Nous obtenons que \\(\\hat \\gamma(c)\\) suit une loi normale d’espérance \\(\\mathbf E(\\mathop{\\mathrm{diag}}(c_i)Z_{1:p})=\\mathop{\\mathrm{diag}}(c_i)\\Delta\\gamma\\) et de variance \\[\n\\mathop{\\mathrm{V}}(\\hat \\gamma(c))= \\mathop{\\mathrm{diag}}(c_i)\\mathop{\\mathrm{V}}(Z_{1:p})\\mathop{\\mathrm{diag}}(c_i)'= \\sigma^2\\mathop{\\mathrm{diag}}(c_i^2).\n\\] La loi de \\(\\hat \\gamma(c)\\) étant une loi normale de matrice de variance diagonale, nous en déduisons que les coordonnées de \\(\\hat \\gamma(c)\\) sont indépendantes entre elles.\nCalculons l’EQM de la \\(i^e\\) coordonnée de \\(\\hat \\gamma(c)\\) \\[\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)=\\mathbf E(\\hat \\gamma(c)_i -\\gamma)^2=\\mathbf E(\\hat \\gamma(c)_i^2)+\n\\mathbf E(\\gamma_i^2)-2\\mathbf E(\\hat \\gamma(c)_i \\gamma_i).\n\\] Comme \\(\\gamma_i\\) et que \\(\\mathbf E(\\hat \\gamma(c)_i^2)=\\mathop{\\mathrm{V}}(\\hat \\gamma(c)_i^2)+\\{\\mathbf E(\\hat \\gamma(c)_i^2)\\}^2\\), nous avons \\[\n\\begin{align*}\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\gamma_i\\mathbf E(\\hat \\gamma(c)_i)\\\\\n&=\\sigma^2 c_i^2+(c_i\\delta_i\\gamma_i)^2+\\gamma_i^2-2\\sigma^2 c_i\\delta_i\\gamma_i= \\sigma^2c_i^2+\\gamma_i^2(c_i\\delta_i -1)^2.\n\\end{align*}\n\\]\nDe manière évidente si \\(\\gamma_i^2\\) diminue, alors l’EQM de \\(\\hat \\gamma(c)_i\\) diminue aussi. Calculons la valeur de l’EQM quand \\(\\gamma_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\). Nous avons, grâce à la question précédente, \\[\n\\begin{eqnarray*}\n\\mathop{\\mathrm{EQM}}(\\hat \\gamma(c)_i)&=&\\sigma^2 c_i^2+(c_i\\delta_i -1)^2\\frac{\\sigma^2}{\\delta_i^2}\\frac{(1/\\delta_i)+c_i}{(1/\\delta_i)-c_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)^2\\frac{1+\\delta_ic_i}{1-\\delta_ic_i}\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1 - c_i\\delta_i)(1+\\delta_ic_i)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}(1-\\delta_i^2c_i^2)\\\\\n&=&\\sigma^2 c_i^2+\\frac{\\sigma^2}{\\delta_i^2}-\\sigma^2c_i^2=\\frac{\\sigma^2}{\\delta_i^2}\\\\\n&=&\\mathop{\\mathrm{EQM}}(\\hat \\gamma_{\\mathrm{MC}}),\n\\end{eqnarray*}\n\\] d’où la conclusion demandée.\nPar définition de \\(\\hat \\gamma(c)\\), nous avons \\[\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&\\mathop{\\mathrm{diag}}(c_i)Z_{1:p}=\\mathop{\\mathrm{diag}}(\\frac{\\delta_i}{\\delta_i^2+\\kappa})Z_{1:p}\\\\\n&=&(\\Delta'\\Delta + \\kappa I_p)^{-1}\\Delta'Z_{1:p},\n\\end{eqnarray*}\n\\] puisque \\(\\Delta\\) est diagonale. De plus nous avons \\[\nD =\n\\bigl( \\begin{smallmatrix}\n  \\Delta\\\\\n0\n\\end{smallmatrix}\\bigr),\n\\] ce qui entraîne que \\(D'D=\\Delta'\\Delta\\) et \\(D'Z=\\Delta' Z_{1:p}\\). Nous obtenons donc \\[\n\\hat \\gamma(c)=(D'D+\\kappa I_p)^{-1}D'Z.\n\\] Rappelons que \\(D=PXQ'\\) avec \\(P\\) et \\(Q\\) matrices orthogonales, nous avons alors \\[\n\\begin{eqnarray*}\n\\hat \\gamma(c)&=&(QX'P'PXQ' + \\kappa I_p)^{-1} D'Z=(QX'XQ' + \\kappa QQ')^{-1}D'Z\\\\\n&=&(Q(X'X  + \\kappa I_p)Q')^{-1}D'Z=(Q')^{-1}(X'X  + \\kappa I_p)^{-1}(Q)^{-1}D'Z\\\\\n&=&Q(X'X  + \\kappa I_p)^{-1}Q'D'Z.\n\\end{eqnarray*}\n\\] Comme \\(Z=PY\\) et \\(D=PXQ'\\), nous avons \\[\n\\hat \\gamma(c)=Q(X'X  + \\kappa I_p)^{-1}Q' QX'P' PY=Q(X'X  + \\kappa I_p)^{-1}XY.\n\\] Enfin, nous savons que \\(Q\\hat\\gamma=\\hat \\beta\\), nous en déduisons que \\(\\hat\\gamma=Q'\\hat \\beta\\) et donc que dans le cas particulier où \\(c_i=\\frac{\\delta_i}{\\delta_i^2+\\kappa}\\) nous obtenons \\[\n\\hat \\beta=Q\\hat \\gamma(c)=(X'X  + \\kappa I_p)^{-1}XY,\n\\] c’est-à-dire l’estimateur de la régression ridge.\n\n\n\nExercice 6 (Coefficient constant et régression sous contraintes)  \n\n\nExercice 7 (Unicité pour la régression lasso, Giraud (2014))  \n\n\nExercice 8  \n\n\nlibrary(tidyverse)\nsignal &lt;- read_csv(\"../donnees/courbe_lasso.csv\")\ndonnees &lt;- read_csv(\"../donnees/echan_lasso.csv\")\nggplot(signal)+aes(x=x,y=y)+geom_line()+\n  geom_point(data=donnees,aes(x=X,y=Y))\n\n\n\n\nNous cherchons à reconstruire le signal à partir de l’échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme \\[\ny_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\n\\] n’est pas approprié. De nombreuses approches en traitement du signal proposent d’utiliser une base ou dictionnaire représentée par une collection de fonctions \\(\\{\\psi_j(x)\\}_{j=1,\\dots,K}\\) et de décomposer le signal dans cette base : \\[\nm(x)\\approx \\sum_{j=1}^K \\beta_j\\psi_j(x).\n\\] Pour un dictionnaire donné, on peut alors considérer un modèle linéaire \\[\n  y_i=\\sum_{j=1}^K \\beta_j\\psi_j(x_i)+\\varepsilon_i.\n\\tag{1}\\] Le problème est toujours d’estimer les paramètres \\(\\beta_j\\) mais les variables sont maintenant définies par les éléments du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par \\[\n\\psi_0(x)=1,\\quad \\psi_{2j-1}(x)=\\cos(2j\\pi x)\\quad\\text{et}\\quad \\psi_{2j}(x)=\\sin(2j\\pi x),\\quad j=1,\\dots,K.\n\\]\n\nmat.dict &lt;- function(K,x){\n    res &lt;- matrix(0,nrow=length(x),ncol=2*K) |&gt; as_tibble()\n    for (j in 1:K){\n      res[,2*j-1] &lt;- cos(2*j*pi*x)\n      res[,2*j] &lt;- sin(2*j*pi*x)\n    }\n    return(res)\n}\n\nIl suffit d’ajuster le modèle linéaire où les variables explicatives sont données par le dictionnaire :\n\nD25 &lt;- mat.dict(25,donnees$X) |&gt; mutate(Y=donnees$Y)\nmod.lin &lt;- lm(Y~.,data=D25)\nS25 &lt;- mat.dict(25,signal$x)\nprev.MCO &lt;- predict(mod.lin,newdata = S25)\nsignal1 &lt;- signal |&gt; mutate(MCO=prev.MCO) |&gt; rename(signal=y)\nsignal2 &lt;- signal1 |&gt; pivot_longer(-x,names_to=\"meth\",values_to=\"y\")\nggplot(signal2)+aes(x=x,y=y)+geom_line(aes(color=meth))+\n  scale_y_continuous(limits = c(-2,2))+geom_point(data=donnees,aes(x=X,y=Y))\n\n\n\n\nLe signal estimé a tendance à surajuster les données. Cela vient du fait qu’on estime 51 paramètres avec seulement 60 observations.\nOn regarde tout d’abord le chemin de régularisation des estimateurs lasso\n\nlibrary(glmnet)\nX.25 &lt;- model.matrix(Y~.,data=D25)[,-1]\nlasso1 &lt;- glmnet(X.25,D25$Y,alpha=1)\nplot(lasso1)\n\n\n\n\nIl semble que quelques coefficients quittent la valeur 0 bien avant les autres. On effectue maintenant la validation croisée pour sélectionner le paramètre \\(\\lambda\\).\n\nset.seed(1234)\nlasso.cv &lt;- cv.glmnet(X.25,D25$Y,alpha=1)\nplot(lasso.cv)\n\n\n\n\nOn calcule les prévisions et on trace le signal.\n\nprev.lasso &lt;- as.vector(predict(lasso.cv,newx=as.matrix(S25)))\nsignal1$lasso &lt;- prev.lasso\nsignal2 &lt;- signal1 |&gt; pivot_longer(-x,names_to=\"meth\",values_to=\"y\")\nggplot(signal2)+aes(x=x,y=y)+geom_line(aes(color=meth))+\n  scale_y_continuous(limits = c(-2,2))+geom_point(data=donnees,aes(x=X,y=Y))\n\n\n\n\nL’algorithme lasso a permis de corriger le problème de sur-apprentissage. Les coefficients lasso non nuls sont les suivants\n\nv.sel &lt;- which(coef(lasso.cv)!=0)\nv.sel\n\n [1]  1  2  4  5  6  8 21 28 30 38 40"
  },
  {
    "objectID": "correction/chap9.html",
    "href": "correction/chap9.html",
    "title": "9 Régression sur composantes : PCR et PLS",
    "section": "",
    "text": "Exercice 1 (Questions de cours) A, B, C, A, B (les composantes sont orthogonales donc faire des régressions univariées \\(Y\\) contre chaque composante \\(X^{*}_{j}\\) ou faire la régression multiple \\(Y\\) contre toutes les composantes \\(X^{*}_{j}\\) revient au même pour des données centrées), A, C.\n\n\nExercice 2 (Régression sur composantes)  \n\n\nExercice 3 (Régression sur composantes)  \n\n\nExercice 4 (Régression sur composantes)  \n\n\nExercice 5 (Théorème 9.2) Elle s’effectue par récurrence. Nous allons ajouter à cette propriété un résultat intermédiaire qui constituera la première partie de la propriété: \\[\nX^{(j)}=X\\prod_{i=1}^{j-1}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X).\n\\] La seconde partie sera bien sûr de vérifier que \\(\\tilde{w}^{(j)}\\) s’écrit bien sous la forme annoncée.\nLa propriété pour \\(j=1\\) : la première partie n’a pas de sens et, concernant \\(\\tilde{w}^{(j)}\\), par construction \\(X=X^{(1)}\\) et donc \\(\\tilde{w}^{(1)}=w^{(1)}\\).\nLa propriété pour \\(j=2\\) est-elle vraie ?  Nous savons que par définition \\(X^{(2)}=P_{{t^{(1)}}^\\perp}X^{(1)}\\) et \\(X^{(1)}=X\\) donc \\[\n\\begin{eqnarray*}\nX^{(2)}&=&P_{{t^{(1)}}^\\perp}X^{(1)}=X-P_{t^{(1)}}X\n=X-t^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X\\\\\n&=&X(I-w^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X),\n\\end{eqnarray*}\n\\] car \\(t^{(1)}=Xw^{(1)}\\). Ceci démontre la première partie de la propriété. Ensuite, puisque \\(t^{(2)}=X^{(2)}w^{(2)}=X\\tilde{w}^{(2)}\\), en remplaçant \\(X^{(2)}\\) par \\(X(I-w^{(1)}({t^{(1)}}'t^{(1)})^{-1}{t^{(1)}}'X)\\) nous avons démontré la propriété pour le rang \\(j=2\\).\nSupposons maintenant la propriété vraie au rang \\((j-1)\\). Nous avons par définition : \\(X^{(j)}=P_{{t^{(j-1)}}^\\perp}X^{(j-1)}\\) donc \\(X^{(j)}=X^{(j-1)}-P_{t^{(j-1)}}X^{(j-1)}\\). Or par construction les \\(\\{t^{(k)}\\}_{k=1}^j\\) sont toutes orthogonales donc \\(P_{t^{(j-1)}}X^{(j-1)}=P_{t^{(j-1)}}X\\). Nous avons, grâce à la propriété vraie pour le rang \\((j-1)\\), que\n\\[\n\\begin{split}\nX^{(j)}&=X^{(j-1)}-t^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X\\\\\n&=X^{(j-1)}-X^{(j-1)}w^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X\\\\\n&=X\\prod_{i=1}^{j-2}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X)\n(I-w^{(j-1)}({t^{(j-1)}}'t^{(j-1)})^{-1}{t^{(j-1)}}'X)\n\\end{split}\n\\] démontrant la première partie de la proposition. Ensuite, puisque \\(t^{(j)}=X^{(j)}w^{(j)}=X\\tilde{w}^{(j)}\\), en remplaçant \\(X^{(j)}\\) par \\(X\\prod_{i=1}^{j-1}(I-w^{(i)}({t^{(i)}}'t^{(i)})^{-1}{t^{(i)}}'X)\\), nous avons démontré la propriété pour le rang \\(j\\).\n\n\nExercice 6 (Géométrie des estimateurs)  \n\nLes quatre premières réponses sont évidentes, les coordonnées de \\(\\hat Y\\) valent \\(1.5,0.5\\) et \\(0\\). Ici \\(p\\) vaut 2 et \\(B_1\\) est un cercle de centre \\(O\\) de rayon 1, alors que \\(B_2\\) est un losange.\nIntuitivement, l’image d’un cercle par une application linéaire est une ellipse et l’image d’un losange est un parallélogramme.\nLe dessin suivant représente les ensembles \\(C_1\\) et \\(C_2\\) et \\(\\hat Y\\) grâce aux ordres R suivants :\n\nX &lt;- matrix(c(1,0,0,1/sqrt(3),2/sqrt(3),0),3,2)\nsss &lt;- 1\niter &lt;- 1\ncoord &lt;- matrix(0,500,2)\nfor (tt in  seq(-pi,pi,length=500)) {\n   coord[iter,] &lt;- (X%*%as.matrix(sqrt(sss)\n                         *c(cos(tt),sin(tt))))[1:2,]\n   iter &lt;- iter+1\n }\niter &lt;- 1\ncoord2 &lt;- matrix(0,500,2)\nfor (tt in  seq(-1,1,length=250)) {\n   coord2[iter,] &lt;- (X%*%as.matrix(c(tt,1-abs(tt))))[1:2,]\n   coord2[iter+250,] &lt;- (X%*%as.matrix(c(tt,\n                                abs(tt)-1)))[1:2,]\n   iter &lt;- iter+1\n }\nplot(coord,type=\"l\",xlab=\"\",ylab=\"\")\nlines(coord2)\n\n\n\n\n\n\nPar définition, \\(X\\hat \\beta_{\\mathrm{ridge}}\\) est l’élément de \\(C_1\\) le plus proche de \\(\\hat Y\\). De même, \\(X\\hat \\beta_{\\mathrm{lasso}}\\) est l’élément de \\(C_2\\) le plus proche de \\(\\hat Y\\). Cela donne graphiquement \nL’ensemble \\(C_1\\), composé de vecteurs de la forme \\(u=X_1\\alpha_1+X_2\\alpha_2\\) avec la norme du vecteur \\(\\alpha\\) valant 1, peut être vu comme l’ensemble des composantes dans lequel on va choisir la composante PLS. La première composante PLS est le vecteur de \\(C_1\\) dont le produit scalaire avec \\(Y\\) (et donc \\(\\hat Y\\)) est le plus grand. Graphiquement, c’est le vecteur de \\(C_1\\) dont l’extrémité sur l’ellipse est le pied de la tangente à l’ellipse perpendiculaire à \\(O\\hat Y\\). La prévision de \\(Y\\) par la régression PLS est la projection de \\(Y\\) et donc de \\(\\hat Y\\) sur la composante PLS.\nLa calcul donne simplement \\[\n\\begin{eqnarray*}\nX'X = \\begin{pmatrix}\n1&\\sqrt{3}/3\\\\\n\\sqrt{3}/3&5/3\n\\end{pmatrix}.\n\\end{eqnarray*}\n\\] Les valeurs propres sont 2 et 2/3. Le premier axe principal correspond au vecteur propre associé à la valeur 2. Pour trouver la première composante principale, il faut pré-multiplier ce vecteur par \\(X\\). Cela donne le vecteur de coordonnées \\((1,1,0)'\\). Les commandes GNU-R sont\n\nX &lt;- matrix(c(1,0,0,1/sqrt(3),2/sqrt(3),0),3,2)\nsvd &lt;- eigen(t(X)%*%X)\nX%*%svd$vect[,1]\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    0\n\n\nLa prévision de \\(Y\\) par la régression PCR est la projection de \\(Y\\) (et donc de \\(\\hat Y\\)) sur la composante PCR. Dans cet exemple, la projection de \\(\\hat Y\\) sur la composante PCR est un point de l’ellipse, mais cela est uniquement dû aux données de cet exercice. Le graphique suivant représente les 4 projections :\n\n\n\n\n\n\n\n\nExercice 7 (Orthonormalisation)"
  },
  {
    "objectID": "correction/chap11.html",
    "href": "correction/chap11.html",
    "title": "11 Régression logistique",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nA\nA\nB\nA\nA\nA\nB\nA\n\n\n\nExercice 2 (Interprétation des coefficients)  \n\nOn génère l’échantillon.\n\nn &lt;- 100\nset.seed(48967365)\nX &lt;- sample(c(\"A\",\"B\",\"C\"),100,replace=TRUE)\nY &lt;- rep(0,n)\nset.seed(487365)\nY[X==\"A\"] &lt;- rbinom(sum(X==\"A\"),size=1,prob=0.95)\nset.seed(4878365)\nY[X==\"B\"] &lt;- rbinom(sum(X==\"B\"),size=1,prob=0.95)\nset.seed(4653965)\nY[X==\"C\"] &lt;- rbinom(sum(X==\"C\"),size=1,prob=0.05)\nY &lt;- factor(Y)\ndonnees&lt;-data.frame(Y,X)\n\nOn ajuste le modèle avec les contraintes par défaut.\n\nmodel1 &lt;- glm(Y~X,data=donnees,family=binomial)\nsummary(model1)\n\n\nCall:\nglm(formula = Y ~ X, family = binomial, data = donnees)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.2336     0.6075   3.677 0.000236 ***\nXB            0.6568     0.9470   0.694 0.487977    \nXC           -5.6348     1.1842  -4.758 1.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.489  on 99  degrees of freedom\nResidual deviance:  44.218  on 97  degrees of freedom\nAIC: 50.218\n\nNumber of Fisher Scoring iterations: 6\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_2\\) et \\(\\beta_3\\).\nOn change la modalité de référence.\n\nmodel2 &lt;- glm(Y~C(X,base=3),data=donnees,family=binomial)\nsummary(model2)\n\n\nCall:\nglm(formula = Y ~ C(X, base = 3), family = binomial, data = donnees)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -3.401      1.017  -3.346  0.00082 ***\nC(X, base = 3)A    5.635      1.184   4.758 1.95e-06 ***\nC(X, base = 3)B    6.292      1.249   5.035 4.77e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 129.489  on 99  degrees of freedom\nResidual deviance:  44.218  on 97  degrees of freedom\nAIC: 50.218\n\nNumber of Fisher Scoring iterations: 6\n\n\nOn obtient les résultats du test de Wald sur la nullité des paramètres \\(\\beta_0,\\beta_1\\) et \\(\\beta_2\\).\nOn remarque que dans model1 on accepte la nullité de \\(\\beta_2\\) alors qu’on la rejette dans model2. Ceci est logique dans la mesure où ces tests dépendent de la contrainte identifiante choisie. Dans model1 le test de nullité de \\(\\beta_2\\) permet de vérifier si \\(B\\) à un effet similaire à \\(A\\) sur \\(Y\\). Dans model2, on compare l’effet de \\(B\\) à celui de \\(C\\). On peut donc conclure \\(A\\) et \\(B\\) ont des effets proches sur \\(Y\\) alors que \\(B\\) et \\(C\\) ont un impact différent. Ceci est logique vu la façon dont les données ont été générées.\nTester l’effet global de \\(X\\) sur \\(Y\\) revient à tester si les coefficients \\(\\beta_1,\\beta_2\\) et \\(\\beta_3\\) sont égaux, ce qui, compte tenu des contraintes revient à considérer les hypothèses nulles :\n\n\\(\\beta_2=\\beta_3=0\\) dans model1 ;\n\\(\\beta_1=\\beta_2=0\\) dans model2.\n\nOn peut effectuer les tests de Wald ou du rapport de vraisemblance. On obtient les résultats du rapport de vraisemblance avec :\n\nlibrary(car)\nAnova(model1,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n  LR Chisq Df Pr(&gt;Chisq)    \nX   85.271  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(model2,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n               LR Chisq Df Pr(&gt;Chisq)    \nC(X, base = 3)   85.271  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn remarque ici que ces deux tests sont identiques : ils ne dépendent pas de la contrainte identifiante choisie.\n\n\n\nExercice 3 (Séparabilité)  \n\nOn génère l’échantillon demandé.\n\nset.seed(1234)\nX &lt;- c(runif(50,-1,0),runif(50,0,1))\nset.seed(5678)\nY &lt;- c(rep(0,50),rep(1,50))\ndf &lt;- data.frame(X,Y)\n\nLe graphe s’obtient avec :\n\nbeta &lt;- seq(0,100,by=0.01)\nlog_vrais &lt;- function(X,Y,beta){\n  LV &lt;- rep(0,length(beta))\n  for (i in 1:length(beta)){\n    Pbeta &lt;- exp(beta[i]*X)/(1+exp(beta[i]*X))\n    LV[i] &lt;- sum(Y*X*beta[i]-log(1+exp(X*beta[i])))\n#    gradln[i] &lt;- t(Xb)%*%(Yb-Pbeta)\n  }\n  return(LV)\n}\nLL &lt;- log_vrais(X,Y,beta)\nplot(beta,LL,type=\"l\")\n\n\n\n\nOn obtient un avertissement qui nous dit que l’algorithme d’optimisation n’a pas convergé.\n\nmodel &lt;- glm(Y~X-1,data=df,family=\"binomial\")\nmodel$coef\n\n       X \n1999.371 \n\n\nLe changement proposé supprime la séparabilité des données. On obtient bien un maximum fini pour cette nouvelle vraisemblance.\n\nY1 &lt;- Y;Y1[1] &lt;- 1\nLL1 &lt;- log_vrais(X,Y1,beta)\nplot(beta,LL1,type=\"l\")\n\n\n\nmodel1 &lt;- glm(Y1~X-1,family=\"binomial\")\nmodel1$coef\n\n       X \n10.17868 \n\n\n\n\n\nExercice 4 (Matrice hessienne) Le gradient de la log-vraisemblance en \\(\\beta\\) est donné par \\(\\nabla \\mathcal L(Y,\\beta)=X'(Y-P_\\beta)\\). Sa \\(j\\)ème composante vaut \\[\\frac{\\partial\\mathcal L}{\\partial\\beta_j}(\\beta)=\\sum_{i=1}^nx_{ij}(y_i-p_\\beta(x_i)).\\]\nOn peut donc calculer la drivée par rapport à \\(\\beta_\\ell\\) : \\[\\begin{align*}\n\\frac{\\partial\\mathcal L}{\\partial\\beta_j\\partial\\beta_\\ell}(\\beta)= & \\frac{\\partial}{\\partial\\beta_\\ell}\\left[\n\\sum_{i=1}^nx_{ij}\\left(y_i-\\frac{\\exp(x_i'\\beta)}{1+\\exp(x_i'\\beta)}\\right)\\right] \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}\\frac{\\exp(x_i'\\beta)}{[1+\\exp(x_i'\\beta)]^2} \\\\\n=& -\\sum_{i=1}^nx_{ij}x_{i\\ell}p_\\beta(x_i)(1-p_\\beta(x_i)).\n\\end{align*}\\] Matriciellement on déduit donc que la hessienne vaut \\[\\nabla^2\\mathcal L(Y,\\beta)=-X'W_\\beta X,\\] où \\(W_\\beta\\) est la matrice \\(n\\times n\\) diagonale dont le \\(i\\)ème terme de la diagonale vaut \\(p_\\beta(x_i)(1-p_\\beta(x_i))\\). Par ailleurs, comme pour tout \\(i=1,\\dots,n\\), on a \\(p_\\beta(x_i)(1-p_\\beta(x_i))&gt;0\\) et que \\(X\\) est de plein rang, on déduit que \\(X'W_\\beta X\\) est définie positive et par conséquent que la hessienne est définie négative.\n\n\nExercice 5 (Modèles avec R) On importe les données\n\npanne &lt;- read.table(\"../donnees/panne.txt\",header=T)\nhead(panne)\n\n  etat age marque\n1    0   4      A\n2    0   2      C\n3    0   3      C\n4    0   9      B\n5    0   7      B\n6    0   6      A\n\n\n\nLa commande\n\nmodel &lt;- glm(etat~.,data=panne,family=binomial)\nmodel\n\n\nCall:  glm(formula = etat ~ ., family = binomial, data = panne)\n\nCoefficients:\n(Intercept)          age      marqueB      marqueC  \n    0.47808      0.01388     -0.41941     -1.45608  \n\nDegrees of Freedom: 32 Total (i.e. Null);  29 Residual\nNull Deviance:      45.72 \nResidual Deviance: 43.5     AIC: 51.5\n\n\najuste le modèle \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x_1+\\beta_2\\mathsf{1}_{x_2=B}+\\beta_3\\mathsf{1}_{x_2=C}\\] où \\(x_1\\) et \\(x_2\\) désigne respectivement les variables age et marque. On obtient les estimateurs avec\n\ncoef(model)\n\n(Intercept)         age     marqueB     marqueC \n 0.47808311  0.01388395 -0.41941071 -1.45608147 \n\n\nIl s’agit des tests de Wald pour tester l’effet des variables dans le modèle. Pour l’effet de marque, on va par exemple tester \\[H_0:\\beta_2=\\beta_3=0\\quad\\text{contre}\\quad H_1:\\beta_2\\neq 0\\text{ ou }\\beta_3\\neq 0.\\] Sous \\(H_0\\) la statistique de Wald suit une loi du \\(\\chi^2\\) à 4-2=2 degrés de liberté. Pour le test de la variable age le nombre de degrés de liberté manquant est 1. On retrouve cela dans la sortie\n\nlibrary(car)\nAnova(model,type=3,test.statistic=\"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: etat\n            Df  Chisq Pr(&gt;Chisq)\n(Intercept)  1 0.3294     0.5660\nage          1 0.0218     0.8826\nmarque       2 1.9307     0.3809\n\n\nIl s’agit cette fois du test du rapport de vraisemblance. Les degrés de liberté manquants sont identiques.\n\nAnova(model,type=3,test.statistic=\"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: etat\n       LR Chisq Df Pr(&gt;Chisq)\nage     0.02189  1     0.8824\nmarque  2.09562  2     0.3507\n\n\n\nLe modèle s’écrit \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1\\mathsf{1}_{x_2=A}+\\beta_2\\mathsf{1}_{x_2=B}.\\]\nLe modèle ajusté ici est \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\gamma_0+\\gamma_1\\mathsf{1}_{x_2=B}+\\gamma_2\\mathsf{1}_{x_2=C}.\\] Par identification on a \\[\\begin{cases}\n\\beta_0+\\beta_1=\\gamma_0 \\\\\n\\beta_0+\\beta_2=\\gamma_0+\\gamma_1 \\\\\n\\beta_0=\\gamma_0+\\gamma_2 \\\\\n\\end{cases}\n\\Longleftrightarrow\n\\begin{cases}\n\\beta_0=\\gamma_0+\\gamma_2 \\\\\n\\beta_1=-\\gamma_2 \\\\\n\\beta_2=\\gamma_1-\\gamma_2 \\\\\n\\end{cases}\n\\Longrightarrow\n\\begin{cases}\n\\widehat\\beta_0=-0.92 \\\\\n\\widehat\\beta_1=1.48 \\\\\n\\widehat\\beta_2=1.05 \\\\\n\\end{cases}\\] On peut retrouver ces résultats avec\n\nglm(etat~C(marque,base=3),data=panne,family=\"binomial\")\n\n\nCall:  glm(formula = etat ~ C(marque, base = 3), family = \"binomial\", \n    data = panne)\n\nCoefficients:\n         (Intercept)  C(marque, base = 3)A  C(marque, base = 3)B  \n             -0.9163                1.4759                1.0498  \n\nDegrees of Freedom: 32 Total (i.e. Null);  30 Residual\nNull Deviance:      45.72 \nResidual Deviance: 43.52    AIC: 49.52\n\n\n\nIl y a interaction si l’age agit différemment sur la panne en fonction de la marque.\nLe modèle ajusté sur R est \\[\\log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\delta_0+\\delta_1\\mathsf{1}_{x_2=B}+\\delta_2\\mathsf{1}_{x_2=C}+\\delta_3x_1+\\delta_4x_1\\mathsf{1}_{x_2=B}+\\delta_5x_1\\mathsf{1}_{x_2=C}.\\] On obtient ainsi par identification : \\[\\begin{cases}\n\\alpha_0=\\delta_0\\\\\n\\alpha_1=\\delta_3\\\\\n\\beta_0=\\delta_0+\\delta_1\\\\\n\\beta_1=\\delta_3+\\delta_4\\\\\n\\gamma_0=\\delta_0+\\delta_2\\\\\n\\gamma_1=\\delta_3+\\delta_5\n\\end{cases}\\] On peut ainsi en déduire les valeurs des estimateurs que l’on peut retrouver avec la commande :\n\nglm(etat~-1+marque+marque:age,data=panne,family=\"binomial\")\n\n\nCall:  glm(formula = etat ~ -1 + marque + marque:age, family = \"binomial\", \n    data = panne)\n\nCoefficients:\n    marqueA      marqueB      marqueC  marqueA:age  marqueB:age  marqueC:age  \n    0.23512      0.43375     -2.19633      0.05641     -0.05547      0.27228  \n\nDegrees of Freedom: 33 Total (i.e. Null);  27 Residual\nNull Deviance:      45.75 \nResidual Deviance: 42.62    AIC: 54.62\n\n\n\n\n\nExercice 6 (Interprétation)  \n\ndf &lt;- read.csv(\"../donnees/logit_ex6.csv\")\nmod &lt;- glm(Y~.,data=df,family=binomial)\nmod1 &lt;- glm(Y~X1,data=df,family=binomial)\nsummary(mod)\n\n\nCall:\nglm(formula = Y ~ ., family = binomial, data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.57866    0.11926  -4.852 1.22e-06 ***\nX1          -0.19471    0.06556  -2.970  0.00298 ** \nX2           0.31899    0.04404   7.244 4.36e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 499  degrees of freedom\nResidual deviance: 618.26  on 497  degrees of freedom\nAIC: 624.26\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(mod1)\n\n\nCall:\nglm(formula = Y ~ X1, family = binomial, data = df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  0.001092   0.089499   0.012    0.990\nX1          -0.020467   0.051733  -0.396    0.692\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 693.15  on 499  degrees of freedom\nResidual deviance: 692.99  on 498  degrees of freedom\nAIC: 696.99\n\nNumber of Fisher Scoring iterations: 3\n\n\nOn remarque que la nullité du paramètre associé à X1 est accepté dans le modèle avec uniquement X1 alors qu’elle est refusée lorsqu’on considère X1 et X2 dans le modèle.\n\n\nExercice 7 (Tests à la main)  \n\nLe modèle s’écrit \\[log\\left(\\frac{p_\\beta(x)}{1-p_\\beta(x)}\\right)=\\beta_0+\\beta_1x.\\]\nLa log vraisemblance s’obtient avec\n\np &lt;- c(0.76,0.4,0.6,0.89,0.35)\nY &lt;- c(1,0,0,1,1)\nL1 &lt;- log(prod(p^Y*(1-p)^(1-Y)))\nL1\n\n[1] -2.867909\n\n\n\nOn calcule les écart-type des estimateurs\n\nX1 &lt;- c(0.47,-0.55,-0.01,1.07,-0.71)\nX &lt;- matrix(c(rep(1,5),X1),ncol=2)\nW &lt;- diag(p*(1-p))\nSIG &lt;- solve(t(X)%*%W%*%X)\nsig &lt;- sqrt(diag(SIG))\nsig\n\n[1] 1.023252 1.744935\n\n\nOn en déduit les statistiques de test :\n\nbeta &lt;- c(0.4383,1.5063)\nbeta/sig\n\n[1] 0.4283401 0.8632411\n\n\nOn peut faire le test de Wald et du rapport de vraisemblance.\nLa statistique de test vaut 0.8632411, on obtient donc la probabilité critique\n\n2*(1-pnorm(0.8632411))\n\n[1] 0.3880049\n\n\nOn peut également effectuer un test du rapport de vraisemblance. Le modèle null sans X1 a pour log-vraisemblance\n\np0 &lt;- 3/5\nL0 &lt;- log(prod(p0^Y*(1-p0)^(1-Y)))\nL0\n\n[1] -3.365058\n\n\nLa statistique de test vaut donc\n\n2*(L1-L0)\n\n[1] 0.9942984\n\n\net la probabilité critique vaut\n\n1-pchisq(2*(L1-L0),df=1)\n\n[1] 0.3186941\n\n\nOn peut retrouver (aux arrondis près) les résultats de l’exercice avec\n\nX &lt;- c(0.47,-0.55,-0.01,1.07,-0.71)\nY &lt;- c(1,0,0,1,1)\ndf &lt;- data.frame(X,Y)\nmodel &lt;- glm(Y~X,data=df,family=\"binomial\")\nlogLik(model)\n\n'log Lik.' -2.898765 (df=2)\n\nAnova(model,type=3,test.statistic = \"Wald\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n            Df  Chisq Pr(&gt;Chisq)\n(Intercept)  1 0.1846     0.6675\nX            1 0.7552     0.3848\n\nAnova(model,type=3,test.statistic = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Y\n  LR Chisq Df Pr(&gt;Chisq)\nX  0.93259  1     0.3342\n\n\n\n\n\n\nExercice 8 (Vraisemblance du modèle saturé)  \n\nLes variables \\((y_t,t=1,\\dots,y_T)\\) étant indépendantes et de loi binomiales \\(B(n_t,p_t)\\), la log-vraisemblance est donnée par \\[\\begin{align*}\n\\mathcal L_{\\text{sat}}(Y,p)= & \\log\\left(\\prod_{t=1}^T\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\np_t^{\\tilde y_t}(1-p_t)^{n_t-\\tilde y_t}\\right) \\\\\n= &\n\\sum_{t=1}^T\\left(\\log\n\\begin{pmatrix}\nn_t\\\\\n\\tilde y_t\n\\end{pmatrix}\n+\\tilde y_t\\log(p_t)+(n_t-\\tilde y_t)\\log(1-p_t)\\right)\n\\end{align*}\\]\nLa dérivée de la log-vraisemblance par rapport à \\(p_t\\) s’écrit \\[\\frac{\\tilde y_t}{p_t}-\\frac{n_t-\\tilde y_t}{1-p_t}.\\] Cette dérivée s’annule pour \\[\\widehat p_t=\\frac{\\tilde y_t}{n_t}.\\]\nOn note \\(\\widehat \\beta\\) l’EMV du modèle logistique et \\(p_{\\widehat\\beta}\\) le vecteur qui contient les valeurs ajustées \\(p_{\\widehat\\beta}(x_t),t=1,\\dots,T\\). On a pour tout \\(\\beta\\in\\mathbb R^p\\) : \\[\\mathcal L(Y,\\beta)\\leq\\mathcal L(Y,\\widehat\\beta)=\\mathcal L_{\\text{sat}}(Y,p_{\\widehat\\beta})\\leq L_{\\text{sat}}(Y,\\widehat p_t).\\]\n\n\n\nExercice 9 (Intervalle de confiance profilé)  \n\n\nartere &lt;- read.table(\"../donnees/artere.txt\",header=T)\nmodele &lt;- glm(chd~age,data=artere,family=binomial)\nB0 &lt;- coef(modele)\nOriginalDeviance &lt;- modele$deviance\n\n\nalpha &lt;- 0.05    \n\n\nstderr &lt;- summary(modele)$coefficients[, \"Std. Error\"]\ndelta &lt;- sqrt(qchisq((1-alpha/4),df=1))* stderr[2] /5\ngrille &lt;- B0[2]+(-10):10*delta\n\nOn a \\[\\begin{align*}\n\\mathcal D_1&=-2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L_{sat})\n  \\end{align*}\\] Pour celle avec l’offset \\(K_i=x_i\\beta_2^*\\) elle vaut \\[\\begin{align*}\n\\mathcal D_o&=-2(\\mathcal L(Y,K,\\hat\\beta_1)-\\mathcal L_{sat})\n  \\end{align*}\\] où \\(\\hat \\beta_1\\) maximise \\(\\mathcal L(Y,K,\\hat\\beta_1)\\) c’est à dire \\(\\mathcal L(Y,K,\\hat\\beta_1)=l(\\beta_2^*)\\) et nous avons donc \\[\\begin{align*}\n\\mathcal D_o - \\mathcal D_1= 2(\\mathcal L(Y,\\hat\\beta)-\\mathcal L(Y,K,\\beta_1)= 2(\\mathcal L(Y,\\hat\\beta)-l(\\beta_2^*))=P(\\beta_2^*).\n  \\end{align*}\\]\n\nprofil2 &lt;- rep(0,length(grille))\nfor (k in 1:length(grille)) {\n  modeleo &lt;- glm(chd~1,family=binomial,offset=artere[,\"age\"]*grille[k],data=artere)\n  profil2[k] &lt;- modeleo$deviance - OriginalDeviance\n}\n\n\nprofil &lt;- sign(-10:10)*sqrt(profil2)\n\n\nspline(x=profil,y=grille,xout=c(-sqrt(qchisq(1-alpha,1)),sqrt(qchisq(1-alpha,1))))$y\n\n[1] 0.0669275 0.1620014\n\n\n\nconfint(modele)\n\n                  2.5 %     97.5 %\n(Intercept) -7.72587162 -3.2461547\nage          0.06693158  0.1620067"
  },
  {
    "objectID": "correction/chap12.html",
    "href": "correction/chap12.html",
    "title": "12 Régression de Poisson",
    "section": "",
    "text": "Exercice 1 (Questions de cours) C, A, B, A, B, B, C, A\n\n\nExercice 2  \n\n\nExercice 3  \n\n\nExercice 4 (Stabilisation de la variance)  \n\n\nExercice 5 (Stabilisation de la variance)  \n\n\nExercice 6  \n\n\nExercice 7 (Table de contingence et loi de Poisson)  \n\n\nExercice 8 (Table de contingence et probabilité)  \n\n\nExercice 9 (Loi Multinomiale)  \n\n\nExercice 10"
  },
  {
    "objectID": "correction/chap13.html",
    "href": "correction/chap13.html",
    "title": "13 Régularisation de la vraisemblance",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nA, B\nC\nA\nC, D\n\n\n\nExercice 2 (Lasso sur des données centrées réduites)  \n\nlibrary(bestglm)\ndata(SAheart)\nSAheart.X &lt;- model.matrix(chd~.,data=SAheart)[,-1]\nSAheart.Y &lt;- SAheart$chd \nlibrary(glmnet)\nmod.lasso &lt;- glmnet(SAheart.X,SAheart.Y,family=\"binomial\",alpha=1)\n\n\n\nlam.lasso &lt;- mod.lasso$lambda\nlam &lt;- lam.lasso[50]\ncoef(mod.lasso,s=lam)\n\n10 x 1 sparse Matrix of class \"dgCMatrix\"\n                         s1\n(Intercept)    -6.105111733\nsbp             0.006097928\ntobacco         0.077763845\nldl             0.169776425\nadiposity       0.012176169\nfamhistPresent  0.902337702\ntypea           0.037527467\nobesity        -0.050604210\nalcohol         .          \nage             0.045476625\n\n\n\nmu &lt;- apply(SAheart.X,2,mean)\nsig &lt;- apply(SAheart.X,2,sd)\nmu.mat &lt;- matrix(rep(mu,nrow(SAheart.X)),nrow=nrow(SAheart.X),byrow=T)\nsig.mat &lt;- matrix(rep(sig,nrow(SAheart.X)),nrow=nrow(SAheart.X),byrow=T)\nSAheart.X.cr &lt;- (SAheart.X-mu.mat)/sig.mat\nmod.lasso1 &lt;- glmnet(SAheart.X.cr,SAheart.Y,family=\"binomial\",alpha=1)\n\n\nlam1 &lt;- mod.lasso1$lambda[50]\ncoef(mod.lasso1,s=lam1)[-1]/sig\n\n           sbp        tobacco            ldl      adiposity famhistPresent \n   0.006097928    0.077763845    0.169776425    0.012176169    0.902337702 \n         typea        obesity        alcohol            age \n   0.037527467   -0.050604210    0.000000000    0.045476625 \n\n\n\n\n\nExercice 3 (Comparaison de méthodes et courbes ROC)  \n\nOn importe les données et on les sépare en un échantillon d’apprentissage et de test.\n\ndf &lt;- read.csv(\"../donnees/logit_ridge_lasso.csv\")\nset.seed(1254)\nperm &lt;- sample(nrow(df))\ndapp &lt;- df[perm[1:300],]\ndtest &lt;- df[perm[301:500],]\n\nOn construit les modèles demandés sur les données d’apprentissage uniquement.\n\nlogit &lt;- glm(Y~.,data=dapp,family=\"binomial\")\nlogit.step &lt;- step(logit,direction=\"backward\",trace=0)\n\n\nXapp &lt;- model.matrix(Y~.,data=dapp)[,-1]\nXtest &lt;- model.matrix(Y~.,data=dtest)[,-1]\nYapp &lt;- dapp$Y\nYtest &lt;- dtest$Y\n\n\nlasso1 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=1)\nridge1 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=0,lambda=exp(seq(-6,-1,length=100)))\nlasso2 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=1,type.measure = \"auc\")\nridge2 &lt;- cv.glmnet(Xapp,Yapp,family=\"binomial\",alpha=0,type.measure = \"auc\",lambda=exp(seq(-3,2,length=100)))\n\n\nlibrary(tidyverse)\nscore &lt;- data.frame(logit=predict(logit,newdata=dtest,type=\"response\"),\n                    step=predict(logit.step,newdata=dtest,type=\"response\"),\n                    lasso1=as.vector(predict(lasso1,type=\"response\",newx=Xtest)),\n                    ridge1=as.vector(predict(ridge1,type=\"response\",newx=Xtest)),\n                    lasso2=as.vector(predict(lasso2,type=\"response\",newx=Xtest)),\n                    ridge2=as.vector(predict(ridge2,type=\"response\",newx=Xtest))) %&gt;% \n  mutate(obs=Ytest) %&gt;% gather(key=\"Methode\",value=\"score\",-obs)\n\n\nlibrary(plotROC)\nggplot(score)+aes(m=score,d=obs,color=Methode)+geom_roc()+theme_classic()\n\n\n\n\n\nlibrary(tidymodels)\nscore %&gt;% \n  group_by(Methode) %&gt;% \n  mutate(obs=as.factor(obs)) %&gt;%\n  roc_auc(obs,score,event_level = \"second\") %&gt;%\n  select(Methode,.estimate) %&gt;%\n  arrange(desc(.estimate))\n\n# A tibble: 6 × 2\n  Methode .estimate\n  &lt;chr&gt;       &lt;dbl&gt;\n1 lasso1      0.945\n2 lasso2      0.944\n3 ridge2      0.883\n4 ridge1      0.880\n5 logit       0.816\n6 step        0.772\n\n\n\n\n\nExercice 4 (Surapprentissage)  \n\n\nscore.app &lt;- data.frame(logit=predict(logit,newdata=dapp,type=\"response\"),\n                    step=predict(logit.step,newdata=dapp,type=\"response\"),\n                    lasso1=as.vector(predict(lasso1,type=\"response\",newx=Xapp)),\n                    ridge1=as.vector(predict(ridge1,type=\"response\",newx=Xapp)),\n                    lasso2=as.vector(predict(lasso2,type=\"response\",newx=Xapp)),\n                    ridge2=as.vector(predict(ridge2,type=\"response\",newx=Xapp))) %&gt;% \n  mutate(obs=Yapp) %&gt;% gather(key=\"Methode\",value=\"score\",-obs) \n\nOn prédit 1 si la probabilité que Y soit égale à 1 est supérieure ou égale à 0.5 :\n\nprev.app &lt;- score.app %&gt;% mutate(prev=round(score))\nprev.app %&gt;% group_by(Methode) %&gt;% summarise(Err=mean(prev!=obs)) %&gt;% arrange(Err)\n\n# A tibble: 6 × 2\n  Methode    Err\n  &lt;chr&gt;    &lt;dbl&gt;\n1 logit   0     \n2 step    0     \n3 ridge1  0.0567\n4 ridge2  0.0767\n5 lasso1  0.0967\n6 lasso2  0.11  \n\n\nOn fait de même avec l’échantillon test.\n\nprev.test &lt;- score %&gt;% mutate(prev=round(score))\nprev.test %&gt;% group_by(Methode) %&gt;% summarise(Err=mean(prev!=obs)) %&gt;% arrange(Err)\n\n# A tibble: 6 × 2\n  Methode   Err\n  &lt;chr&gt;   &lt;dbl&gt;\n1 lasso1  0.12 \n2 lasso2  0.13 \n3 ridge1  0.215\n4 ridge2  0.215\n5 step    0.23 \n6 logit   0.24 \n\n\nSur les données d’apprentissage ce sont les modèles logistiques complets et construits avec step qui ont les plus petites erreurs. Ces modèles souffrent de sur-apprentissage : ils ajustent très bien les données d’apprentissage mais ont du mal à bien prédire de nouveaux individus."
  },
  {
    "objectID": "correction/chap14.html",
    "href": "correction/chap14.html",
    "title": "14 Comparaison en classification supervisée",
    "section": "",
    "text": "Exercice 1 (Questions de cours) C, D, A, B.\n\n\nExercice 2 (Règle de Bayes)  \n\n\nLa règle de Bayes est définie par \\[\ng^\\star(x)=\\left\\{\n\\begin{array}{ll}\n1& \\text{si }\\mathbf P(Y=1|X=x)\\geq 0.5 \\\\\n0& \\text{sinon.}\n\\end{array}\\right.\n\\] L’erreur de Bayes est définie par \\(L^\\star=\\mathbf P(g^\\star(X)\\neq Y)\\).\nOn a \\[\n\\begin{aligned}\n  \\mathbf P(g(X)\\neq Y|X=x) & = 1-\\Big(\\mathbf P(g(X)=Y,g(X)=1|X=x) \\\\\n  & \\hspace{2cm}+\\mathbf P(g(X)=Y,g(X)=0|X=x)\\Big) \\\\\n  & = 1-\\Big(\\mathbf 1_{g(x)=1}\\mathbf P(Y=1|X=x) \\\\\n  & \\hspace{2cm}+\\mathbf 1_{g(x)=0}\\mathbf P(Y=0|X=x)\\Big) \\\\\n  & = 1-(\\mathbf 1_{g(x)=1}\\eta(x)+\\mathbf 1_{g(x)=0}(1-\\eta(x))).\n\\end{aligned}\n\\]\nOn déduit \\[\n\\begin{aligned}\n  \\mathbf P(&  g(X)\\neq Y|X=x)-\\mathbf P(g^\\star(X)\\neq Y|X=x) \\\\\n  &  = \\eta(x)\\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right)+(1-\\eta(x))\\left(\\mathbf 1_{g^\\star(x)=0}-\\mathbf 1_{g(x)=0}\\right) \\\\\n  &  = (2\\eta(x)-1) \\left(\\mathbf 1_{g^\\star(x)=1}-\\mathbf 1_{g(x)=1}\\right) \\\\\n  &  \\geq 0\n\\end{aligned}\n\\] par définition de \\(g^\\star\\).\nOn conclut en intégrant par la loi de \\(X\\) que \\[\\mathbf P(g(X)\\neq Y)\\geq \\mathbf P(g^\\star(X)\\neq Y).\\]\n\nPour la règle de Bayes, on a\n\nsi \\(x\\leq 0\\), on a \\(\\mathbf P(Y=1|X=x)=\\mathbf P(U\\leq 2)=\\frac{1}{5}\\) \\(\\Longrightarrow\\) \\(g^\\star(x)=0\\).\nsi \\(x&gt; 0\\), on a \\(\\mathbf P(Y=1|X=x)=\\mathbf P(U&gt; 2)=\\frac{9}{10}\\) \\(\\Longrightarrow\\) \\(g^\\star(x)=1\\).\n\nL’erreur de Bayes vaut \\[\n\\begin{aligned}\n  L^\\star=\\mathbf P(g^\\star(X)\\neq Y&  |X\\leq 0)\\mathbf P(X\\leq 0) \\\\\n  &  +\\mathbf P(g^\\star(X)\\neq Y|X&gt; 0)\\mathbf P(X&gt;0).\n\\end{aligned}\n\\] Or \\[\n\\mathbf P(g^\\star(X)\\neq Y|X\\leq 0)=\\mathbf P(Y\\neq 0|X\\leq 0)=\\frac{1}{5}\n\\] et \\[\\mathbf P(g^\\star(X)\\neq Y|X&gt; 0)=\\mathbf P(Y\\neq 1|X&gt; 0)=\\frac{1}{10}.\\] On obtient \\[L^\\star=\\frac{1}{5}\\,\\frac{1}{2}+\\frac{1}{10}\\,\\frac{1}{2}=\\frac{3}{20}.\\]\n\n\n\nExercice 3 (Fonctions R)  \n\n\nExercice 4 (Score aléatoire) Il suffit d’utiliser l’indépendance entre \\(S(X)\\) et \\(Y\\) : \\[\n\\begin{aligned}\nx(s)= & \\mathbf P(S(X)&gt; s|Y=-1)=\\mathbf P(S(X)&gt; s)=\\mathbf P(S(X)\\geq s) \\\\\n=& \\mathbf P(S(X)\\geq s|Y=1)=y(s),\n\\end{aligned}\n\\]\n\n\nExercice 5 (Score parfait et courbe ROC)  \n\n\nExercice 6 (Score parfait et courbe lift)  \n\n\nExercice 7 (Calculs de scores avec R)  \n\nOn propose d’utiliser la version empirique de \\[\nAUC(S)=\\mathbf P(S(X_1)\\geq S(X_2)|(Y_1,Y_2)=(1,-1)),\n\\] c’est-à-dire d’estimer l’AUC en calculant, parmi les paires qui vérifient \\((Y_1,Y_2)=(1,-1)\\), la proportion de paires qui vérifient \\(S(X_1)\\lt S(X_2)\\). Si on note \\[\n\\mathcal I=\\{(i,j),y_i=1,y_j=0\\},\n\\] alors l’estimateur s’écrit \\[\n\\widehat{AUC}(S)=\\frac{1}{|\\mathcal I|}\\sum_{(i,j)\\in\\mathcal I}\\mathbf 1_{S(X_i)&gt;S(S_j)}.\n\\]\nOn importe les données\n\ndf &lt;- read.csv(\"../donnees/logit_ex6.csv\")\n\n\n\n\n\nset.seed(1234)\nind_app &lt;- sample(nrow(df),300)\ndapp &lt;- df[ind_app,]\ndtest &lt;- df[-ind_app,]\n\n\n\n\n\nlogit &lt;- glm(Y~.,data=dapp,family=\"binomial\")\n\n\n\n\n\nscore &lt;- predict(logit,newdata=dtest,type=\"response\")\n\n\n\n\n\nD0 &lt;- which(dtest$Y==0)\nD1 &lt;- which(dtest$Y==1)\nscore0 &lt;- score[D0]\nscore1 &lt;- score[D1]\nscore.01 &lt;- expand.grid(S0=score0,S1=score1)\nmean(score.01$S1&gt;score.01$S0)\n\n[1] 0.7385817\n\n\n\n\n\n\npROC::roc(dtest$Y,score)\n\n\nCall:\nroc.default(response = dtest$Y, predictor = score)\n\nData: score in 104 controls (dtest$Y 0) &lt; 96 cases (dtest$Y 1).\nArea under the curve: 0.7386\n\n\n\n\n\nExercice 8 (Validation croisée)"
  },
  {
    "objectID": "correction/chap15.html",
    "href": "correction/chap15.html",
    "title": "15 Données déséquilibrées",
    "section": "",
    "text": "library(tidyverse)\n\n\nExercice 1 (Critères pour un exemple de données déséquilibrées)  \n\n\nset.seed(1235)\nn &lt;- 500\nY &lt;- rbinom(n,1,0.05) %&gt;% as.factor()\n\n\nset.seed(12345)\nP1 &lt;- rbinom(n,1,0.005) %&gt;% factor(levels=c(\"0\",\"1\"))\n\n\nset.seed(123)\nP2 &lt;- rep(0,n)\nP2[Y==1] &lt;- rbinom(sum(Y==1),1,0.85) \nP2[Y==0] &lt;- rbinom(sum(Y==0),1,0.1)  \nP2 &lt;- factor(P2,levels=c(\"0\",\"1\"))\n\n\ntable(P1,Y)\n\n   Y\nP1    0   1\n  0 473  24\n  1   3   0\n\ntable(P2,Y)\n\n   Y\nP2    0   1\n  0 432   8\n  1  44  16\n\n\n\nT2 &lt;- table(P2,Y)\nacc &lt;- sum(T2[c(1,4)])/sum(T2)\nrec &lt;- T2[2,2]/sum(T2[,2])\nprec &lt;- T2[2,2]/sum(T2[2,])\nc(acc,rec,prec)\n\n[1] 0.8960000 0.6666667 0.2666667\n\n\n\nF1 &lt;- 2*(rec*prec)/(rec+prec)\nF1\n\n[1] 0.3809524\n\n\n\nrand &lt;- sum(T2[1,])/n*sum(T2[,1])/n+sum(T2[2,])/n*sum(T2[,2])/n\nkappa &lt;- (acc-rand)/(1-rand)\nkappa\n\n[1] 0.3353783\n\n\nRetrouver ces indicateurs à l’aide de la fonction confusionMatrix de caret puis comparer les prévisions P1 et P2.\n::: {.cell}\nlibrary(yardstick)\ndf &lt;- data.frame(Y,P1,P2)\nmulti_metric &lt;- metric_set(accuracy,recall,precision,f_meas,kap)\ndf %&gt;% pivot_longer(-Y,names_to = \"algo\",values_to = \"prev\") %&gt;%\n  group_by(algo) %&gt;%\n  multi_metric(truth=Y,estimate = prev,event_level = \"second\") %&gt;%\n  pivot_wider(names_from = algo,values_from = .estimate) %&gt;% \n  select(-2)\n::: {.cell-output .cell-output-stdout} # A tibble: 5 × 3    .metric        P1    P2    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  1 accuracy   0.946  0.896  2 recall     0      0.667  3 precision  0      0.267  4 f_meas     0      0.381  5 kap       -0.0108 0.335 ::: :::\n\n\n\nExercice 2 (Échantillonnage rétrospectif) On remarque d’abord que \\(\\mathbf P(\\tilde y_i=1)=\\mathbf P(y_i=1|s_i=1)\\). De plus \\[\n\\text{logit}\\, p_\\beta(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}\\quad\\text{et}\\quad \\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1|s_i=1)}{\\mathbf P(y_i=0|s_i=1)}.\n\\] Or \\[\n\\mathbf P(y_i=1|s_i=1)=\\frac{\\mathbf P(y_i=1,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=1)\\mathbf P(y_i=1)}{\\mathbf P(s_i=1)}\n\\] et \\[\n\\mathbf P(y_i=0|s_i=1)=\\frac{\\mathbf P(y_i=0,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=0)\\mathbf P(y_i=0)}{\\mathbf P(s_i=1)}.\n\\] Donc \\[\n\\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}+\\log\\frac{\\mathbf P(s_i=1|y_i=1)}{\\mathbf P(s_i=1|y_i=0)}=\\text{logit}\\,p_\\beta(x_i)+\\log\\left(\\frac{\\tau_{1i}}{\\tau_{0i}}\\right).\n\\]\n\n\nExercice 3 (Rééquilibrage)  \n\n\ndf1 &lt;- read.csv(\"../donnees/dd_exo3_1.csv\") %&gt;% mutate(Y=as.factor(Y))\ndf2 &lt;- read.csv(\"../donnees/dd_exo3_2.csv\") %&gt;% mutate(Y=as.factor(Y))\ndf3 &lt;- read.csv(\"../donnees/dd_exo3_3.csv\") %&gt;% mutate(Y=as.factor(Y))\n\n\nsummary(df1$Y)\n\n  0   1 \n559 441 \n\nsummary(df2$Y)\n\n  0   1 \n692 308 \n\nsummary(df3$Y)\n\n  0   1 \n842 158 \n\n\n\nggplot(df1)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\nggplot(df2)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\nggplot(df3)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\nlibrary(caret)\nset.seed(123)\na1 &lt;- createDataPartition(1:nrow(df1),p=2/3)\na2 &lt;- createDataPartition(1:nrow(df2),p=2/3)\na3 &lt;- createDataPartition(1:nrow(df3),p=2/3)\ntrain1 &lt;- df1[a1$Resample1,]\ntrain2 &lt;- df2[a2$Resample1,]\ntrain3 &lt;- df3[a3$Resample1,]\ntest1 &lt;- df1[-a1$Resample1,]\ntest2 &lt;- df2[-a2$Resample1,]\ntest3 &lt;- df3[-a3$Resample1,]\n\n\nlogit1 &lt;- glm(Y~.^2,data=train1,family=binomial)\nlogit2 &lt;- glm(Y~.^2,data=train2,family=binomial)\nlogit3 &lt;- glm(Y~.^2,data=train3,family=binomial)\np1 &lt;- predict(logit1,newdata=test1,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np2 &lt;- predict(logit2,newdata=test2,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3 &lt;- predict(logit3,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\n\n\nconfusionMatrix(data=p1,reference=test1$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 139  53\n         1  45  95\n\n               Accuracy : 0.7048          \n                 95% CI : (0.6526, 0.7534)\n    No Information Rate : 0.5542          \n    P-Value [Acc &gt; NIR] : 1.282e-08       \n\n                  Kappa : 0.3994          \n\n Mcnemar's Test P-Value : 0.4795          \n\n            Sensitivity : 0.7554          \n            Specificity : 0.6419          \n         Pos Pred Value : 0.7240          \n         Neg Pred Value : 0.6786          \n             Prevalence : 0.5542          \n         Detection Rate : 0.4187          \n   Detection Prevalence : 0.5783          \n      Balanced Accuracy : 0.6987          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(data=p2,reference=test2$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 206  52\n         1  27  47\n\n               Accuracy : 0.762           \n                 95% CI : (0.7125, 0.8068)\n    No Information Rate : 0.7018          \n    P-Value [Acc &gt; NIR] : 0.008642        \n\n                  Kappa : 0.387           \n\n Mcnemar's Test P-Value : 0.006930        \n\n            Sensitivity : 0.8841          \n            Specificity : 0.4747          \n         Pos Pred Value : 0.7984          \n         Neg Pred Value : 0.6351          \n             Prevalence : 0.7018          \n         Detection Rate : 0.6205          \n   Detection Prevalence : 0.7771          \n      Balanced Accuracy : 0.6794          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(data=p3,reference=test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 274  58\n         1   0   0\n\n               Accuracy : 0.8253          \n                 95% CI : (0.7801, 0.8646)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 0.535           \n\n                  Kappa : 0               \n\n Mcnemar's Test P-Value : 7.184e-14       \n\n            Sensitivity : 1.0000          \n            Specificity : 0.0000          \n         Pos Pred Value : 0.8253          \n         Neg Pred Value :    NaN          \n             Prevalence : 0.8253          \n         Detection Rate : 0.8253          \n   Detection Prevalence : 1.0000          \n      Balanced Accuracy : 0.5000          \n\n       'Positive' Class : 0               \n\n\n\nOn remarque que l’accuracy est meilleure pour le 3ème échantillon, contrairement à des indicateurs tels que le \\(\\kappa\\) de Cohen ou le balanced accuracy.\n\nlibrary(UBL)\nset.seed(1243)\ntrain3.over &lt;- RandOverClassif(Y~.,dat=train3)\ntrain3.smote &lt;- SmoteClassif(Y~.,dat=train3)\ntrain3.under &lt;- RandUnderClassif(Y~.,dat=train3)\ntrain3.tomek &lt;- TomekClassif(Y~.,dat=train3)[[1]]\n\n\nggplot(train3.under)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\nggplot(train3.over)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\nggplot(train3.smote)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\nggplot(train3.tomek)+aes(x=X1,y=X2,color=Y)+geom_point()\n\n\n\n\n\nlogit3.over &lt;- glm(Y~.^2,data=train3.over,family=binomial)\nlogit3.smote &lt;- glm(Y~.^2,data=train3.smote,family=binomial)\nlogit3.under &lt;- glm(Y~.^2,data=train3.under,family=binomial)\nlogit3.tomek &lt;- glm(Y~.^2,data=train3.tomek,family=binomial)\np3.over &lt;- predict(logit3.over,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3.smote &lt;- predict(logit3.smote,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3.under &lt;- predict(logit3.under,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\np3.tomek &lt;- predict(logit3.tomek,newdata=test3,type=\"response\") %&gt;%\n  round() %&gt;%\n  as.factor()\n\n\nconfusionMatrix(p3.over,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 182   9\n         1  92  49\n\n               Accuracy : 0.6958          \n                 95% CI : (0.6432, 0.7448)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 1               \n\n                  Kappa : 0.3255          \n\n Mcnemar's Test P-Value : 3.37e-16        \n\n            Sensitivity : 0.6642          \n            Specificity : 0.8448          \n         Pos Pred Value : 0.9529          \n         Neg Pred Value : 0.3475          \n             Prevalence : 0.8253          \n         Detection Rate : 0.5482          \n   Detection Prevalence : 0.5753          \n      Balanced Accuracy : 0.7545          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(p3.smote,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 181   9\n         1  93  49\n\n               Accuracy : 0.6928         \n                 95% CI : (0.6401, 0.742)\n    No Information Rate : 0.8253         \n    P-Value [Acc &gt; NIR] : 1              \n\n                  Kappa : 0.3217         \n\n Mcnemar's Test P-Value : &lt;2e-16         \n\n            Sensitivity : 0.6606         \n            Specificity : 0.8448         \n         Pos Pred Value : 0.9526         \n         Neg Pred Value : 0.3451         \n             Prevalence : 0.8253         \n         Detection Rate : 0.5452         \n   Detection Prevalence : 0.5723         \n      Balanced Accuracy : 0.7527         \n\n       'Positive' Class : 0              \n\n\nconfusionMatrix(p3.under,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 187  11\n         1  87  47\n\n               Accuracy : 0.7048          \n                 95% CI : (0.6526, 0.7534)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 1               \n\n                  Kappa : 0.325           \n\n Mcnemar's Test P-Value : 3.56e-14        \n\n            Sensitivity : 0.6825          \n            Specificity : 0.8103          \n         Pos Pred Value : 0.9444          \n         Neg Pred Value : 0.3507          \n             Prevalence : 0.8253          \n         Detection Rate : 0.5633          \n   Detection Prevalence : 0.5964          \n      Balanced Accuracy : 0.7464          \n\n       'Positive' Class : 0               \n\n\nconfusionMatrix(p3.tomek,test3$Y)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 274  55\n         1   0   3\n\n               Accuracy : 0.8343          \n                 95% CI : (0.7899, 0.8727)\n    No Information Rate : 0.8253          \n    P-Value [Acc &gt; NIR] : 0.3641          \n\n                  Kappa : 0.0826          \n\n Mcnemar's Test P-Value : 3.305e-13       \n\n            Sensitivity : 1.00000         \n            Specificity : 0.05172         \n         Pos Pred Value : 0.83283         \n         Neg Pred Value : 1.00000         \n             Prevalence : 0.82530         \n         Detection Rate : 0.82530         \n   Detection Prevalence : 0.99096         \n      Balanced Accuracy : 0.52586         \n\n       'Positive' Class : 0               \n\n\n\nLes indicateurs adaptés aux données déséquilibrées sont améliorés, on détecte mieux les 1 (quitte à faire plus d’erreur sur les 0).\n\n\n\nExercice 4 (Rééquilibrage et information de Fisher)"
  },
  {
    "objectID": "correction/chap16.html",
    "href": "correction/chap16.html",
    "title": "16 Introduction à la régression spline",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nC\nA\nA\nB\n\n\n\nExercice 2 (Fonction polyreg)  \n\nOn importe les données\n\nozone &lt;- read.table(\"../donnees/ozone_simple.txt\",header=T,sep=\";\")\nsdT12 &lt;- sd(ozone$T12)\n\nOn crée la grille\n\ngrillex &lt;- seq(min(ozone$T12)-sdT12,max(ozone$T12)+sdT12, length =100)\n\nOn transforme en data frame\n\ndf &lt;- data.frame(T12=grillex)\n\nOn effectue une regression polynomiale de degré 3\n\nbasepoly &lt;- poly(ozone$T12,degree=3,raw=T)\nnewval &lt;- predict(basepoly,df$T12)\ndfpoly &lt;- data.frame(O3=ozone$O3,basepoly)\nregpoly &lt;- lm(O3~.,data=dfpoly)\n\nOn prévoit sur une grille\n\ndfnewval &lt;- data.frame(newval)\nprev &lt;- predict(regpoly,dfnewval) \nplot(O3~T12,data=ozone)\nlines(grillex,prev,col=2)\n\n\n\n\nCréation de la fonction\n\npolyreg &lt;- function(ozone,degre=3){\n  sdT12 &lt;- sd(ozone$T12)\n  grillex &lt;- seq(min(ozone$T12)-sdT12,max(ozone$T12)+sdT12, length =100)\n  df &lt;- data.frame(T12=grillex)\n  basepoly &lt;- poly(ozone$T12,degree=3,raw=T)\n  newval &lt;- predict(basepoly,df$T12)\n  dfpoly &lt;- data.frame(O3=ozone$O3,basepoly)\n  regpoly &lt;- lm(O3~.,data=dfpoly)\n  dfnewval &lt;- data.frame(newval)\n  prev &lt;- predict(regpoly,dfnewval)\n  return(list(grillex,prev))\n  }\n\n\n\n\nExercice 3 On applique la fonction précédente\n\nozone &lt;- read.table(\"../donnees//ozone_simple.txt\",header=T,sep=\";\")\nplot(O3 ~ T12, xlim=c(0,35), ylim=c(0,150), data=ozone)\niter &lt;- 1\nfor(ii in c(1,2,3,9)){\n    tmp &lt;- polyreg(ozone,d=ii)\n    lines(tmp$grillex,tmp$grilley,col=iter,lty=iter)\n    iter &lt;- iter+1\n    }\nlegend(15,150,c(\"d=1\",\"d=2\",\"d=3\",\"d=9\"),col=1:4,lty=1:4)\n\n\n\n\n\n\nExercice 4 Considérons la matrice \\(X_B\\) du plan d’expérience obtenue à partir d’une variable réelle \\(X\\) transformée dans \\(\\mathcal{S}_{\\xi}^{d+1}\\) . Cette matrice est composée des \\(d+K+1\\) fonction de base notée \\(b_j\\) et où \\(K\\) est le nombre de noeuds intérieurs et \\(d\\) le degré.\nDans le cours, il est indiqué que les fonctions de base \\(b_j\\) et \\(b_{j+d+1}\\) en conservant l’ordre des fonctions. Donc \\(b_1\\) est orthogonale à toutes les fonctions \\(b_j\\) avec \\(j&gt;d+1\\), idem pour \\(b_2\\) avec \\(j&gt;d+2\\).\nEn faisant donc le calcul \\(X_B'X_B\\) on obtient une matrice bande et donc les termes \\(a_{ij}\\) sont nuls quand \\(j&gt;i+d+1\\).\nOn en déduit que les paramètres estimées \\(\\hat \\beta_k\\) ne sont pas corrélés avec les \\(\\hat \\beta_j\\) dès que \\(j&gt;k+d+1\\). XB est une matrice bande."
  },
  {
    "objectID": "correction/chap17.html",
    "href": "correction/chap17.html",
    "title": "17 Estimateurs à noyau et \\(k\\) plus proches voisins",
    "section": "",
    "text": "Exercice 1 (Questions de cours)  \n\nA\nB\nA\n\n\n\nExercice 2 (Estimateur de Nadaraya-Watson) Il suffit de dériver la quantité à minimiser par rapport à \\(\\beta_1\\) et la valeur de \\(\\beta_1\\) qui annule cette dérivée : \\[-2\\sum_{i=1}^n(y_i-\\beta_1)p_i(x)=0\\quad\\Longleftrightarrow \\quad\\widehat\\beta_1(x)=\\frac{\\sum_{i=1}^ny_ip_i(x)}{\\sum_{i=1}^np_i(x)}.\\]\n\n\nExercice 3 (Polynômes locaux)  \n\n\nExercice 4 (Estimateur à noyau uniforme dans \\(\\mathbb R^p\\))  \n\nEn annulant la dérivée par rapport à \\(a\\), on obtient \\[\\widehat m(x)=\\frac{\\sum_{i=1}^ny_iK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\nOn a \\[\\mathop{\\mathrm{V}}[\\widehat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}\\] et \\[\\mathbf E[\\widehat m(x)]-m(x)=\\frac{\\sum_{i=1}^n(m(x_i)-m(x))K\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\nOn a maintenant \\(|m(x_i)-m(x)|\\leq L\\|x_i-x\\|\\). Or \\[K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] est non nul si et seulement si \\(\\|x_i-x\\|\\leq h\\). Donc pour tout \\(i=1,\\dots,n\\) \\[L\\|x_i-x\\|K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\leq Lh K\\left(\\frac{\\|x_i-x\\|}{h}\\right).\\] D’où le résultat.\nOn a \\[\\mathop{\\mathrm{V}}[\\widehat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}=\\frac{\\sigma^2}{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}.\\] Or \\[\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)\\geq C_1n\\textrm{Vol}(B_h)\\geq C_1\\gamma_dnh^d\\] où \\(\\gamma_d=\\pi^{d/2}/\\Gamma(d/2+1)\\). On a donc \\[\\mathop{\\mathrm{V}}[\\widehat m(x)]\\leq \\frac{\\sigma^2}{C_1\\gamma_dnh^d}=\\frac{C_2\\sigma^2}{nh^d}\\] avec \\(C_2=1/(C_1\\gamma_d)\\).\nOn déduit \\[\\mathbf E[(\\widehat m(x)-m(x))^2]\\leq L^2h^2+\\frac{C_2\\sigma^2}{nh^d}.\\]\nSoit \\(M(h)\\) le majorant ci-dessus. On a \\[M(h)'=2hL^2-\\frac{C_2\\sigma^2d}{n}h^{-d-1}.\\] La dérivée s’annule pour \\[h_{opt}=\\frac{2L^2}{C_2\\sigma^2d}n^{-\\frac{1}{d+2}}.\\] Lorsque \\(h=h_{opt}\\) l’erreur quadratique vérifie \\[\\mathbf E[(\\hat m(x)-m(x))^2]=\\mathrm{O}\\left(n^{-\\frac{2}{d+2}}\\right).\\] La vitesse diminue lorsque la dimension \\(d\\) augmente, c’est le fléau de la dimension.\n\n\n\nExercice 5 (Vitesse de la régression univariée en design equi-espacé)  \n\n\\(\\widehat\\beta\\) minimise \\(\\sum_{i=1}^n(Y_i-\\beta x_i)^2\\). On a donc \\[\\widehat\\beta=\\frac{\\sum_{i=1}^nx_iY_i}{\\sum_{i=1}^nx_i^2}.\\]\nOn déduit \\[\\mathbf E[\\widehat\\beta]=\\beta\\quad\\textrm{et}\\quad\\mathop{\\mathrm{V}}(\\widehat\\beta)=\\frac{\\sigma^2}{\\sum_{i=1}^nx_i^2}.\\]\nComme \\[\\sum_{i=1}^nx_i^2=\\frac{(n+1)(2n+1)}{6n},\\] on obtient le résultat demandé.\n\n\n\nExercice 6 (Critère LOO)  \n\nOn désigne par \\(\\widehat F_h\\) le vecteur \\((\\widehat f_h(x_i),i=1,\\dots,n)\\), \\(\\widehat F_k\\) le vecteur \\((\\widehat f_k(x_i),i=1,\\dots,n)\\) et \\(\\mathbb Y=(y_1,\\dots,y_n)\\). On voit facilement que \\[\\widehat F_h=S_h\\mathbb Y\\quad\\text{et}\\quad \\widehat F_k=S_k\\mathbb Y\\] où \\(S_h\\) et \\(S_k\\) sont des matrices \\(n\\times n\\) dont le terme général est défini par \\[S_{ij,h}=\\frac{K((x_i-x_j)/h)}{\\sum_l K((x_i-x_l)/h)}\n\\quad\\text{et}\\quad\nS_{ij,k}=\n\\left\\{\n\\begin{array}{ll}\n  1/k&\\text{ si $x_j$ est parmi les $k$-ppv de $x_i$} \\\\\n  0 & \\text{ sinon}.\n  \\end{array}\n\\right.\\]\nPour simplifier on note \\(K_{ij}=K((x_i-x_j)/h)\\) On a \\[\\widehat f_h^i(x_i)=\\frac{\\sum_{j\\neq i}K_{ij}y_j}{\\sum_{j\\neq i}K_{ij}}.\\] Par conséquent \\[\\widehat f_h^i(x_i)\\left[\\sum_{j=1}^nK_{ij}-K_{ii}\\right]=\\sum_{j\\neq i}K_{ij}y_j.\\] On obtient le résultat demandé en divisant tout \\(\\sum_{j=1}^nK_{ij}\\). Pour l’estimateur de plus proches voisins, on remarque que, si on enlève la \\(i\\)ème observation alors l’estimateur des \\(k\\) plus proches voisins de \\(x_i\\) s’obtient à partir ce celui des \\(k+1\\) plus proches voisins avec la \\(i\\)ème observation de la façon suivante : \\[\\widehat f_k^i(x_i)=\\frac{k+1}{k}\\sum_{j\\neq i}S_{ij,k+1}y_j.\\] On obtient le résultat demandé on observant que \\(S_{ii,k+1}=1/(k+1)\\) et donc \\[\\frac{1}{1-S_{ii,k+1}}=\\frac{k+1}{k}.\\]\nOn obtient pour l’estimateur à noyau \\[\\begin{align*}\nLOO(\\widehat f_h)= & \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-\\widehat f_h^i(x_i)\\right)^2 \\\\\n= & \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-S_{ii,h}y_i-\\sum_{j\\neq i}S_{ij,h}y_{j}}{1-S_{ii,h}}\\right)^2 \\\\\n= & \\frac{1}{n}\\sum_{i=1}^n\\left(\\frac{y_i-\\widehat f_h(x_i)}{1-S_{ii,h}}\\right)^2.\n\\end{align*}\\] Le calcul est similaire pour l’estimateur des plus proches voisins.\n\n\n\nExercice 7 (Caret et kppv) On importe les données et on ne garde que les deux variables demandées :\n\nozone &lt;- read.table(\"../donnees/ozone.txt\",header=TRUE,sep=\";\")\ndf &lt;- ozone[,c(\"O3\",\"T12\")]\n\nOn construit la grille de plus proches voisins candidats :\n\ngrille &lt;- data.frame(k=1:40)\n\nOn indique à caret qu’on veut faire de la validation croisée 10 blocs :\n\nlibrary(caret)\nctrl &lt;- trainControl(method=\"cv\")\n\nOn lance la validation croisée avec la fonction train :\n\nset.seed(1234)\nsel.k &lt;- train(O3~.,data=df,method=\"knn\",trControl=ctrl,tuneGrid=grille)\nsel.k\n\nk-Nearest Neighbors \n\n50 samples\n 1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 44, 46, 45, 44, 45, 45, ... \nResampling results across tuning parameters:\n\n  k   RMSE      Rsquared   MAE     \n   1  25.39941  0.2777479  21.83800\n   2  20.89184  0.3784027  17.05972\n   3  17.81455  0.5039548  14.88541\n   4  16.49216  0.5636676  14.01838\n   5  16.75867  0.5579746  14.33716\n   6  16.76845  0.5384320  13.92737\n   7  16.11270  0.5792998  13.77474\n   8  15.96855  0.5992164  13.64931\n   9  16.48290  0.5679445  14.13226\n  10  16.89146  0.5572318  14.62217\n  11  17.14669  0.5541549  14.68934\n  12  17.69184  0.5469553  15.27732\n  13  17.41791  0.5668045  15.10278\n  14  17.53775  0.5484378  15.17445\n  15  17.61846  0.5669393  15.35179\n  16  17.89693  0.5550383  15.60799\n  17  18.08559  0.5579067  15.72184\n  18  18.25932  0.5655324  15.89826\n  19  18.84244  0.5402343  16.27675\n  20  19.08747  0.5429639  16.30312\n  21  19.44070  0.5266746  16.57676\n  22  19.66410  0.5196443  16.65315\n  23  19.69539  0.5339187  16.60436\n  24  19.94851  0.5245251  16.73931\n  25  20.04650  0.5072557  16.78093\n  26  20.14871  0.5013264  16.78305\n  27  20.28257  0.5036138  16.89089\n  28  20.45060  0.4988594  16.89900\n  29  20.54389  0.5105147  16.92002\n  30  20.79567  0.5080200  17.12472\n  31  20.96099  0.5014461  17.25705\n  32  21.14524  0.4891512  17.23304\n  33  21.38173  0.4875148  17.38980\n  34  21.59260  0.4810615  17.43883\n  35  21.83266  0.4707654  17.67392\n  36  22.06004  0.4448406  17.74640\n  37  22.34462  0.4236828  17.95419\n  38  22.54304  0.3999950  18.11507\n  39  22.61034  0.4147744  18.15091\n  40  22.68049  0.4296545  18.23871\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 8.\n\n\nOn sélectionnera\n\nsel.k$bestTune\n\n  k\n8 8\n\n\nplus proches voisins."
  }
]